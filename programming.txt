Computer programming is the process of designing and building an executable computer program for accomplishing a specific computing task Programming involves tasks such as analysis generating algorithms profiling algorithms accuracy and resource consumption and the implementation of algorithms in a chosen programming language commonly referred to as coding The source code of a program is written in one or more languages that are intelligible to programmers rather than machine code which is directly executed by the central processing unit The purpose of programming is to find a sequence of instructions that will automate the performance of a task which can be as complex as an operating system on a computer often for solving a given problem The process of programming thus often requires expertise in several different subjects including knowledge of the application domain specialized algorithms and formal logic
Tasks accompanying and related to programming include testing debugging source code maintenance implementation of build systems and management of derived artifacts such as the machine code of computer programs These might be considered part of the programming process but often the term software development is used for this larger process with the term programming implementation or coding reserved for the actual writing of code Software engineering combines engineering techniques with software development practices Reverse engineering is the opposite process A hacker is any skilled computer expert that uses their technical knowledge to overcome a problem but it can also mean a security hacker in common language


==========

Programmable devices have existed at least as far back as 1206 AD when the automata of AlJazari were programmable via pegs and cams to play various rhythms and drum patterns and the 1801 Jacquard loom could produce entirely different weaves by changing the program  a series of pasteboard cards with holes punched in them
However the first computer program is generally dated to 1843 when mathematician Ada Lovelace published an algorithm to calculate a sequence of Bernoulli numbers intended to be carried out by Charles Babbages Analytical Engine

In the 1880s Herman Hollerith invented the concept of storing data in machinereadable form Later a control panel plugboard added to his 1906 Type I Tabulator allowed it to be programmed for different jobs and by the late 1940s unit record equipment such as the IBM 602 and IBM 604 were programmed by control panels in a similar way as were the first electronic computers However with the concept of the storedprogram computers introduced in 1949 both programs and data were stored and manipulated in the same way in computer memoryMachine code was the language of early programs written in the instruction set of the particular machine often in binary notation Assembly languages were soon developed that let the programmer specify instruction in a text format eg ADD X TOTAL with abbreviations for each operation code and meaningful names for specifying addresses However because an assembly language is little more than a different notation for a machine language any two machines with different instruction sets also have different assembly languages

Highlevel languages made the process of developing a program simpler and more understandable and less bound to the underlying hardware FORTRAN the first widely used highlevel language to have a functional implementation came out in 1957 and many other languages were soon developed – in particular COBOL aimed at commercial data processing and Lisp for computer research
Programs were mostly still entered using punched cards or paper tape See computer programming in the punch card era By the late 1960s data storage devices and computer terminals became inexpensive enough that programs could be created by typing directly into the computers Text editors were developed that allowed changes and corrections to be made much more easily than with punched cards


==========


==========
Whatever the approach to development may be the final program must satisfy some fundamental properties The following properties are among the most important

Reliability how often the results of a program are correct This depends on conceptual correctness of algorithms and minimization of programming mistakes such as mistakes in resource management eg buffer overflows and race conditions and logic errors such as division by zero or offbyone errors
Robustness how well a program anticipates problems due to errors not bugs This includes situations such as incorrect inappropriate or corrupt data unavailability of needed resources such as memory operating system services and network connections user error and unexpected power outages
Usability the ergonomics of a program the ease with which a person can use the program for its intended purpose or in some cases even unanticipated purposes Such issues can make or break its success even regardless of other issues This involves a wide range of textual graphical and sometimes hardware elements that improve the clarity intuitiveness cohesiveness and completeness of a programs user interface
Portability the range of computer hardware and operating system platforms on which the source code of a program can be compiledinterpreted and run This depends on differences in the programming facilities provided by the different platforms including hardware and operating system resources expected behavior of the hardware and operating system and availability of platform specific compilers and sometimes libraries for the language of the source code
Maintainability the ease with which a program can be modified by its present or future developers in order to make improvements or customizations fix bugs and security holes or adapt it to new environments Good practices during initial development make the difference in this regard This quality may not be directly apparent to the end user but it can significantly affect the fate of a program over the long term
Efficiencyperformance Measure of system resources a program consumes processor time memory space slow devices such as disks network bandwidth and to some extent even user interaction the less the better This also includes careful management of resources for example cleaning up temporary files and eliminating memory leaks


==========
In computer programming readability refers to the ease with which a human reader can comprehend the purpose control flow and operation of source code It affects the aspects of quality above including portability usability and most importantly maintainability
Readability is important because programmers spend the majority of their time reading trying to understand and modifying existing source code rather than writing new source code Unreadable code often leads to bugs inefficiencies and duplicated code A study found that a few simple readability transformations made code shorter and drastically reduced the time to understand it
Following a consistent programming style often helps readability However readability is more than just programming style Many factors having little or nothing to do with the ability of the computer to efficiently compile and execute the code contribute to readability Some of these factors include

Different indent styles whitespace
Comments
Decomposition
Naming conventions for objects such as variables classes procedures etcThe presentation aspects of this such as indents line breaks color highlighting and so on are often handled by the source code editor but the content aspects reflect the programmers talent and skills
Various visual programming languages have also been developed with the intent to resolve readability concerns by adopting nontraditional approaches to code structure and display Integrated development environments IDEs aim to integrate all such help Techniques like Code refactoring can enhance readability


==========
The academic field and the engineering practice of computer programming are both largely concerned with discovering and implementing the most efficient algorithms for a given class of problem For this purpose algorithms are classified into orders using socalled Big O notation which expresses resource use such as execution time or memory consumption in terms of the size of an input Expert programmers are familiar with a variety of wellestablished algorithms and their respective complexities and use this knowledge to choose algorithms that are best suited to the circumstances


============
Programming a Computer for Playing Chess was a 1950 paper that evaluated a minimax algorithm that is part of the history of algorithmic complexity a course on IBMs Deep Blue chess computer is part of the computer science curriculum at Stanford University


==========
The first step in most formal software development processes is requirements analysis followed by testing to determine value modeling implementation and failure elimination debugging There exist a lot of differing approaches for each of those tasks One approach popular for requirements analysis is Use Case analysis Many programmers use forms of Agile software development where the various stages of formal software development are more integrated together into short cycles that take a few weeks rather than years There are many approaches to the Software development process
Popular modeling techniques include ObjectOriented Analysis and Design OOAD and ModelDriven Architecture MDA The Unified Modeling Language UML is a notation used for both the OOAD and MDA
A similar technique used for database design is EntityRelationship Modeling ER Modeling
Implementation techniques include imperative languages objectoriented or procedural functional languages and logic languages


==========

It is very difficult to determine what are the most popular of modern programming languages Methods of measuring programming language popularity include counting the number of job advertisements that mention the language the number of books sold and courses teaching the language this overestimates the importance of newer languages and estimates of the number of existing lines of code written in the language this underestimates the number of users of business languages such as COBOL
Some languages are very popular for particular kinds of applications while some languages are regularly used to write many different kinds of applications For example COBOL is still strong in corporate data centers often on large mainframe computers Fortran in engineering applications scripting languages in Web development and C in embedded software Many applications use a mix of several languages in their construction and use  New languages are generally designed around the syntax of a prior language with new functionality added for example C++ adds objectorientation to C and Java adds memory management and bytecode to C++ but as a result loses efficiency and the ability for lowlevel manipulation


==========

Debugging is a very important task in the software development process since having defects in a program can have significant consequences for its users Some languages are more prone to some kinds of faults because their specification does not require compilers to perform as much checking as other languages Use of a static code analysis tool can help detect some possible problems Normally the first step in debugging is to attempt to reproduce the problem This can be a nontrivial task for example as with parallel processes or some unusual software bugs Also specific user environment and usage history can make it difficult to reproduce the problem
After the bug is reproduced the input of the program may need to be simplified to make it easier to debug For example a bug in a compiler can make it crash when parsing some large source file However after simplification of the test case only few lines from the original source file can be sufficient to reproduce the same crash Such simplification can be done manually using a divideandconquer approach The programmer will try to remove some parts of original test case and check if the problem still exists When debugging the problem in a GUI the programmer can try to skip some user interaction from the original problem description and check if remaining actions are sufficient for bugs to appear
Debugging is often done with IDEs like Eclipse Visual Studio Xcode Kdevelop NetBeans and CodeBlocks Standalone debuggers like GDB are also used and these often provide less of a visual environment usually using a command line Some text editors such as Emacs allow GDB to be invoked through them to provide a visual environment


==========

Different programming languages support different styles of programming called programming paradigms The choice of language used is subject to many considerations such as company policy suitability to task availability of thirdparty packages or individual preference Ideally the programming language best suited for the task at hand will be selected Tradeoffs from this ideal involve finding enough programmers who know the language to build a team the availability of compilers for that language and the efficiency with which programs written in a given language execute Languages form an approximate spectrum from lowlevel to highlevel lowlevel languages are typically more machineoriented and faster to execute whereas highlevel languages are more abstract and easier to use but execute less quickly It is usually easier to code in highlevel languages than in lowlevel ones
Allen Downey in his bookHow To Think Like A Computer Scientist writes

The details look different in different languages but a few basic instructions appear in just about every language
Input Gather data from the keyboard a file or some other device
Output Display data on the screen or send data to a file or other device
Arithmetic Perform basic arithmetical operations like addition and multiplication
Conditional Execution Check for certain conditions and execute the appropriate sequence of statements
Repetition Perform some action repeatedly usually with some variationMany computer languages provide a mechanism to call functions provided by shared libraries Provided the functions in a library follow the appropriate runtime conventions eg method of passing arguments then these functions may be written in any other language


==========

Computer programmers are those who write computer software Their jobs usually involve


==========


==========


==========
Ceruzzi Paul E 1998 History of Computing Cambridge Massachusetts MIT Press ISBN 9780262032551 – via EBSCOhost
Evans Claire L 2018 Broad Band The Untold Story of the Women Who Made the Internet New York PortfolioPenguin ISBN 9780735211759
Gürer Denise 1995 Pioneering Women in Computer Science PDF Communications of the ACM 38 1 45–54 doi101145204865204875
Smith Erika E 2013 Recognizing a Collective Inheritance through the History of Women in Computing CLCWeb Comparative Literature  Culture A WWWeb Journal 15 1 1–9 – via EBSCOhost


==========
AK Hartmann Practical Guide to Computer Simulations Singapore World Scientific 2009
A Hunt D Thomas and W Cunningham The Pragmatic Programmer From Journeyman to Master Amsterdam AddisonWesley Longman 1999
Brian W Kernighan The Practice of Programming Pearson 1999
Weinberg Gerald M The Psychology of Computer Programming New York Van Nostrand Reinhold 1971
Edsger W Dijkstra A Discipline of Programming PrenticeHall 1976
OJ Dahl EWDijkstra CAR Hoare Structured Pogramming Academic Press 1972
David Gries The Science of Programming SpringerVerlag 1981


==========
 Media related to Computer programming at Wikimedia Commons
 Quotations related to Programming at Wikiquote
Software engineering at CurlieACCU previously known as the Association of C and C++ Users is a nonprofit user group of people interested in software development dedicated to raising the standard of computer programming  The ACCU publishes two journals and organizes an annual conference


==========
ACCU was formed in 1987 by Martin Houston  The original name of the organisation was C Users Group UK and this remained the formal name of the organisation until 2011 although it adopted the public name Association of C and C++ Users for the period 1993–2003 and adopted the shorter form ACCU from 2003 onward  
As the formal name suggests the organisation was originally created for people in the United Kingdom  However the membership is worldwide predominantly European and North American but also with members from central and southern America Australasia Africa and Asia
Originally the voluntary association was mainly for C programmers but it has expanded over time to include all programming languages especially C++ C Java Perl and Python


==========
The ACCU currently publishes two journals
C Vu is a membersonly journal which acts as the associations newsletter and carries book reviews articles on software development and a number of regular columns such as Student Code Critique and Professionalism in Programming It was edited by Phil Stubbington from its first issue until 1991
Overload aims to carry more indepth articles aimed at professional software developers  Topics range from programming and design through to process and management  Overload is available online to members and nonmembers free of chargeOther journals have been published by ACCU in the past  Accent was the news letter of the Silicon Valley chapter and CAUGers was the news letter of the Acorn special interest group  Overload was originally the journal of ACCUs C++ special interest group but is no longer languagespecific


==========
The Silicon Valley chapter organized local meetings in San Jose  Local groups were formed in London Bristol  Bath Oxford Cambridge North East England Southern England and Zurich


==========
The ACCU is operated by a volunteer committee elected at an Annual General Meeting during the
annual conference each Spring which from 1997 to 2012 took place in Oxford and for the first time in Bristol in 2013
It attracts speakers from the computing community including David Abrahams Andrei Alexandrescu Ross J Anderson James Coplien Tom Gilb Kevlin Henney Andrew Koenig Simon PeytonJones Eric S Raymond Guido van Rossum Greg Stein Bjarne Stroustrup the designer and original implementor of C++ Herb Sutter and  Daveed Vandevoorde
The UK Python Conference for the Python programming language originally started out as a track at the ACCU conference


==========
ACCU supports the standardisation process for computer programming languages  ACCU provided financial sponsorship of meetings in the UK for both the International Organization for Standardization ISO C programming language working group and the ISO C++ working groups and helped finance travel to ECMA meetings in mainland Europe


==========
The ACCU operates mailing lists some of which are also open to nonmembers These lists allow for general programmingorientated discussions but also for mentored discussions
Mentored groups have included Effective C++ Python software patterns functional programming and XML They are often based around study of a book


==========


==========
ACCU Official Site
The C Acorn User Group with back issues of CAUGers
CUG
ACCU Silicon Valley ChapterThe ACM Computing Classification System CCS is a subject classification system for computing devised by the Association for Computing Machinery ACM The system is comparable to the Mathematics Subject Classification MSC in scope aims and structure being used by the various ACM journals to organise subjects by area


==========
The system has gone through seven revisions the first version being published in 1964 and revised versions appearing in 1982 1983 1987 1991 1998 and the now current version in 2012


==========
The ACM Computing Classification System version 2012 has a revolutionary change in some areas for example in Software that now is called Software and its engineering which has three main subjects

Software organization and properties This subject addresses properties of the software itself
Software notations and tools This subject covers programming languages and other tools for writing programs
Software creation and management This subject covers human activities including software managementIt is hierarchically structured in four levels Thus for example one branch of the hierarchy contains

Computing methodologies
Artificial intelligence
Knowledge representation and reasoning
Ontology engineering


==========

Mathematics Subject Classification MSC
Physics and Astronomy Classification Scheme PACS
arXiv a preprint server that uses a somewhat different subdivision of topics in its computer science subject areas but also allows papers to be classified using the ACM system
PhySH Physics Subject Headings


==========
Coulter Neal 1997 ACMs computing classification system reflects changing times Communications of the ACM New York NY USA ACM 40 12 111–112 doi101145265563265579
Coulter Neal chair French James Glinert Ephraim Horton Thomas Mead Nancy Ralston Anthony Rada Roy Rodkin Craig Rous Bernard Tucker Allen Wegner Peter Weiss Eric Wierzbicki Carol January 21 1998 Computing Classification System 1998 Current Status and Future Maintenance Report of the CCS Update Committee PDF Computing Reviews New York NY USA ACM 1–5
Mirkin Boris Nascimento Susana Pereira Luis Moniz 2008 Representing a Computer Science Research Organization on the ACM Computing Classification System  in Eklund Peter Haemmerlé Ollivier eds Supplementary Proceedings of the 16th International Conference on Conceptual Structures ICCS2008 PDF CEUR Workshop Proceedings 354 RWTH Aachen University pp 57–65


==========
ACM Computing Classification System is the homepage of the system including links to four complete versions of the system for 1964 1 1991 2 1998 3 and the current 2012 version 4
The ACM Computing Research Repository uses a classification scheme that is much coarser than the ACM subject classification and does not cover all areas of CS but is intended to better cover active areas of research In addition papers in this repository are classified according to the ACM subject classificationA Guide to the Business Analysis Body of Knowledge BABOK Guide is a standard for the practice of business analysis


==========
BABOK Guide was first published by International Institute of Business Analysis IIBA as a draft document version 14 in October 2005 for consultation with the wider business analysis and project management community to document and standardize generally accepted business analysis practices The first formal release was at version 16 in June 2006 Version 20 was released 31 March 2009  Version 3 was released in April 2015


==========
Once the body of knowledge was established IIBA created the Certified Business Analysis Professional CBAP designation to recognise senior business analysts who could demonstrate indepth longterm experience in these knowledge areas 5–10 years in a dedicated business analyst roleIIBA also offers the Certification of Competency on Business Analysis CCBA designation that recognizes Business Analysts with 3750 hours business analysis experience including 900 hours experience in two knowledge areas or 500 hours experience in four knowledge areas and 21 professional development hoursFor both certifications above the applicant must have a minimum high school education or equivalent two references from a career manager client or Certified Business Analyst Professional and sign the IIBA Code of ConductIIBA also offers the Entry Certificate in Business Analysis ECBA that does not require a reference


==========
BABOK Guide includes chapters on

Business Analysis Key Concepts define important terms that are the foundation of the practice of business analysis
Knowledge Areas represents the core content of BABOK Guide and contain the business analysis tasks that are used to perform business analysis
Underlying Competencies describes the behaviours characteristics knowledge and personal qualities that help business analysts be effective in their job
Techniques describes 50 of the most common techniques used by business analysts
Perspectives describes 5 different views of business analysis Agile Business Intelligence Information Technology Business Architecture and Business Process Management New to BABOK Guide version 3 Perspectives demonstrate the continued evolution of the practice of business analysis


==========
BABOK Guide organizes business analysis tasks within 6 knowledge areas Each task describes the typical knowledge skills deliverables and techniques that the business analyst requires to be able to perform those tasks competently The knowledge areas logically organize tasks but do not specify a sequence process or methodology
The knowledge areas of BABOK Guide are

Business Analysis Planning and Monitoring describes the tasks used to organize and coordinate business analysis efforts
Elicitation and Collaboration describes the tasks used to prepare for and conduct elicitation activities and confirm the results
Requirements Life Cycle Management  the tasks used to manage and maintain requirements and design information from inception to retirement
Strategy Analysis describes the tasks used to identify the business need address that need and align the change strategy within the enterprise
Requirements Analysis and Design Definition describes the tasks used to organize requirements specify and model requirements and designs validate and verify information identify solution options and estimate the potential value that could be realized
Solution Evaluation describes the tasks used to assess the performance of and value delivered by a solution and to recommend improvements on increasing values


==========
BABOK 2 2009 ISBN 9780981129211
BABOK 3 2015 ISBN 9781927584026


==========


==========
International Institute of Business AnalysisAcceptance test–driven development ATDD is a development methodology based on communication between the business customers the developers and the testers ATDD encompasses many of the same practices as specification by example SBE behaviordriven development BDD exampledriven development EDD and supportdriven development also called story test–driven development SDD All these processes aid developers and testers in understanding the customers needs prior to implementation and allow customers to be able to converse in their own domain language
ATDD is closely related to testdriven development TDD  It differs by the emphasis on developertesterbusiness customer collaboration   ATDD encompasses acceptance testing but highlights writing acceptance tests before developers begin coding


==========
Acceptance tests are from the users point of view – the external view of the system They examine externally visible effects such as specifying the correct output of a system given a particular input  Acceptance tests can verify how the state of something changes such as an order that goes from paid to shipped  They also can check the interactions with interfaces of other systems such as shared databases or web services  In general they are implementation independent although automation of them may not be


==========
Acceptance tests are created when the requirements are analyzed and prior to coding  They can be developed collaboratively by requirement requester product owner business analyst customer representative etc developer and tester  Developers implement the system using the acceptance tests  Failing tests provide quick feedback that the requirements are not being met  The tests are specified in business domain terms  The terms then form a ubiquitous language that is shared between the customers developers and testers Tests and requirements are interrelated A requirement that lacks a test may not be implemented properly  A test that does not refer to a requirement is an unneeded test  An acceptance test that is developed after implementation begins represents a new requirement


==========
Acceptance tests are a part of an overall testing strategy They are the customer tests that demonstrate the business intent of a system Component tests are technical acceptance tests developed by an architect that specify the behavior of large modules  Unit tests are created by the developer to drive easytomaintain code They are often derived from acceptance tests and unit tests  Crossfunctional testing includes usability testing exploratory testing and property testing scaling and security


==========
Acceptance criteria are a description of what would be checked by a test Given a requirement such as “As a user I want to check out a book from the library” an acceptance criterion might be “Verify the book is marked as checked out  An acceptance test for this requirement gives the details so that the test can be run with the same effect each time


==========
Acceptance tests usually follow this formGiven setup

A specified state of a systemWhen trigger

An action or event occursThen verification

The state of the system has changed or an output has been producedAlso it is possible to add Statements that start with AND in any of the sections below Given When Then

For the example requirement the steps could be listed as


==========
The previous steps do not include any specific example data so that is added to complete the test
Given

Book that has not been checked outUser who is registered on the systemWhen

User checks out a bookThen

Book is marked as checked out


==========
Examination of the test with specific data usually leads to many questions For the sample these might be

What if the book is already checked out
What if the book does not exist
What if the user is not registered on the system
Is there a date that the book is due to be checkedin
How many books can a user check outThese questions help illuminate missing or ambiguous requirements  Additional details such as a duedate can be added to the expected result  Other acceptance tests can check that conditions such as attempting to check out a book that is already checked out produces the expected error


==========
Suppose the business customer wanted a business rule that a user could only check out one book at a time  The following test would demonstrate that
Scenario
Check that checkout business rule is enforced
Given

Book that has been checked outWhen

User checks out another bookThen

Error occurs


==========
In addition to acceptance tests for requirements acceptance tests can be used on a project as a whole For example if this requirement was part of a library book checkout project there could be acceptance tests for the whole project These are often termed SMART objectives An example test is When the new library system is in production the users will be able to check books in and out three times as fast as they do today


==========
Concordion
FitNesse
Robot Framework
Gauge software


==========


==========
Example of automation frameworksAugusta Ada King Countess of Lovelace née Byron 10 December 1815 – 27 November 1852 was an English mathematician and writer chiefly known for her work on Charles Babbages proposed mechanical generalpurpose computer the Analytical Engine She was the first to recognise that the machine had applications beyond pure calculation and published the first algorithm intended to be carried out by such a machine As a result she is sometimes regarded as the first to recognise the full potential of a computing machine and one of the first computer programmersLovelace was the only legitimate child of poet Lord Byron and his wife Lady Byron All of Byrons other children were born out of wedlock to other women Byron separated from his wife a month after Ada was born and left England forever four months later He commemorated the parting in a poem that begins Is thy face like thy mothers my fair child ADA sole daughter of my house and heart He died of disease in the Greek War of Independence when Ada was eight years old Her mother remained bitter and promoted Adas interest in mathematics and logic in an effort to prevent her from developing her fathers perceived insanity Despite this Ada remained interested in Byron Upon her eventual death she was buried next to him at her request Although often ill in her childhood Ada pursued her studies assiduously She married William King in 1835 King was made Earl of Lovelace in 1838 Ada thereby becoming Countess of Lovelace
Her educational and social exploits brought her into contact with scientists such as Andrew Crosse Charles Babbage Sir David Brewster Charles Wheatstone Michael Faraday and the author Charles Dickens contacts which she used to further her education Ada described her approach as poetical science and herself as an Analyst  MetaphysicianWhen she was a teenager her mathematical talents led her to a long working relationship and friendship with fellow British mathematician Charles Babbage who is known as the father of computers She was in particular interested in Babbages work on the Analytical Engine Lovelace first met him in June 1833 through their mutual friend and her private tutor Mary Somerville
Between 1842 and 1843 Ada translated an article by Italian military engineer Luigi Menabrea on the calculating engine supplementing it with an elaborate set of notes simply called Notes These notes contain what many consider to be the first computer program—that is an algorithm designed to be carried out by a machine Other historians reject this perspective and point out that Babbages personal notes from the years 18361837 contain the first programs for the engine Lovelaces notes are important in the early history of computers She also developed a vision of the capability of computers to go beyond mere calculating or numbercrunching while many others including Babbage himself focused only on those capabilities Her mindset of poetical science led her to ask questions about the Analytical Engine as shown in her notes examining how individuals and society relate to technology as a collaborative toolShe died of uterine cancer in 1852 at the age of 36


==========


==========

Lord Byron expected his child to be a glorious boy and was disappointed when Lady Byron gave birth to a girl The child was named after Byrons halfsister Augusta Leigh and was called Ada by Byron himself On 16 January 1816 at Lord Byrons command Lady Byron left for her parents home at Kirkby Mallory taking their fiveweekold daughter with her Although English law at the time granted full custody of children to the father in cases of separation Lord Byron made no attempt to claim his parental rights but did request that his sister keep him informed of Adas welfare

On 21 April Lord Byron signed the deed of separation although very reluctantly and left England for good a few days later Aside from an acrimonious separation Lady Byron continued throughout her life to make allegations about her husbands immoral behaviour This set of events made Lovelace infamous in Victorian society She did not have a relationship with her father He died in 1824 when she was eight years old Her mother was the only significant parental figure in her life Lovelace was not shown the family portrait of her father until her 20th birthdayLovelace did not have a close relationship with her mother She was often left in the care of her maternal grandmother Judith Hon Lady Milbanke who doted on her However because of societal attitudes of the time—which favoured the husband in any separation with the welfare of any child acting as mitigation—Lady Byron had to present herself as a loving mother to the rest of society This included writing anxious letters to Lady Milbanke about her daughters welfare with a cover note saying to retain the letters in case she had to use them to show maternal concern In one letter to Lady Milbanke she referred to her daughter as it I talk to it for your satisfaction not my own and shall be very glad when you have it under your own Lady Byron had her teenage daughter watched by close friends for any sign of moral deviation Lovelace dubbed these observers the Furies and later complained they exaggerated and invented stories about her

Lovelace was often ill beginning in early childhood At the age of eight she experienced headaches that obscured her vision In June 1829 she was paralysed after a bout of measles She was subjected to continuous bed rest for nearly a year something which may have extended her period of disability By 1831 she was able to walk with crutches Despite the illnesses she developed her mathematical and technological skills

When Ada was twelve years old this future Lady Fairy as Charles Babbage affectionately called her decided she wanted to fly Ada Byron went about the project methodically thoughtfully with imagination and passion Her first step in February 1828 was to construct wings She investigated different material and sizes She considered various materials for the wings paper oilsilk wires and feathers She examined the anatomy of birds to determine the right proportion between the wings and the body She decided to write a book Flyology illustrating with plates some of her findings She decided what equipment she would need for example a compass to cut across the country by the most direct road so that she could surmount mountains rivers and valleys Her final step was to integrate steam with the art of flying
Ada Byron had an affair with a tutor in early 1833 She tried to elope with him after she was caught but the tutors relatives recognized her and contacted her mother Lady Byron and her friends covered the incident up to prevent a public scandal Lovelace never met her younger halfsister Allegra the daughter of Lord Byron and Claire Clairmont Allegra died in 1822 at the age of five Lovelace did have some contact with Elizabeth Medora Leigh the daughter of Byrons halfsister Augusta Leigh who purposely avoided Lovelace as much as possible when introduced at court


==========

Lovelace became close friends with her tutor Mary Somerville who introduced her to Charles Babbage in 1833 She had a strong respect and affection for Somerville and they corresponded for many years Other acquaintances included the scientists Andrew Crosse Sir David Brewster Charles Wheatstone Michael Faraday and the author Charles Dickens She was presented at Court at the age of seventeen and became a popular belle of the season in part because of her brilliant mind By 1834 Ada was a regular at Court and started attending various events She danced often and was able to charm many people and was described by most people as being dainty although John Hobhouse Byrons friend described her as a large coarseskinned young woman but with something of my friends features particularly the mouth This description followed their meeting on 24 February 1834 in which Ada made it clear to Hobhouse that she did not like him probably due to her mothers influence which led her to dislike all of her fathers friends This first impression was not to last and they later became friendsOn 8 July 1835 she married William 8th Baron King becoming Lady King They had three homes Ockham Park Surrey a Scottish estate on Loch Torridon in Rossshire and a house in London They spent their honeymoon at Worthy Manor in Ashley Combe near Porlock Weir Somerset The Manor had been built as a hunting lodge in 1799 and was improved by King in preparation for their honeymoon It later became their summer retreat and was further improved during this time From 1845 the familys main house was East Horsley Towers rebuilt in the Victorian Gothic fashion by the architect of the Houses of Parliament Charles BarryThey had three children Byron born 12 May 1836 Anne Isabella called Annabella born 22 September 1837 and Ralph Gordon born 2 July 1839 Immediately after the birth of Annabella Lady King experienced a tedious and suffering illness which took months to cure Ada was a descendant of the extinct Barons Lovelace and in 1838 her husband was made Earl of Lovelace and Viscount Ockham meaning Ada became the Countess of Lovelace In 1843–44 Adas mother assigned William Benjamin Carpenter to teach Adas children and to act as a moral instructor for Ada He quickly fell for her and encouraged her to express any frustrated affections claiming that his marriage meant he would never act in an unbecoming manner When it became clear that Carpenter was trying to start an affair Ada cut it offIn 1841 Lovelace and Medora Leigh the daughter of Lord Byrons halfsister Augusta Leigh were told by Adas mother that her father was also Medoras father On 27 February 1841 Ada wrote to her mother I am not in the least astonished In fact you merely confirm what I have for years and years felt scarcely a doubt about but should have considered it most improper in me to hint to you that I in any way suspected She did not blame the incestuous relationship on Byron but instead blamed Augusta Leigh I fear she is more inherently wicked than he ever was In the 1840s Ada flirted with scandals firstly from a relaxed approach to extramarital relationships with men leading to rumours of affairs and secondly from her love of gambling She apparently lost more than £3000 on the horses during the later 1840s The gambling led to her forming a syndicate with male friends and an ambitious attempt in 1851 to create a mathematical model for successful large bets This went disastrously wrong leaving her thousands of pounds in debt to the syndicate forcing her to admit it all to her husband She had a shadowy relationship with Andrew Crosses son John from 1844 onwards John Crosse destroyed most of their correspondence after her death as part of a legal agreement She bequeathed him the only heirlooms her father had personally left to her During her final illness she would panic at the idea of the younger Crosse being kept from visiting her


==========
Throughout her illnesses she continued her education Her mothers obsession with rooting out any of the insanity of which she accused Byron was one of the reasons that Ada was taught mathematics from an early age She was privately schooled in mathematics and science by William Frend William King and Mary Somerville the noted 19thcentury researcher and scientific author One of her later tutors was the mathematician and logician Augustus De Morgan From 1832 when she was seventeen her mathematical abilities began to emerge and her interest in mathematics dominated the majority of her adult life In a letter to Lady Byron De Morgan suggested that her daughters skill in mathematics could lead her to become an original mathematical investigator perhaps of firstrate eminenceLovelace often questioned basic assumptions by integrating poetry and science While studying differential calculus she wrote to De Morgan

I may remark that the curious transformations many formulae can undergo the unsuspected and to a beginner apparently impossible identity of forms exceedingly dissimilar at first sight is I think one of the chief difficulties in the early part of mathematical studies I am often reminded of certain sprites and fairies one reads of who are at ones elbows in one shape now and the next minute in a form most dissimilar
Lovelace believed that intuition and imagination were critical to effectively applying mathematical and scientific concepts She valued metaphysics as much as mathematics viewing both as tools for exploring the unseen worlds around us


==========

Lovelace died at the age of 36 – the same age at which her father had died – on 27 November 1852 from uterine cancer probably exacerbated by bloodletting by her physicians The illness lasted several months in which time Annabella took command over whom Ada saw and excluded all of her friends and confidants Under her mothers influence she had a religious transformation and was coaxed into repenting of her previous conduct and making Annabella her executor She lost contact with her husband after confessing something to him on 30 August which caused him to abandon her bedside It is not known what she told him She was buried at her request next to her father at the Church of St Mary Magdalene in Hucknall Nottinghamshire A memorial plaque in Latin to her and her father is in the chapel attached to Horsley Towers


==========
Throughout her life Lovelace was strongly interested in scientific developments and fads of the day including phrenology and mesmerism After her work with Babbage Lovelace continued to work on other projects In 1844 she commented to a friend Woronzow Greig about her desire to create a mathematical model for how the brain gives rise to thoughts and nerves to feelings a calculus of the nervous system She never achieved this however In part her interest in the brain came from a longrunning preoccupation inherited from her mother about her potential madness As part of her research into this project she visited the electrical engineer Andrew Crosse in 1844 to learn how to carry out electrical experiments In the same year she wrote a review of a paper by Baron Karl von Reichenbach Researches on Magnetism but this was not published and does not appear to have progressed past the first draft In 1851 the year before her cancer struck she wrote to her mother mentioning certain productions she was working on regarding the relation of maths and music

Lovelace first met Charles Babbage in June 1833 through their mutual friend Mary Somerville Later that month Babbage invited Lovelace to see the prototype for his difference engine She became fascinated with the machine and used her relationship with Somerville to visit Babbage as often as she could Babbage was impressed by Lovelaces intellect and analytic skills He called her The Enchantress of Number In 1843 he wrote to her

Forget this world and all its troubles and if possible its multitudinous Charlatans—every thing in short but the Enchantress of Number

During a ninemonth period in 1842–43 Lovelace translated the Italian mathematician Luigi Menabreas article on Babbages newest proposed machine the Analytical Engine With the article she appended a set of notes Explaining the Analytical Engines function was a difficult task as even many other scientists did not really grasp the concept and the British establishment had shown little interest in it Lovelaces notes even had to explain how the Analytical Engine differed from the original Difference Engine Her work was well received at the time the scientist Michael Faraday described himself as a supporter of her writingThe notes are around three times longer than the article itself and include in Note G in complete detail a method for calculating a sequence of Bernoulli numbers using the Analytical Engine which might have run correctly had it ever been built only Babbages Difference Engine has been built completed in London in 2002 Based on this work Lovelace is now widely considered to be the first computer programmer and her method is recognised as the worlds first computer programNote G also contains Lovelaces dismissal of artificial intelligence She wrote that The Analytical Engine has no pretensions whatever to originate anything It can do whatever we know how to order it to perform It can follow analysis but it has no power of anticipating any analytical relations or truths This objection has been the subject of much debate and rebuttal for example by Alan Turing in his paper Computing Machinery and IntelligenceLovelace and Babbage had a minor falling out when the papers were published when he tried to leave his own statement criticising the governments treatment of his Engine as an unsigned preface which could have been mistakenly interpreted as a joint declaration When Taylors Scientific Memoirs ruled that the statement should be signed Babbage wrote to Lovelace asking her to withdraw the paper This was the first that she knew he was leaving it unsigned and she wrote back refusing to withdraw the paper The historian Benjamin Woolley theorised that His actions suggested he had so enthusiastically sought Adas involvement and so happily indulged her  because of her celebrated name Their friendship recovered and they continued to correspond On 12 August 1851 when she was dying of cancer Lovelace wrote to him asking him to be her executor though this letter did not give him the necessary legal authority Part of the terrace at Worthy Manor was known as Philosophers Walk as it was there that Lovelace and Babbage were reputed to have walked while discussing mathematical principles


==========

In 1840 Babbage was invited to give a seminar at the University of Turin about his Analytical Engine Luigi Menabrea a young Italian engineer and the future Prime Minister of Italy transcribed Babbages lecture into French and this transcript was subsequently published in the Bibliothèque universelle de Genève in October 1842
Babbages friend Charles Wheatstone commissioned Ada Lovelace to translate Menabreas paper into English She then augmented the paper with notes which were added to the translation Ada Lovelace spent the better part of a year doing this assisted with input from Babbage These notes which are more extensive than Menabreas paper were then published in the September 1843 edition of Taylors Scientific Memoirs under the initialism AALAda Lovelaces notes were labelled alphabetically from A to G In note G she describes an algorithm for the Analytical Engine to compute Bernoulli numbers It is considered to be the first published algorithm ever specifically tailored for implementation on a computer and Ada Lovelace has often been cited as the first computer programmer for this reason The engine was never completed so her program was never testedIn 1953 more than a century after her death Ada Lovelaces notes on Babbages Analytical Engine were republished as an appendix to BV Bowdens Faster than Thought A Symposium on Digital Computing Machines The engine has now been recognised as an early model for a computer and her notes as a description of a computer and software


==========
In her notes Lovelace emphasised the difference between the Analytical Engine and previous calculating machines particularly its ability to be programmed to solve problems of any complexity She realised the potential of the device extended far beyond mere number crunching In her notes she wrote

The Analytical Engine might act upon other things besides number were objects found whose mutual fundamental relations could be expressed by those of the abstract science of operations and which should be also susceptible of adaptations to the action of the operating notation and mechanism of the engineSupposing for instance that the fundamental relations of pitched sounds in the science of harmony and of musical composition were susceptible of such expression and adaptations the engine might compose elaborate and scientific pieces of music of any degree of complexity or extent
This analysis was an important development from previous ideas about the capabilities of computing devices and anticipated the implications of modern computing one hundred years before they were realised Walter Isaacson ascribes Lovelaces insight regarding the application of computing to any process based on logical symbols to an observation about textiles When she saw some mechanical looms that used punchcards to direct the weaving of beautiful patterns it reminded her of how Babbages engine used punched cards to make calculations This insight is seen as significant by writers such as Betty Toole and Benjamin Woolley as well as the programmer John GrahamCumming whose project Plan 28 has the aim of constructing the first complete Analytical Engine
According to the historian of computing and Babbage specialist Doron Swade Ada saw something that Babbage in some sense failed to see In Babbages world his engines were bound by numberWhat Lovelace saw—what Ada Byron saw—was that number could represent entities other than quantity So once you had a machine for manipulating numbers if those numbers represented other things letters musical notes then the machine could manipulate symbols of which number was one instance according to rules It is this fundamental transition from a machine which is a number cruncher to a machine for manipulating symbols according to rules that is the fundamental transition from calculation to computation—to generalpurpose computation—and looking back from the present high ground of modern computing if we are looking and sifting history for that transition then that transition was made explicitly by Ada in that 1843 paper


==========
Though Lovelace is referred to as the first computer programmer some biographers computer scientists and historians of computing claim otherwise
Allan G Bromley in the 1990 article Difference and Analytical Engines

All but one of the programs cited in her notes had been prepared by Babbage from three to seven years earlier The exception was prepared by Babbage for her although she did detect a bug in it Not only is there no evidence that Ada ever prepared a program for the Analytical Engine but her correspondence with Babbage shows that she did not have the knowledge to do so
Bruce Collier who later wrote a biography of Babbage wrote in his 1970 Harvard University PhD thesis that Lovelace made a considerable contribution to publicizing the Analytical Engine but there is no evidence that she advanced the design or theory of it in any wayEugene Eric Kim and Betty Alexandra Toole consider it incorrect to regard Lovelace as the first computer programmer as Babbage wrote the initial programs for his Analytical Engine although the majority were never published Bromley notes several dozen sample programs prepared by Babbage between 1837 and 1840 all substantially predating Lovelaces notes Dorothy K Stein regards Lovelaces notes as more a reflection of the mathematical uncertainty of the author the political purposes of the inventor and above all of the social and cultural context in which it was written than a blueprint for a scientific developmentIn his book Idea Makers Stephen Wolfram defends Lovelaces contributions While acknowledging that Babbage wrote several unpublished algorithms for the Analytical Engine prior to Lovelaces notes Wolfram argues that theres nothing as sophisticated—or as clean—as Adas computation of the Bernoulli numbers Babbage certainly helped and commented on Adas work but she was definitely the driver of it Wolfram then suggests that Lovelaces main achievement was to distill from Babbages correspondence a clear exposition of the abstract operation of the machine—something which Babbage never didDoron Swade a specialist on history of computing known for his work on Babbage analyzed four claims about Lovelace during a lecture on Babbages analytical engine

She was a mathematical genius
She made an influential contribution to the analytical engine
She was the first computer programmer
She was a prophet of the computer ageAccording to him only the fourth claim had any substance at all He explained that Ada was only a promising beginner instead of genius in mathematics that she began studying basic concepts of mathematics five years after Babbage conceived the analytical engine so she could not have made important contributions to it and that she only published the first computer program instead of actually writing it But he agrees that Ada was the only person to see the potential of the analytical engine as a machine capable of expressing entities other than quantities


==========

Lovelace has been portrayed in Romulus Linneys 1977 play Childe Byron the 1990 steampunk novel The Difference Engine by William Gibson and Bruce Sterling the 1997 film Conceiving Ada and in John Crowleys 2005 novel Lord Byrons Novel The Evening Land where she is featured as an unseen character whose personality is forcefully depicted in her annotations and antiheroic efforts to archive her fathers lost novelIn Tom Stoppards 1993 play Arcadia the precocious teenage genius Thomasina Coverly a character apparently based on Ada Lovelace—the play also involves Lord Byron comes to understand chaos theory and theorises the second law of thermodynamics before either is officially recognised The 2015 play Ada and the Memory Engine by Lauren Gunderson portrays Lovelace and Charles Babbage in unrequited love and it imagines a postdeath meeting between Lovelace and her fatherLovelace and Babbage are the main characters in Sydney Paduas webcomic and graphic novel The Thrilling Adventures of Lovelace and Babbage The comic features extensive footnotes on the history of Ada Lovelace and many lines of dialogue are drawn from actual correspondenceLovelace and Mary Shelley as teenagers are the central characters in Jordan Stratfords steampunk series The Wollstonecraft Detective AgencyLovelace identified as Ada Augusta Byron is portrayed by Lily Lesser in the second season of The Frankenstein Chronicles  She is employed as an analyst to provide the workings of a lifesized humanoid automaton  The brass workings of the machine are reminiscent of Babbages analytical engine  Her employment is described as keeping her occupied until she returns to her studies in advanced mathematicsAs of November 2015 all new British passports have included an illustration of Lovelace and Babbage on pages 46 and 47In 2017 a Google Doodle honoured her on International Womens DayLovelace and Babbage appear as characters in the second season of the ITV series Victoria 2017 Emerald Fennell portrays Lovelace in the episode The GreenEyed MonsterThe Cardano platform uses ADA as the name for their cryptocurrency and Lovelace as the smallest subunit of an ADAOn 2 February 2018 Satellogic a highresolution Earth observation imaging and analytics company launched a ÑuSat type microsatellite named in honor of Ada Lovelace


==========

The computer language Ada created on behalf of the United States Department of Defense was named after Lovelace The reference manual for the language was approved on 10 December 1980 and the Department of Defense Military Standard for the language MILSTD1815 was given the number of the year of her birth
In 1981 the Association for Women in Computing inaugurated its Ada Lovelace Award Since 1998 the British Computer Society BCS has awarded the Lovelace Medal and in 2008 initiated an annual competition for women students BCSWomen sponsors the Lovelace Colloquium an annual conference for women undergraduates Ada College is a furthereducation college in Tottenham Hale London focused on digital skillsAda Lovelace Day is an annual event celebrated on the second Tuesday of October which began in 2009 Its goal is to  raise the profile of women in science technology engineering and maths and to create new role models for girls and women in these fields Events have included Wikipedia editathons with the aim of improving the representation of women on Wikipedia in terms of articles and editors to reduce unintended gender bias on Wikipedia The Ada Initiative was a nonprofit organisation dedicated to increasing the involvement of women in the free culture and open source movementsThe Engineering in Computer Science and Telecommunications College building in Zaragoza University is called the Ada Byron Building The computer centre in the village of Porlock near where Lovelace lived is named after her Ada Lovelace House is a councilowned building in KirkbyinAshfield Nottinghamshire near where Lovelace spent her infancy the building which formerly housed the local district council offices now provides highquality office space for a number of local startup businessesShe is also the inspiration and influence for the Ada Developers Academy in Seattle Washington The academy is a nonprofit that seeks to increase diversity in tech by training women trans and nonbinary people to be software engineersIn 2018 The New York Times published a belated obituary for Ada LovelaceOn 27 July 2018 Senator Ron Wyden submitted in the United States Senate the designation of 9 October 2018 as National Ada Lovelace Day To honor the life and contributions of Ada Lovelace as a leading woman in science and mathematics The resolution SRes592 was considered and agreed to without amendment and with a preamble by unanimous consent


==========
The bicentenary of Ada Lovelaces birth was celebrated with a number of events including
The Ada Lovelace Bicentenary Lectures on Computability Israel Institute for Advanced Studies 20 December 2015 – 31 January 2016
Ada Lovelace Symposium University of Oxford 13–14 October 2015
AdaAdaAda a onewoman show about the life and work of Ada Lovelace using an LED dress premiered at Edinburgh International Science Festival on 11 April 2015 and continues to touring internationally to promote diversity on STEM at technology conferences businesses government and educational organisationsSpecial exhibitions were displayed by the Science Museum in London England and the Weston Library part of the Bodleian Library in Oxford England


==========
Lovelace Ada King Ada the Enchantress of Numbers A Selection from the Letters of Lord Byrons Daughter and her Description of the First Computer Mill Valley CA Strawberry Press 1992 ISBN 9780912647098
Menabrea Luigi Federico Lovelace Ada 1843 Sketch of the Analytical Engine invented by Charles Babbage with notes by the translator Translated by Ada Lovelace  In Richard Taylor ed Scientific Memoirs 3 London Richard and John E Taylor pp 666–731


==========
Six copies of the 1843 first edition of Sketch of the Analytical Engine with Ada Lovelaces Notes have been located Three are held at Harvard University one at the University of Oklahoma and one at the United States Air Force Academy On 20 July 2018 the sixth copy was sold at auction to an anonymous buyer for £95000 A digital facsimile of one of the copies in the Harvard University Library is available online


==========

Code Debugging the Gender Gap
Great Lives § Series 31 August – October 2013 — the episode aired on 17 September 2013 was dedicated to the story of Ada Lovelace
List of pioneers in computer science
Timeline of women in science
Women in computing
Women in STEM fields


==========


==========


==========


==========
Jenny Uglow Stepping Out of Byrons Shadow review of Miranda Seymour In Byrons Wake  The Turbulent Lives of Byrons Wife and Daughter  Annabella Milbanke and Ada Lovelace Pegasus 2018 547 pp and Christopher Hollings Ursula Martin and Adrian Rice Ada Lovelace The Making of a Computer Scientist Bodleian Library 2018 114 pp The New York Review of Books vol LXV no 18 22 November 2018 pp 30–32


==========
Adas Army gets set to rewrite history at Inspirefest 2018 by Luke Maxwell 4 August 2018
Works by Ada Lovelace at Open Library
Untangling the Tale of Ada Lovelace by Stephen Wolfram December 2015
Ada Lovelace Founder of Scientific Computing Women in Science SDSC
Ada Byron Lady Lovelace Biographies of Women Mathematicians Agnes Scott College
Papers of the Noel Byron and Lovelace families archive UK Archives hub
Ada Lovelace  The Analytical Engine Babbage Computer History
Ada  the Analytical Engine archive Educause
Ada Lovelace Countess of Controversy Tech TV vault G4 TV
Ada Lovelace streaming In Our Time audio UK The BBC Radio 4 6 March 2008
Ada Lovelaces Notes and The Ladies Diary Yale
The fascinating story Ada Lovelace Sabine Allaeys Youtube
Ada Lovelace the Worlds First Computer Programmer on Science and Religion Maria Popova Brain
How Ada Lovelace Lord Byrons Daughter Became the Worlds First Computer Programmer Maria Popova Brain
OConnor John J Robertson Edmund F Ada Lovelace MacTutor History of Mathematics archive University of St AndrewsAdaptive software development ASD is a software development process that grew out of the work by Jim Highsmith and Sam Bayer on rapid application development RAD It embodies the principle that continuous adaptation of the process to the work at hand is the normal state of affairs

Adaptive software development replaces the traditional waterfall cycle with a repeating series of speculate collaborate and learn cycles This dynamic cycle provides for continuous learning and adaptation to the emergent state of the project The characteristics of an ASD life cycle are that it is mission focused feature based iterative timeboxed risk driven and change tolerant As with RAD ASD is also an antecedent to agile software development
The word speculate refers to the paradox of planning – it is more likely to assume that all stakeholders are comparably wrong for certain aspects of the project’s mission while trying to define it During speculation the project is initiated and adaptive cycle planning is conducted
Adaptive cycle planning uses project initiation information—the customer’s
mission statement project constraints eg delivery dates or user descriptions and
basic requirements—to define the set of release cycles software increments that
will be required for the project
Collaboration refers to the efforts for balancing the work based on predictable parts of the environment planning and guiding them and adapting to the uncertain surrounding mix of changes caused by various factors such as technology requirements stakeholders software vendors  The learning cycles challenging all stakeholders are based on the short iterations with design build and testing During these iterations the knowledge is gathered by making small mistakes based on false assumptions and correcting those mistakes thus leading to greater experience and eventually mastery in the problem domain


==========

Adaptive Software Development A Collaborative Approach to Managing Complex Systems Highsmith JA 2000 New York Dorset House 392pp ISBN 0932633404
Agile Project Management Creating Innovative Products AddisonWesley Jim Highsmith March 2004 277pp ISBN 0321219775
Lev Virine  Michael Trumper 2007 Project Decisions The Art and Science Management Concepts ISBN 9781567262179
Adaptive SD
Software EngineeringA Practitioners Approach Roger Pressman Bruce Maxim ISBN 9780078022128Adele Goldstine née Katz December 21 1920 – November 1964 was an American mathematician and computer programmer She wrote the manual for the first electronic digital computer ENIAC Through her work programming the computer she was also an instrumental player in converting the ENIAC from a computer that needed to be reprogrammed each time it was used to one that was able to perform a set of fifty stored instructions


==========
Goldstine was born in New York City on December 21 1920 to Jewish parents She attended Hunter College High School then Hunter College After receiving her BA she attended the University of Michigan where she earned a Masters in mathematics  At Michigan she met Herman Goldstine the military liaison and administrator for the construction of the ENIAC and the two were married in 1941


==========
As an instructor of mathematics for the women computers at the Moore School of Electrical Engineering Goldstine also trained some of the six women who were the original programmers of ENIAC to manually calculate ballistic trajectories complex differential calculations The job of computer was critical to the war effort and women were regarded as capable of doing the work more rapidly and accurately than men By 1943 and for the balance of World War II essentially all computers were women as were many of their direct supervisors 
She wrote the Operators Manual for the ENIAC after the six women Kay McNulty Betty Jean Jennings Betty Snyder Marlyn Wescoff Fran Bilas and Ruth Lichterman trained themselves to program the ENIAC using its logical and electrical block diagrams Reconfiguring the machine to solve a different problem involved physically plugging and unplugging wires on the machine it was called settingup as the modern terminology of program had not yet come into useIn 1946 Goldstine sat in on programming sessions with Bartik and Dick Clippinger and was hired to help implement Clippingers stored program modification to the ENIAC John von Neumann was a consultant on the selection of the instruction set implemented  This solved the problem of the programmers having to unplug and replug patch cables for every program the machine was to run instead the program was entered on the three function tables which had previously been used only for storage of a trajectorys drag function ENIAC programmer Jean Bartik called Goldstine one of her three great programming partners along with Betty Holberton and Art Gehring They worked together to program the Taub program for the ENIAC


==========
After the war Goldstine continued her programming work with von Neumann at Los Alamos National Laboratory where she devised problems for ENIAC to process She had two children born in 1952 and 1959 She was diagnosed with cancer in 1962 and died two years later at the age of 43 in 1964


==========
Kathleen Antonelli
Jean Bartik
Betty Holberton
Marlyn Meltzer
Frances Spence
Ruth Teitelbaum


==========


==========
Women in Computer Science Womens First Roles in the 20th Century Computer World Archived from the original on March 2 2006 Retrieved February 25 2006 


==========
Adele Katz Goldstine biodata ieeeghnorg accessed October 19 2016Agile software development is an approach to software development under which requirements and solutions evolve through the collaborative effort of selforganizing and crossfunctional teams and their customersend users It advocates adaptive planning evolutionary development early delivery and continual improvement and it encourages rapid and flexible response to changeThe term agile sometimes written Agile was popularized in this context by the Manifesto for Agile Software Development The values and principles espoused in this manifesto were derived from and underpin a broad range of software development frameworks including Scrum and KanbanWhile there is much anecdotal evidence that adopting agile practices and values improves the agility of software professionals teams and organizations some empirical studies have disputed that evidence


==========
Iterative and incremental development methods can be traced back as early as 1957 with evolutionary project management and adaptive software development emerging in the early 1970s
During the 1990s a number of lightweight software development methods evolved in reaction to the prevailing heavyweight methods that critics described as overly regulated planned and micromanaged These included rapid application development RAD from 1991 the unified process UP and dynamic systems development method DSDM both from 1994 Scrum from 1995 Crystal Clear and extreme programming XP both from 1996 and featuredriven development from 1997 Although these all originated before the publication of the Agile Manifesto they are now collectively referred to as agile software development methods At the same time similar changes were underway in manufacturing and aerospaceIn 2001 these seventeen software developers met at a resort in Snowbird Utah to discuss these lightweight development methods Kent Beck Ward Cunningham Dave Thomas Jeff Sutherland Ken Schwaber Jim Highsmith Alistair Cockburn Robert C Martin Mike Beedle Arie van Bennekum  Martin Fowler James Grenning Andrew Hunt Ron Jeffries Jon Kern Brian Marick and Steve Mellor Together they published the Manifesto for Agile Software DevelopmentIn 2005 a group headed by Cockburn and Highsmith wrote an addendum of project management principles the PM Declaration of Interdependence to guide software project management according to agile software development methods
In 2009 a group working with Martin wrote an extension of software development principles the Software Craftsmanship Manifesto to guide agile software development according to professional conduct and mastery
In 2011 the Agile Alliance created the Guide to Agile Practices renamed the Agile Glossary in 2016 an evolving opensource compendium of the working definitions of agile practices terms and elements along with interpretations and experience guidelines from the worldwide community of agile practitioners


==========


==========
Based on their combined experience of developing software and helping others do that the seventeen signatories to the manifesto proclaimed that they value
  Individuals and Interactions over processes and tools 
  Working Software over comprehensive documentation 
  Customer Collaboration over contract negotiation 
  Responding to Change over following a plan That is to say the items on the left are valued more than the items on the right
As Scott Ambler elucidated
Tools and processes are important but it is more important to have competent people working together effectively
Good documentation is useful in helping people to understand how the software is built and how to use it but the main point of development is to create software not documentation
A contract is important but is no substitute for working closely with customers to discover what they need
A project plan is important but it must not be too rigid to accommodate changes in technology or the environment stakeholders priorities and peoples understanding of the problem and its solutionSome of the authors formed the Agile Alliance a nonprofit organization that promotes software development according to the manifestos values and principles Introducing the manifesto on behalf of the Agile Alliance Jim Highsmith said

The Agile movement is not antimethodology in fact many of us want to restore credibility to the word methodology We want to restore a balance We embrace modeling but not in order to file some diagram in a dusty corporate repository We embrace documentation but not hundreds of pages of nevermaintained and rarelyused tomes We plan but recognize the limits of planning in a turbulent environment Those who would brand proponents of XP or SCRUM or any of the other Agile Methodologies as hackers are ignorant of both the methodologies and the original definition of the term hacker


==========
The Manifesto for Agile Software Development is based on twelve principles
Customer satisfaction by early and continuous delivery of valuable software
Welcome changing requirements even in late development
Deliver working software frequently weeks rather than months
Close daily cooperation between business people and developers
Projects are built around motivated individuals who should be trusted
Facetoface conversation is the best form of communication colocation
Working software is the primary measure of progress
Sustainable development able to maintain a constant pace
Continuous attention to technical excellence and good design
Simplicity—the art of maximizing the amount of work not done—is essential
Best architectures requirements and designs emerge from selforganizing teams
Regularly the team reflects on how to become more effective and adjusts accordingly


==========


==========
Most agile development methods break product development work into small increments that minimize the amount of upfront planning and design Iterations or sprints are short time frames timeboxes that typically last from one to four weeks Each iteration involves a crossfunctional team working in all functions planning analysis design coding unit testing and acceptance testing At the end of the iteration a working product is demonstrated to stakeholders This minimizes overall risk and allows the product to adapt to changes quickly An iteration might not add enough functionality to warrant a market release but the goal is to have an available release with minimal bugs at the end of each iteration Multiple iterations might be required to release a product or new features Working software is the primary measure of progress


==========
The principle of colocation is that coworkers on the same team should be situated together to better establish the identity as a team and to improve communication This enables facetoface interaction ideally in front of a whiteboard that reduces the cycle time typically taken when questions and answers are mediated through phone persistent chat wiki or emailNo matter which development method is followed every team should include a customer representative Product Owner in Scrum This person is agreed by stakeholders to act on their behalf and makes a personal commitment to being available for developers to answer questions throughout the iteration At the end of each iteration stakeholders and the customer representative review progress and reevaluate priorities with a view to optimizing the return on investment ROI and ensuring alignment with customer needs and company goals
In agile software development an information radiator is a normally large physical display located prominently near the development team where passersby can see it It presents an uptodate summary of the product development status A build light indicator may also be used to inform a team about the current status of their product development


==========
A common characteristic in agile software development is the daily standup also known as the daily scrum In a brief session team members report to each other what they did the previous day toward their teams iteration goal what they intend to do today toward the goal and any roadblocks or impediments they can see to the goal


==========
Specific tools and techniques such as continuous integration automated unit testing pair programming testdriven development design patterns behaviordriven development domaindriven design code refactoring and other techniques are often used to improve quality and enhance product development agility This is predicated on designing and building quality in from the beginning and being able to demonstrate software for customers at any point or at least at the end of every iteration


==========
Compared to traditional software engineering agile software development mainly targets complex systems and product development with dynamic nondeterministic and nonlinear characteristics Accurate estimates stable plans and predictions are often hard to get in early stages and confidence in them is likely to be low Agile practitioners will seek to reduce the leapoffaith that is needed before any evidence of value can be obtained Requirements and design are held to be emergent Big upfront specifications would probably cause a lot of waste in such cases ie are not economically sound These basic arguments and previous industry experiences learned from years of successes and failures have helped shape agile developments favor of adaptive iterative and evolutionary development


==========
Development methods exist on a continuum from adaptive to predictive Agile software development methods lie on the adaptive side of this continuum One key of adaptive development methods is a rolling wave approach to schedule planning which identifies milestones but leaves flexibility in the path to reach them and also allows for the milestones themselves to changeAdaptive methods focus on adapting quickly to changing realities When the needs of a project change an adaptive team changes as well An adaptive team has difficulty describing exactly what will happen in the future The further away a date is the more vague an adaptive method is about what will happen on that date An adaptive team cannot report exactly what tasks they will do next week but only which features they plan for next month When asked about a release six months from now an adaptive team might be able to report only the mission statement for the release or a statement of expected value vs cost
Predictive methods in contrast focus on analysing and planning the future in detail and cater for known risks In the extremes a predictive team can report exactly what features and tasks are planned for the entire length of the development process Predictive methods rely on effective early phase analysis and if this goes very wrong the project may have difficulty changing direction Predictive teams often institute a change control board to ensure they consider only the most valuable changes
Risk analysis can be used to choose between adaptive agile or valuedriven and predictive plandriven methods Barry Boehm and Richard Turner suggest that each side of the continuum has its own home ground as follows


==========
One of the differences between agile software development methods and waterfall is the approach to quality and testing In the waterfall model there is always a separate testing phase after a build phase however in agile software development testing is completed in the same iteration as programming 
Another difference is that traditional waterfall software development moves a project through various Software Development Lifecycle SDLC phases One phase is completed in its entirety before moving on to the next phase
Because testing is done in every iteration—which develops a small piece of the software—users can frequently use those new pieces of software and validate the value After the users know the real value of the updated piece of software they can make better decisions about the softwares future Having a value retrospective and software replanning session in each iteration—Scrum typically has iterations of just two weeks—helps the team continuously adapt its plans so as to maximize the value it delivers This follows a pattern similar to the PDCA cycle as the work is planned done checked in the review and retrospective and any changes agreed are acted upon
This iterative approach supports a product rather than a project mindset This provides greater flexibility throughout the development process whereas on projects the requirements are defined and locked down from the very beginning making it difficult to change them later Iterative product development allows the software to evolve in response to changes in business environment or market requirementsBecause of the short iteration style of agile software development it also has strong connections with the lean startup concept


==========
In a letter to IEEE Computer Steven Rakitin expressed cynicism about agile software development calling it yet another attempt to undermine the discipline of software engineering and translating working software over comprehensive documentation as we want to spend all our time coding Remember real programmers dont write documentationThis is disputed by proponents of agile software development who state that developers should write documentation if that is the best way to achieve the relevant goals but that there are often better ways to achieve those goals than writing static documentationScott Ambler states that documentation should be just barely good enough JBGE that too much or comprehensive documentation would usually cause waste and developers rarely trust detailed documentation because its usually out of sync with code while too little documentation may also cause problems for maintenance communication learning and knowledge sharing Alistair Cockburn wrote of the Crystal Clear method 

Crystal considers development a series of cooperative games and intends that the documentation is enough to help the next win at the next game The work products for Crystal include use cases risk list iteration plan core domain models and design notes to inform on choiceshowever there are no templates for these documents and descriptions are necessarily vague but the objective is clear just enough documentation for the next game I always tend to characterize this to my team as what would you want to know if you joined the team tomorrow


==========

Agile software development methods support a broad range of the software development life cycle Some focus on the practices eg XP pragmatic programming agile modeling while some focus on managing the flow of work eg Scrum Kanban Some support activities for requirements specification and development eg FDD while some seek to cover the full development life cycle eg DSDM RUP
Notable agile software development frameworks include


==========
Agile software development is supported by a number of concrete practices covering areas like requirements design modeling coding testing planning risk management process quality etc Some notable agile software development practices include


==========
In the literature different terms refer to the notion of method adaptation including method tailoring method fragment adaptation and situational method engineering Method tailoring is defined as

A process or capability in which human agents determine a system development approach for a specific project situation through responsive changes in and dynamic interplays between contexts intentions and method fragments

Situationappropriateness should be considered as a distinguishing characteristic between agile methods and more plandriven software development methods with agile methods allowing product development teams to adapt working practices according to the needs of individual products Potentially most agile methods could be suitable for method tailoring such as DSDM tailored in a CMM context and XP tailored with the Rule Description Practices RDP technique Not all agile proponents agree however with Schwaber noting that is how we got into trouble in the first place thinking that the problem was not having a perfect methodology Efforts should center on the changes needed in the enterprise Bas Vodde reinforced this viewpoint suggesting that unlike traditional large methodologies that require you to pick and choose elements Scrum provides the basics on top of which you add additional elements to localise and contextualise its use Practitioners seldom use system development methods or agile methods specifically by the book often choosing to omit or tailor some of the practices of a method in order to create an inhouse methodIn practice methods can be tailored using various tools Generic process modeling languages such as Unified Modeling Language can be used to tailor software development methods However dedicated tools for method engineering such as the Essence Theory of Software Engineering of SEMAT also exist


==========
Agile software development has been widely seen as highly suited to certain types of environments including small teams of experts working on greenfield projects and the challenges and limitations encountered in the adoption of agile software development methods in a large organization with legacy infrastructure are welldocumented and understoodIn response a range of strategies and patterns has evolved for overcoming challenges with largescale development efforts 20 developers or distributed noncolocated development teams amongst other challenges and there are now several recognised frameworks that seek to mitigate or avoid these challenges 

Scaled agile framework SAFe Dean Leffingwell et al
Disciplined agile delivery DAD Scott Ambler et al
Largescale scrum LeSS Craig Larman and Bas Vodde
Nexus scaled professional Scrum Ken Schwaber
Scrum at Scale Jeff Sutherland Alex Brown
Enterprise Scrum Mike Beedle
Setchu Scrumbased lightweight framework Michael Ebbage
Xscale
Agile path
Holistic Software Development There are many conflicting viewpoints on whether all of these are effective or indeed fit the definition of agile development and this remains an active and ongoing area of researchWhen agile software development is applied in a distributed setting with teams dispersed across multiple business locations it is commonly referred to as distributed agile development The goal is to leverage the unique benefits offered by each approach Distributed development allow organizations to build software by strategically setting up teams in different parts of the globe virtually building software roundtheclock more commonly referred to as followthesun model On the other hand agile development provides increased transparency continuous feedback and more flexibility when responding to changes


==========
Agile software development methods were initially seen as best suitable for noncritical product developments thereby excluded from use in regulated domains such as medical devices pharmaceutical financial nuclear systems automotive and avionics sectors etc However in the last several years there have been several initiatives for the adaptation of agile methods for these domainsThere are numerous standards that may apply in regulated domains including ISO 26262 ISO 9000 ISO 9001 and ISOIEC 15504
A number of key concerns are of particular importance in regulated domains
Quality assurance QA Systematic and inherent quality management underpinning a controlled professional process and reliability and correctness of product
Safety and security Formal planning and risk management to mitigate safety risks for users and securely protecting users from unintentional and malicious misuse
Traceability Documentation providing auditable evidence of regulatory compliance and facilitating traceability and investigation of problems
Verification and Validation VV Embedded throughout the software development process eg user requirements specification functional specification design specification code review unit tests integration tests system tests


==========
Although agile software development methods can be used with any programming paradigm or language in practice they were originally closely associated with objectoriented environments such as Smalltalk and Lisp and later Java The initial adopters of agile methods were usually small to mediumsized teams working on unprecedented systems with requirements that were difficult to finalize and likely to change as the system was being developed This section describes common problems that organizations encounter when they try to adopt agile software development methods as well as various techniques to measure the quality and performance of agile teams


==========
The best agile practitioners have always emphasized thorough engineering principles As a result there are a number of best practices and tools for measuring the performance of agile software development and teams


============
The Agility measurement index amongst others rates developments against five dimensions of product development duration risk novelty effort and interaction Other techniques are based on measurable goals and one study suggests that velocity can be used as a metric of agility There are also agile selfassessments to determine whether a team is using agile software development practices Nokia test Karlskrona test 42 points test


============
One of the early studies reporting gains in quality productivity and business satisfaction by using agile software developments methods was a survey conducted by Shine Technologies from November 2002 to January 2003A similar survey the State of Agile is conducted every year starting in 2006 with thousands of participants from around the software development community This tracks trends on the benefits of agility lessons learned and good practices Each survey has reported increasing numbers saying that agile software development helps them deliver software faster improves their ability to manage changing customer priorities and increases their productivity Surveys have also consistently shown better results with agile product development methods compared to classical project management In balance there are reports that some feel that agile development methods are still too young to enable extensive academic research of their success


==========
Organizations and teams implementing agile software development often face difficulties transitioning from more traditional methods such as waterfall development such as teams having an agile process forced on them These are often termed agile antipatterns or more commonly agile smells Below are some common examples


============
A goal of agile software development is to focus more on producing working software and less on documentation This is in contrast to waterfall models where the process is often highly controlled and minor changes to the system require significant revision of supporting documentation However this does not justify completely doing without any analysis or design at all Failure to pay attention to design can cause a team to proceed rapidly at first but then to have significant rework required as they attempt to scale up the system One of the key features of agile software development is that it is iterative When done correctly design emerges as the system is developed and commonalities and opportunities for reuse are discovered


============
In agile software development stories similar to use case descriptions are typically used to define requirements and an iteration is a short period of time during which the team commits to specific goals Adding stories to an iteration in progress is detrimental to a good flow of work These should be added to the product backlog and prioritized for a subsequent iteration or in rare cases the iteration could be cancelledThis does not mean that a story cannot expand Teams must deal with new information which may produce additional tasks for a story If the new information prevents the story from being completed during the iteration then it should be carried over to a subsequent iteration However it should be prioritized against all remaining stories as the new information may have changed the storys original priority


============
Agile software development is often implemented as a grassroots effort in organizations by software development teams trying to optimize their development processes and ensure consistency in the software development life cycle By not having sponsor support teams may face difficulties and resistance from business partners other development teams and management Additionally they may suffer without appropriate funding and resources This increases the likelihood of failure


============
A survey performed by VersionOne found respondents cited insufficient training as the most significant cause for failed agile implementations Teams have fallen into the trap of assuming the reduced processes of agile software development compared to other methodologies such as waterfall means that there are no actual rules for agile software development


============
The product owner is responsible for representing the business in the development activity and is often the most demanding roleA common mistake is to have the product owner role filled by someone from the development team This requires the team to make its own decisions on prioritization without real feedback from the business They try to solve business issues internally or delay work as they reach outside the team for direction This often leads to distraction and a breakdown in collaboration


============
Agile software development requires teams to meet product commitments which means they should focus only on work for that product However team members who appear to have spare capacity are often expected to take on other work which makes it difficult for them to help complete the work to which their team had committed


============
Teams may fall into the trap of spending too much time preparing or planning This is a common trap for teams less familiar with agile software development where the teams feel obliged to have a complete understanding and specification of all stories Teams should be prepared to move forward only with those stories in which they have confidence then during the iteration continue to discover and prepare work for subsequent iterations often referred to as backlog refinement or grooming


============
A daily standup should be a focused timely meeting where all team members disseminate information If problemsolving occurs it often can only involve certain team members and potentially is not the best use of the entire teams time If during the daily standup the team starts diving into problemsolving it should be set aside until a subteam can discuss usually immediately after the standup completes 


============
One of the intended benefits of agile software development is to empower the team to make choices as they are closest to the problem Additionally they should make choices as close to implementation as possible to use more timely information in the decision If team members are assigned tasks by others or too early in the process the benefits of localized and timely decision making can be lostBeing assigned work also constrains team members into certain roles for example team member A must always do the database work which limits opportunities for crosstraining Team members themselves can choose to take on tasks that stretch their abilities and provide crosstraining opportunities


============
Another common pitfall is for a scrum master to act as a contributor While not prohibited by the Scrum methodology the scrum master needs to ensure they have the capacity to act in the role of scrum master first and not working on development tasks A scrum masters role is to facilitate the process rather than create the productHaving the scrum master also multitasking may result in too many context switches to be productive Additionally as a scrum master is responsible for ensuring roadblocks are removed so that the team can make forward progress the benefit gained by individual tasks moving forward may not outweigh roadblocks that are deferred due to lack of capacity


============
Due to the iterative nature of agile development multiple rounds of testing are often needed Automated testing helps reduce the impact of repeated unit integration and regression tests and frees developers and testers to focus on higher value workTest automation also supports continued refactoring required by iterative software development Allowing a developer to quickly run tests to confirm refactoring has not modified the functionality of the application may reduce the workload and increase confidence that cleanup efforts have not introduced new defects


============
Focusing on delivering new functionality may result in increased technical debt The team must allow themselves time for defect remediation and refactoring Technical debt hinders planning abilities by increasing the amount of unscheduled work as production defects distract the team from further progressAs the system evolves it is important to refactor as entropy of the system naturally increases Over time the lack of constant maintenance causes increasing defects and development costs


============
A common misconception is that agile software development allows continuous change however an iteration backlog is an agreement of what work can be completed during an iteration Having too much workinprogress WIP results in inefficiencies such as contextswitching and queueing The team must avoid feeling pressured into taking on additional work


============
Agile software development fixes time iteration duration quality and ideally resources in advance though maintaining fixed resources may be difficult if developers are often pulled away from tasks to handle production incidents while the scope remains variable The customer or product owner often push for a fixed scope for an iteration However teams should be reluctant to commit to the locked time resources and scope commonly known as the project management triangle Efforts to add scope to the fixed time and resources of agile software development may result in decreased quality


============
Due to the focused pace and continuous nature of agile practices there is a heightened risk of burnout among members of the delivery team


==========
The term agile management is applied to an iterative incremental method of managing the design and build activities of engineering information technology and other business areas that aim to provide new product or service development in a highly flexible and interactive manner based on the principles expressed in the Manifesto for Agile Software DevelopmentAgile X techniques may also be called extreme project management It is a variant of iterative life cycle where deliverables are submitted in stages The main difference between agile and iterative development is that agile methods complete small portions of the deliverables in each delivery cycle iteration while iterative methods evolve the entire set of deliverables over time completing them near the end of the project Both iterative and agile methods were developed as a reaction to various obstacles that developed in more sequential forms of project organization For example as technology projects grow in complexity end users tend to have difficulty defining the longterm requirements without being able to view progressive prototypes Projects that develop in iterations can constantly gather feedback to help refine those requirements
Agile management also offers a simple framework promoting communication and reflection on past work amongst team members Teams who were using traditional waterfall planning and adopted the agile way of development typically go through a transformation phase and often take help from agile coaches who help guide the teams through a smooth transformation There are typically two styles of agile coaching pushbased and pullbased agile coaching Agile management approaches have also been employed and adapted to the business and government sectors For example within the federal government of the United States the United States Agency for International Development USAID is employing a collaborative project management approach that focuses on incorporating collaborating learning and adapting CLA strategies to iterate and adapt programmingAgile methods are mentioned in the Guide to the Project Management Body of Knowledge PMBOK Guide under the Project Lifecycle definition

Adaptive project life cycle a project life cycle also known as changedriven or agile methods that is intended to facilitate change and require a high degree of ongoing stakeholder involvement Adaptive life cycles are also iterative and incremental but differ in that iterations are very rapid usually 24 weeks in length and are fixed in time and resources


==========

According to JeanLoup Richet Research Fellow at ESSEC Institute for Strategic Innovation  Services this approach can be leveraged effectively for nonsoftware products and for project management in general especially in areas of innovation and uncertainty The end result is a product or project that best meets current customer needs and is delivered with minimal costs waste and time enabling companies to achieve bottom line gains earlier than via traditional approachesAgile software development methods have been extensively used for development of software products and some of them use certain characteristics of software such as object technologies However these techniques can be applied to the development of nonsoftware products such as computers motor vehicles medical devices food clothing and music Agile software development methods have been used in nondevelopment IT infrastructure deployments and migrations Some of the wider principles of agile software development have also found application in general management eg strategy governance risk finance under the terms business agility or agile business management
Under an agile business management model agile software development techniques practices principles and values are expressed across five domains
Integrated customer engagement to embed customers within any delivery process to share accountability for productservice delivery
Facilitationbased management adopting agile management models like the role of Scrum Master to facilitate the daytoday operation of teams
Agile work practices adopting specific iterative and incremental work practices such as Scrum Kanban testdriven development or featuredriven development across all business functions from sales human resources finance and marketing
An enabling organisational structure with a focus on staff engagement personal autonomy and outcomes based governance
Applications of agile process along with DevOps and lean manufacturing to data analytics business intelligence big data and data science is called DataOpsAgile software development paradigms can be used in other areas of life such as raising children Its success in child development might be founded on some basic management principles communication adaptation and awareness In a TED Talk Bruce Feiler shared how he applied basic agile paradigms to household management and raising children


==========
Agile practices can be inefficient in large organizations and certain types of developments Many organizations believe that agile software development methodologies are too extreme and adopt a Hybrid approach  that mixes elements of agile software development and plandriven approaches Some methods such as dynamic systems development method DSDM attempt this in a disciplined way without sacrificing fundamental principles
The increasing adoption of agile practices has also been criticized as being a management fad that simply describes existing good practices under new jargon promotes a one size fits all mindset towards development strategies and wrongly emphasizes method over resultsAlistair Cockburn organized a celebration of the 10th anniversary of the Manifesto for Agile Software Development in Snowbird Utah on 12 February 2011 gathering some 30+ people who had been involved at the original meeting and since A list of about 20 elephants in the room undiscussable agile topicsissues were collected including aspects the alliances failures and limitations of agile software development practices and context possible causes commercial interests decontextualization no obvious way to make progress based on failure limited objective evidence cognitive biases and reasoning fallacies politics and culture As Philippe Kruchten wrote

The agile movement is in some ways a bit like a teenager very selfconscious checking constantly its appearance in a mirror accepting few criticisms only interested in being with its peers rejecting en bloc all wisdom from the past just because it is from the past adopting fads and new jargon at times cocky and arrogant But I have no doubts that it will mature further become more open to the outside world more reflective and therefore more effective


==========
Workers selfmanagement


==========


==========
Abrahamsson P Salo O Ronkainen J Warsta J 2002 Agile Software Development Methods Review and Analysis VTT Publications 478
Ashmore Sondra Runyan Kristin 2014 Introduction to Agile Methods AddisonWesley ISBN 9780321929563
Cohen D Lindvall M Costa P 2004 An introduction to agile methods  In Zelkowitz Marvin ed Advances in Software Engineering Advances in Computers 62 Academic Press pp 1–66 ISBN 9780080471907
Dingsøyr Torgeir Dybå Tore Moe Nils Brede 2010 Agile Software Development Current Research and Future Directions Springer ISBN 9783642125751
Fowler Martin 2001 Is Design Dead  In Succi Giancarlo Marchesi Michele eds Extreme Programming Examined AddisonWesley pp 3–18 ISBN 9780201710403
Larman Craig Basili Victor R June 2003 Iterative and Incremental Development A Brief History PDF IEEE Computer 36 3 47–56 doi101109MC20031204375
Casagni Michelle Benito Robert Mayfield Dr Kathleen M Northern Carlton 8 September 2013 Handbook for Implementing Agile in Department of Defense Information Technology Acquisition The Mitre Corporation MITRE
Moran Alan 2015 Managing Agile Strategy Implementation Organisation and People Springer ISBN 9783319162621
Riehle Dirk A Comparison of the Value Systems of Adaptive Software Development and Extreme Programming How Methodologies May Learn From Each Other In Succi  Marchesi 2001
Shore James Warden Shane 2008 The Art of Agile Development OReilly Media ISBN 9780596527679
Stephens M Rosenberg D 2003 Extreme Programming Refactored The Case Against XP Apress ISBN 9781590590966


==========
Agile Manifesto
Agile Glossary
The New Methodology Martin Fowlers description of the background to agile methods
Ten Authors of The Agile Manifesto Celebrate its Tenth Anniversary
AgilePatternsorgBadīʿ azZaman Abu lʿIzz ibn Ismāʿīl ibn arRazāz alJazarī 1136–1206 Arabic بديع الزمان أَبُ اَلْعِزِ إبْنُ إسْماعِيلِ إبْنُ الرِّزاز الجزري‎ IPA ældʒæzæriː was a Muslim polymath a scholar inventor mechanical engineer artisan artist and mathematician He is best known for writing The Book of Knowledge of Ingenious Mechanical Devices Arabic كتاب في معرفة الحيل الهندسية‎ romanized Kitab fi marifat alhiyal alhandasiya lit Book in knowledge of engineering tricks in 1206 where he described 100 mechanical devices some 80 of which are trick vessels of various kinds along with instructions on how to construct them


==========

The only biographical information known about him is contained in his famed Book of Knowledge of Ingenious Mechanical Devices Like his father before him he served as chief engineer at the Artuklu Palace the residence of the Mardin branch of the Artuqids which ruled across eastern Anatolia as vassals of the Zengid dynasty of Mosul and later of Ayyubid general SaladinAlJazari was part of a tradition of artisans and was thus more a practical engineer than an inventor who appears to have been more interested in the craftsmanship necessary to construct the devices than in the technology which lay behind them and his machines were usually assembled by trial and error rather than by theoretical calculation His Book of Knowledge of Ingenious Mechanical Devices appears to have been quite popular as it appears in a large number of manuscript copies and as he explains repeatedly he only describes devices he has built himself According to Mayr the books style resembles that of a modern doityourself bookSome of his devices were inspired by earlier devices such as one of his monumental water clocks which was based on that of a PseudoArchimedes He also cites the influence of the Banū Mūsā brothers for his fountains alSaghani for the design of a candle clock and Hibatullah ibn alHusayn d 1139 for musical automata AlJazari goes on to describe the improvements he made to the work of his predecessors and describes a number of devices techniques and components that are original innovations which do not appear in the works by his precessors


==========
The most significant aspect of alJazaris machines are the mechanisms components ideas methods and design features which they employ


==========
A camshaft a shaft to which cams are attached was introduced in 1206 by alJazari who employed them in his automata water clocks such as the candle clock and waterraising machines The cam and camshaft also appeared in European mechanisms from the 14th century


==========
The eccentrically mounted handle of the rotary quernstone in fifth century BCE Spain that spread across the Roman Empire constitutes a crank The earliest evidence of a crank and connecting rod mechanism dates to the 3rd century AD Hierapolis sawmill in the Roman Empire The crank also appears in the mid9th century in several of the hydraulic devices described by the Banū Mūsā brothers in their Book of Ingenious DevicesIn 1206 alJazari invented an early crankshaft which he incorporated with a crankconnecting rod mechanism in his twincylinder pump Like the modern crankshaft alJazaris mechanism consisted of a wheel setting several crankpins into motion with the wheels motion being circular and the pins moving backandforth in a straight line The crankshaft described by alJazari transforms continuous rotary motion into a linear reciprocating motion and is central to modern machinery such as the steam engine internal combustion engine and automatic controlsHe used the crankshaft with a connecting rod in two of his waterraising machines the crankdriven saqiya chain pump and the doubleaction reciprocating piston suction pump His water pump also employed the first known crankslider mechanism


==========
English technology historian Donald Hill writes

We see for the first time in alJazaris work several concepts important for both design and construction the lamination of timber to minimize warping the static balancing of wheels the use of wooden templates a kind of pattern the use of paper models to establish designs the calibration of orifices the grinding of the seats and plugs of valves together with emery powder to obtain a watertight fit and the casting of metals in closed mold boxes with sand


==========
AlJazari invented a method for controlling the speed of rotation of a wheel using an escapement mechanism


==========
According to Donald Hill alJazari described several early mechanical controls including a large metal door a combination lock and a lock with four bolts


==========
A segmental gear is a piece for receiving or communicating reciprocating motion from or to a cogwheel consisting of a sector of a circular gear or ring having cogs on the periphery or face Professor Lynn Townsend White Jr wrote

Segmental gears first clearly appear in alJazari in the West they emerge in Giovanni de Dondis astronomical clock finished in 1364 and only with the great Sienese engineer Francesco di Giorgio 1501 did they enter the general vocabulary of European machine design


==========

AlJazari invented five machines for raising water as well as watermills and water wheels with cams on their axle used to operate automata in the 12th and 13th centuries and described them in 1206 It was in these waterraising machines that he introduced his most important ideas and components


==========
The first known use of a crankshaft in a chain pump was in one of alJazaris saqiya machines The concept of minimizing intermittent working is also first implied in one of alJazaris saqiya chain pumps which was for the purpose of maximising the efficiency of the saqiya chain pump AlJazari also constructed a waterraising saqiya chain pump which was run by hydropower rather than manual labour though the Chinese were also using hydropower for chain pumps prior to him Saqiya machines like the ones he described have been supplying water in Damascus since the 13th century up until modern times and were in everyday use throughout the medieval Islamic world


==========
Citing the Byzantine siphon used for discharging Greek fire as an inspiration alJazari went on to describe the first suction pipes suction pump doubleaction pump and made early uses of valves and a crankshaftconnecting rod mechanism when he invented a twincylinder reciprocating piston suction pump This pump is driven by a water wheel which drives through a system of gears an oscillating slotrod to which the rods of two pistons are attached The pistons work in horizontally opposed cylinders each provided with valveoperated suction and delivery pipes The delivery pipes are joined above the centre of the machine to form a single outlet into the irrigation system This waterraising machine had a direct significance for the development of modern engineering This pump is remarkable for three reasons
The first known use of a true suction pipe which sucks fluids into a partial vacuum in a pump
The first application of the doubleacting principle
The conversion of rotary to reciprocating motion via the crankconnecting rod mechanismAlJazaris suction piston pump could lift 136 metres of water with the help of delivery pipes This was more advanced than the suction pumps that appeared in 15thcentury Europe which lacked delivery pipes It was not however any more efficient than the noria commonly used by the Muslim world at the time


==========
alJazari developed the earliest water supply system to be driven by gears and hydropower which was built in 13th century Damascus to supply water to its mosques and Bimaristan hospitals The system had water from a lake turn a scoopwheel and a system of gears which transported jars of water up to a water channel that led to mosques and hospitals in the city


==========
AlJazari built automated moving peacocks driven by hydropower He also invented the earliest known automatic gates which were driven by hydropower created automatic doors as part of one of his elaborate water clocks and invented water wheels with cams on their axle used to operate automata According to Encyclopædia Britannica the Italian Renaissance inventor Leonardo da Vinci may have been influenced by the classic automata of alJazariMark E Rosheim summarizes the advances in robotics made by Muslim engineers especially alJazari as follows

Unlike the Greek designs these Arab examples reveal an interest not only in dramatic illusion but in manipulating the environment for human comfort Thus the greatest contribution the Arabs made besides preserving disseminating and building on the work of the Greeks was the concept of practical application This was the key element that was missing in Greek robotic science
The Arabs on the other hand displayed an interest in creating humanlike machines for practical purposes but lacked like other preindustrial societies any real impetus to pursue their robotic science


==========
One of alJazaris humanoid automata was a waitress that could serve water tea or drinks The drink was stored in a tank with a reservoir from where the drink drips into a bucket and after seven minutes into a cup after which the waitress appears out of an automatic door serving the drink


==========
AlJazari invented a hand washing automaton incorporating a flush mechanism now used in modern flush toilets It features a female humanoid automaton standing by a basin filled with water When the user pulls the lever the water drains and the female automaton refills the basin


==========
AlJazaris peacock fountain was a more sophisticated hand washing device featuring humanoid automata as servants which offer soap and towels Mark E Rosheim describes it as follows
Pulling a plug on the peacocks tail releases water out of the beak as the dirty water from the basin fills the hollow base a float rises and actuates a linkage which makes a servant figure appear from behind a door under the peacock and offer soap When more water is used a second float at a higher level trips and causes the appearance of a second servant figure – with a towel


==========

AlJazaris work described fountains and musical automata in which the flow of water alternated from one large tank to another at hourly or halfhourly intervals This operation was achieved through his innovative use of hydraulic switchingAlJazari created a musical automaton which was a boat with four automatic musicians that floated on a lake to entertain guests at royal drinking parties Professor Noel Sharkey has argued that it is quite likely that it was an early programmable automata and has produced a possible reconstruction of the mechanism it has a programmable drum machine with pegs cams that bump into little levers that operated the percussion The drummer could be made to play different rhythms and different drum patterns if the pegs were moved around


==========
AlJazari constructed a variety of water clocks and candle clocks These included a portable waterpowered scribe clock which was a meter high and half a meter wide reconstructed successfully at the Science Museum in 1976 AlJazari also invented monumental waterpowered astronomical clocks which displayed moving models of the Sun Moon and stars


==========

According to Donald Hill alJazari described the most sophisticated candle clocks known to date Hill described one of alJazaris candle clocks as follows
The candle whose rate of burning was known bore against the underside of the cap and its wick passed through the hole Wax collected in the indentation and could be removed periodically so that it did not interfere with steady burning The bottom of the candle rested in a shallow dish that had a ring on its side connected through pulleys to a counterweight As the candle burned away the weight pushed it upward at a constant speed The automata were operated from the dish at the bottom of the candle No other candle clocks of this sophistication are known

AlJazaris candle clock also included a dial to display the time and for the first time employed a bayonet fitting a fastening mechanism still used in modern times


==========

The elephant clock described by alJazari in 1206 is notable for several innovations It was the first clock in which an automaton reacted after certain intervals of time in this case a humanoid robot striking the cymbal and a mechanical robotic bird chirping and the first water clock to accurately record the passage of the temporal hours to match the uneven length of days throughout the year


==========

AlJazaris largest astronomical clock was the castle clock which was a complex device that was about 11 feet 34 m high and had multiple functions besides timekeeping It included a display of the zodiac and the solar and lunar orbits and an innovative feature of the device was a pointer in the shape of the crescent moon which travelled across the top of a gateway moved by a hidden cart and caused automatic doors to open each revealing a mannequin every hour Another innovative feature was the ability to reprogram the length of day and night in order to account for their changes throughout the year
Another feature of the device was five automata musicians who automatically play music when moved by levers operated by a hidden camshaft attached to a water wheel Other components of the castle clock included a main reservoir with a float a float chamber and flow regulator plate and valve trough two pulleys crescent disc displaying the zodiac and two falcon automata dropping balls into vases AlJazaris castle clock is considered to be the earliest programmable analog computer


==========
AlJazari invented water clocks that were driven by both water and weights These included geared clocks and a portable waterpowered scribe clock which was a meter high and half a meter wide The scribe with his pen was synonymous to the hour hand of a modern clock AlJazaris famous waterpowered scribe clock was reconstructed successfully at the Science Museum London in 1976


==========
Alongside his accomplishments as an inventor and engineer alJazari was also an accomplished artist In The Book of Knowledge of Ingenious Mechanical Devices he gave instructions of his inventions and illustrated them using miniature paintings a medieval style of Islamic art

		
		
		
		
		
		


==========
Banū Mūsā
Hero of Alexandria
History of the internal combustion engine
List of inventions in the medieval Islamic world
Islamic Golden Age
Science in the medieval Islamic world
Lists of Muslim scientists and scholars
Arab Agricultural Revolution
Taqi adDin Muhammad ibn Maruf


==========


==========
Ulrich Alertz The Horologium of Hārūn alRashīd Presented to Charlemagne – An Attempt to Identify and Reconstruct the Clock Using the Instructions Given by alJazarī in Zielinski Siegfried Fürlus Eckhard 2010 Variantology On deep time relations of arts sciences and technologies in the ArabicIslamic world and beyond Distributed Art Pub Incorporated ISBN 9783865607324
Beckwith Guy V 1 October 1997 Readings in Technology and Civilization Pearson Custom Publishing ISBN 9780536005793
ClausPeter Haase Modest Variations—Theoretical Tradition and Practical Innovation in the Mechanical Arts from Antiquity to the Arab Middle Ages in Zielinski Siegfried Fürlus Eckhard 2010 Variantology On deep time relations of arts sciences and technologies in the ArabicIslamic world and beyond Distributed Art Pub Incorporated ISBN 9783865607324
alHassan Ahmad Y Hill Donald 1992 Islamic Technology An Illustrated History Cambridge University Press ISBN 9780521422390
Hill Donald 2012 The Book of Knowledge of Ingenious Mechanical Devices Kitāb fī ma rifat alḥiyal alhandasiyya Springer Science  Business Media ISBN 9789401025737
Hill Donald 2013 A History of Engineering in Classical and Medieval Times Routledge ISBN 9781317761570
Hill Donald 1998 Studies in Medieval Islamic Technology From Philo to AlJazarī from Alexandria to Diyār Bakr Ashgate ISBN 9780860786061
George Saliba Blurred Edges—At the Intersection of Science Culture and Art in Zielinski Siegfried Fürlus Eckhard 2010 Variantology On deep time relations of arts sciences and technologies in the ArabicIslamic world and beyond Distributed Art Pub Incorporated ISBN 9783865607324
Amnon Shiloah The ParadigmaticIndividualistic Approach of Arab Musical Creativeness in Zielinski Siegfried Fürlus Eckhard 2010 Variantology On deep time relations of arts sciences and technologies in the ArabicIslamic world and beyond Distributed Art Pub Incorporated ISBN 9783865607324
Eilhard Wiedemann On Musical Automata in Zielinski Siegfried Fürlus Eckhard 2010 Variantology On deep time relations of arts sciences and technologies in the ArabicIslamic world and beyond Distributed Art Pub Incorporated ISBN 9783865607324


==========
Hill Donald R 2008 197080 Aljazarī Badīʿ Alzamān Abūlʿizz Ismāʿīl Ibn Alrazzāz Complete Dictionary of Scientific Biography Encyclopediacom


==========
A digital manuscript edition of The Book of Knowledge of Ingenious Mechanical Devices Direct link
How Islamic inventors changed the world article in The Independent
Al Jazaris Book  The Book Of Knowledge Of Ingenious Mechanical DevicesAlan Curtis Kay born May 17 1940 is an American computer scientist He has been elected a Fellow of the American Academy of Arts and Sciences the National Academy of Engineering and the Royal Society of Arts He is best known for his pioneering work on objectoriented programming and windowing graphical user interface design
He is the president of the Viewpoints Research Institute and an adjunct professor of computer science at the University of California Los Angeles He is also on the advisory board of TTIVanguard Until mid2005 he was a senior fellow at HP Labs a visiting professor at Kyoto University and an adjunct professor at the Massachusetts Institute of Technology MITKay is also a former professional jazz guitarist composer and theatrical designer and an amateur classical pipe organist


==========
In an interview on education in America with the Davis Group Ltd Kay said

I had the fortune or misfortune to learn how to read fluently starting at the age of three So I had read maybe 150 books by the time I hit 1st grade And I already knew that the teachers were lying to me
Originally from Springfield Massachusetts Kays family relocated several times due to his fathers career in physiology before ultimately settling in the New York metropolitan area when he was nine
He attended the prestigious Brooklyn Technical High School where he was suspended due to insubordination in his senior year Having already accumulated enough credits to graduate Kay then attended Bethany College in Bethany West Virginia He majored in biology and minored in mathematics before he was asked to leave by the administration for protesting the institutions Jewish quota
Thereafter Kay taught guitar in Denver Colorado for a year and hastily enlisted in the United States Air Force when the local draft board inquired about his nonstudent status Assigned as a computer programmer a rare billet dominated by women due to the secretarial connotations of the field in the era after passing an aptitude test he devised an early crossplatform file transfer system
Following his discharge Kay enrolled at the University of Colorado Boulder earning a bachelors degree in mathematics and molecular biology in 1966 Before and during this time he worked as a professional jazz guitarist During his studies at CU he wrote the music for an adaptation of The Hobbit and other campus theatricalsIn the autumn of 1966 he began graduate school at the University of Utah College of Engineering He earned an MS in electrical engineering in 1968 before taking his PhD in computer science in 1969 His doctoral dissertation FLEX A Flexible Extendable Language described the invention of a computer language known as FLEX While there he worked with fathers of computer graphics David C Evans who had been recently recruited from the University of California Berkeley to start Utahs computer science department and Ivan Sutherland best known for writing such pioneering programs as Sketchpad Their mentorship greatly inspired Kays evolving views on objects and programming As he grew busier with DARPA research he ended his musical career
In 1968 he met Seymour Papert and learned of the Logo programming language a dialect of Lisp optimized for educational purposes This led him to learn of the work of Jean Piaget Jerome Bruner Lev Vygotsky and of constructionist learning further influencing his professional orientation
Leaving Utah as an associate professor of computer science in 1969 Kay became a visiting researcher at the Stanford Artificial Intelligence Laboratory in anticipation of accepting a professorship at Carnegie Mellon University Instead in 1970 he joined the Xerox PARC research staff in Palo Alto California Throughout the decade he developed prototypes of networked workstations using the programming language Smalltalk These inventions were later commercialized by Apple in their Lisa and Macintosh computers
Kay is one of the fathers of the idea of objectoriented programming which he named along with some colleagues at PARC Some of the original objectoriented concepts including the use of the words object and class had been developed for Simula 67 at the Norwegian Computing Center Later he said

Im sorry that I long ago coined the term objects for this topic because it gets many people to focus on the  lesser idea The big idea is messaging
While at PARC Kay conceived the Dynabook concept a key progenitor of laptop and tablet computers and the ebook He is also the architect of the modern overlapping windowing graphical user interface GUI Because the Dynabook was conceived as an educational platform Kay is considered to be one of the first researchers into mobile learning many features of the Dynabook concept have been adopted in the design of the One Laptop Per Child educational platform with which Kay is actively involved
The field of computing is awaiting new revolution to happen according to Kay in which educational communities parents and children will not see in it a set of tools invented by Douglas Engelbart but a medium in the Marshall McLuhan sense He wrote

As with Simulas leading to OOP this encounter finally hit me with what the destiny of personal computing really was going to be Not a personal dynamic vehicle as in Engelbarts metaphor opposed to the IBM railroads but something much more profound a personal dynamic medium With a vehicle one could wait until high school and give drivers ed but if it was a medium it had to extend into the world of childhood


==========
From 1981 to 1984 Kay was Ataris Chief Scientist He became an Apple Fellow in 1984 Following the closure of the companys Advanced Technology Group in 1997 he was recruited by his friend Bran Ferren head of research and development at Disney to join Walt Disney Imagineering as a Disney Fellow He remained there until Ferren left to start Applied Minds Inc with Imagineer Danny Hillis leading to the cessation of the Fellows program In 2001 he founded Viewpoints Research Institute a nonprofit organization dedicated to children learning and advanced software development  For its first ten years Kay and his Viewpoints group were based at Applied Minds in Glendale California where he and Ferren continued to work together on various projects Kay was also a Senior Fellow at HewlettPackard until HP disbanded the Advanced Software Research Team on July 20 2005
Kay taught a Fall 2011 class Powerful Ideas Useful Tools to Understand the World at New York Universitys Interactive Telecommunications Program ITP along with fulltime ITP faculty member Nancy Hechinger The goal of the class was to devise new forms of teachinglearning based on fundamental powerful concepts rather than traditional rote learning


==========
In December 1995 while still at Apple Kay collaborated with many others to start the open source Squeak version of Smalltalk and he continues to work on it As part of this effort in November 1996 his team began research on what became the Etoys system More recently he started along with David A Smith David P Reed Andreas Raab Rick McGeer Julian Lombardi and Mark McCahill the Croquet Project an open source networked 2D and 3D environment for collaborative work


==========
In 2001 it became clear that the Etoy architecture in Squeak had reached its limits in what the Morphic interface infrastructure could do Andreas Raab was a researcher working in Kays group then at HewlettPackard He proposed defining a script process and providing a default scheduling mechanism that avoids several more general problems The result was a new user interface proposed to replace the Squeak Morphic user interface in the future Tweak added mechanisms of islands asynchronous messaging players and costumes language extensions projects and tile scripting Its underlying object system is classbased but to users during programming it acts like it is prototypebased Tweak objects are created and run in Tweak project windows


==========
In November 2005 at the World Summit on the Information Society the MIT research laboratories unveiled a new laptop computer for educational use around the world It has many names the 100 Laptop the One Laptop per Child program the Childrens Machine and the XO1 The program was begun and is sustained by Kays friend Nicholas Negroponte and is based on Kays Dynabook ideal Kay is a prominent codeveloper of the computer focusing on its educational software using Squeak and Etoys


==========
Kay has lectured extensively on the idea that the computer revolution is very new and all of the good ideas have not been universally implemented  Lectures at OOPSLA 1997 conference and his ACM Turing award talk entitled The Computer Revolution Hasnt Happened Yet were informed by his experiences with Sketchpad Simula Smalltalk and the bloated code of commercial software
On August 31 2006 Kays proposal to the United States National Science Foundation NSF was granted thus funding Viewpoints Research Institute for several years The proposal title was STEPS Toward the Reinvention of Programming A compact and Practical Model of Personal Computing as a Selfexploratorium A sense of what Kay is trying to do comes from this quote from the abstract of a seminar on this given at Intel Research Labs Berkeley The conglomeration of commercial and most open source software consumes in the neighborhood of several hundreds of millions of lines of code these days We wonder how small could be an understandable practical Model T design that covers this functionality 1M lines of code 200K LOC 100K LOC 20K LOC


==========
Alan Kay has received many awards and honors Among them

2001 UdK 01Award in Berlin Germany for pioneering the GUI JD Warnier Prix DInformatique NEC CC Prize
2002 Telluride Tech Festival Award of Technology in Telluride Colorado
2003 ACM Turing Award For pioneering many of the ideas at the root of contemporary objectoriented programming languages leading the team that developed Smalltalk and for fundamental contributions to personal computing
2004 Kyoto Prize Charles Stark Draper Prize with Butler W Lampson Robert W Taylor and Charles P Thacker
2012 UPE Abacus Award awarded to individuals who have provided extensive support and leadership for studentrelated activities in the computing and information disciplines
Honorary doctorates
2002 Kungliga Tekniska Högskolan Royal Institute of Technology in Stockholm
2005 Georgia Institute of Technology
2005 Columbia College Chicago awarded Doctor of Humane Letters Honoris Causa
2007 Laurea Honoris Causa in Informatica Università di Pisa Italy
2008 University of Waterloo
2010 Universidad de Murcia
2017 University of Edinburgh
Honorary Professor Berlin University of the Arts
Elected fellow of
American Academy of Arts and Sciences
National Academy of Engineering
Royal Society of Arts
1999 Computer History Museum for his fundamental contributions to personal computing and humancomputer interface development
2008 Association for Computing Machinery For fundamental contributions to personal computing and objectoriented programming
2011 Hasso Plattner InstituteHis other honors include the JD Warnier Prix dInformatique the ACM Systems Software Award the NEC Computers  Communication Foundation Prize the Funai Foundation Prize the Lewis Branscomb Technology Award and the ACM SIGCSE Award for Outstanding Contributions to Computer Science Education


==========
List of pioneers in computer science


==========


==========

Viewpoints Research Institute
Alan Kay at TEDIn mathematics and computer science an algorithm  listen is a set of instructions typically to solve a class of problems or perform a computation Algorithms are unambiguous specifications for performing calculation data processing automated reasoning and other tasks
As an effective method an algorithm can be expressed within a finite amount of space and time and in a welldefined formal language for calculating a function Starting from an initial state and initial input perhaps empty the instructions describe a computation that when executed proceeds through a finite number of welldefined successive states eventually producing output and terminating at a final ending state The transition from one state to the next is not necessarily deterministic some algorithms known as randomized algorithms incorporate random inputThe concept of algorithm has existed for centuries Greek mathematicians used algorithms in the sieve of Eratosthenes for finding prime numbers and the Euclidean algorithm for finding the greatest common divisor of two numbersThe word algorithm itself is derived from the 9th century mathematician Muḥammad ibn Mūsā alKhwārizmī Latinized Algoritmi A partial formalization of what would become the modern concept of algorithm began with attempts to solve the Entscheidungsproblem decision problem posed by David Hilbert in 1928 Later formalizations were framed as attempts to define effective calculability or effective method Those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930 1934 and 1935 Alonzo Churchs lambda calculus of 1936 Emil Posts Formulation 1 of 1936 and Alan Turings Turing machines of 1936–37 and 1939


==========
The word algorithm has its roots in Latinizing the name of Muhammad ibn Musa alKhwarizmi in a first step to algorismus AlKhwārizmī Arabic الخوارزمي‎  Persian خوارزمی‎ c 780–850 was a Persian  mathematician astronomer geographer and scholar in the House of Wisdom in Baghdad whose name means the native of Khwarazm a region that was part of Greater Iran and is now in UzbekistanAbout 825 alKhwarizmi wrote an Arabic language treatise on the Hindu–Arabic numeral system which was translated into Latin during the 12th century under the title Algoritmi de numero Indorum This title means Algoritmi on the numbers of the Indians where Algoritmi was the translators Latinization of AlKhwarizmis name AlKhwarizmi was the most widely read mathematician in Europe in the late Middle Ages primarily through another of his books the Algebra In late medieval Latin algorismus English algorism the corruption of his name simply meant the decimal number system In the 15th century under the influence of the Greek word ἀριθμός number cf arithmetic the Latin word was altered to algorithmus and the corresponding English term algorithm is first attested in the 17th century the modern sense was introduced in the 19th centuryIn English it was first used in about 1230 and then by Chaucer in 1391 English adopted the French term but it wasnt until the late 19th century that algorithm took on the meaning that it has in modern English
Another early use of the word is from 1240 in a manual titled Carmen de Algorismo composed by Alexandre de Villedieu It begins thus

Haec algorismus ars praesens dicitur in qua  Talibus Indorum fruimur bis quinque figuris

which translates as

Algorism is the art by which at present we use those Indian figures which number two times five

The poem is a few hundred lines long and summarizes the art of calculating with the new style of Indian dice or Talibus Indorum or Hindu numerals


==========

An informal definition could be a set of rules that precisely defines a sequence of operations which would include all computer programs including programs that do not perform numeric calculations and for example any prescribed bureaucratic procedure
Generally a program is only an algorithm if it stops eventuallyA prototypical example of an algorithm is the Euclidean algorithm to determine the maximum common divisor of two integers an example there are others is described by the flowchart above and as an example in a later section
Boolos Jeffrey  1974 1999 offer an informal meaning of the word in the following quotation

No human being can write fast enough or long enough or small enough†  †smaller and smaller without limit youd be trying to write on molecules on atoms on electrons to list all members of an enumerably infinite set by writing out their names one after another in some notation But humans can do something equally useful in the case of certain enumerably infinite sets They can give explicit instructions for determining the nth member of the set for arbitrary finite n Such instructions are to be given quite explicitly in a form in which they could be followed by a computing machine or by a human who is capable of carrying out only very elementary operations on symbols
An enumerably infinite set is one whose elements can be put into onetoone correspondence with the integers Thus Boolos and Jeffrey are saying that an algorithm implies instructions for a process that creates output integers from an arbitrary input integer or integers that in theory can be arbitrarily large Thus an algorithm can be an algebraic equation such as y = m + n – two arbitrary input variables m and n that produce an output y But various authors attempts to define the notion indicate that the word implies much more than this something on the order of for the addition example

Precise instructions in language understood by the computer for a fast efficient good process that specifies the moves of the computer machine or human equipped with the necessary internally contained information and capabilities to find decode and then process arbitrary input integerssymbols m and n symbols + and =  and effectively produce in a reasonable time outputinteger y at a specified place and in a specified formatThe concept of algorithm is also used to define the notion of decidability That notion is central for explaining how formal systems come into being starting from a small set of axioms and rules In logic the time that an algorithm requires to complete cannot be measured as it is not apparently related to our customary physical dimension From such uncertainties that characterize ongoing work stems the unavailability of a definition of algorithm that suits both concrete in some sense and abstract usage of the term


==========
Algorithms are essential to the way computers process data Many computer programs contain algorithms that detail the specific instructions a computer should perform in a specific order to carry out a specified task such as calculating employees paychecks or printing students report cards Thus an algorithm can be considered to be any sequence of operations that can be simulated by a Turingcomplete system Authors who assert this thesis include Minsky 1967 Savage 1987 and Gurevich 2000

 Minsky But we will also maintain with Turing  that any procedure which could naturally be called effective can in fact be realized by a simple machine Although this may seem extreme the arguments  in its favor are hard to refute
 Gurevich Turings informal argument in favor of his thesis justifies a stronger thesis every algorithm can be simulated by a Turing machine  according to Savage 1987 an algorithm is a computational process defined by a Turing machine
Turing machines can define computational processes that do not terminate The informal definitions of algorithms generally require that the algorithm always terminates This requirement renders the task of deciding whether a formal procedure is an algorithm impossible in the general case This is because of a major theorem of Computability Theory known as the Halting Problem
Typically when an algorithm is associated with processing information data can be read from an input source written to an output device and stored for further processing Stored data are regarded as part of the internal state of the entity performing the algorithm In practice the state is stored in one or more data structures
For some such computational process the algorithm must be rigorously defined specified in the way it applies in all possible circumstances that could arise That is any conditional steps must be systematically dealt with casebycase the criteria for each case must be clear and computable
Because an algorithm is a precise list of precise steps the order of computation is always crucial to the functioning of the algorithm Instructions are usually assumed to be listed explicitly and are described as starting from the top and going down to the bottom an idea that is described more formally by flow of control
So far this discussion of the formalization of an algorithm has assumed the premises of imperative programming This is the most common conception and it attempts to describe a task in discrete mechanical means Unique to this conception of formalized algorithms is the assignment operation setting the value of a variable It derives from the intuition of memory as a scratchpad There is an example below of such an assignment
For some alternate conceptions of what constitutes an algorithm see functional programming and logic programming


==========
Algorithms can be expressed in many kinds of notation including natural languages pseudocode flowcharts drakoncharts programming languages or control tables processed by interpreters Natural language expressions of algorithms tend to be verbose and ambiguous and are rarely used for complex or technical algorithms Pseudocode flowcharts drakoncharts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in natural language statements Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer but are often used as a way to define or document algorithms
There is a wide variety of representations possible and one can express a given Turing machine program as a sequence of machine tables see more at finitestate machine state transition table and control table as flowcharts and drakoncharts see more at state diagram or as a form of rudimentary machine code or assembly code called sets of quadruples see more at Turing machine
Representations of algorithms can be classed into three accepted levels of Turing machine description
1 Highlevel description
prose to describe an algorithm ignoring the implementation details At this level we do not need to mention how the machine manages its tape or head
2 Implementation description
prose used to define the way the Turing machine uses its head and the way that it stores data on its tape At this level we do not give details of states or transition function
3 Formal description
Most detailed lowest level gives the Turing machines state tableFor an example of the simple algorithm Add m+n described in all three levels see AlgorithmExamples


==========

Algorithm design refers to a method or mathematical process for problemsolving and engineering algorithms The design of algorithms is part of many solution theories of operation research such as dynamic programming and divideandconquer Techniques for designing and implementing algorithm designs are also called algorithm design patterns such as the template method pattern and decorator pattern
One of the most important aspects of algorithm design is creating an algorithm that has an efficient runtime also known as its Big O
Typical steps in the development of algorithms

Problem definition
Development of a model
Specification of the algorithm
Designing an algorithm
Checking the correctness of the algorithm
Analysis of algorithm
Implementation of algorithm
Program testing
Documentation preparation


==========

Most algorithms are intended to be implemented as computer programs However algorithms are also implemented by other means such as in a biological neural network for example the human brain implementing arithmetic or an insect looking for food in an electrical circuit or in a mechanical device


==========

In computer systems an algorithm is basically an instance of logic written in software by software developers to be effective for the intended target computers to produce output from given perhaps null input An optimal algorithm even running in old hardware would produce faster results than a nonoptimal higher time complexity algorithm for the same purpose running in more efficient hardware that is why algorithms like computer hardware are considered technology
Elegant compact programs good fast programs  The notion of simplicity and elegance appears informally in Knuth and precisely in Chaitin

Knuth   we want good algorithms in some loosely defined aesthetic sense One criterion  is the length of time taken to perform the algorithm  Other criteria are adaptability of the algorithm to computers its simplicity and elegance etcChaitin   a program is elegant by which I mean that its the smallest possible program for producing the output that it doesChaitin prefaces his definition with Ill show you cant prove that a program is elegant—such a proof would solve the Halting problem ibid
Algorithm versus function computable by an algorithm For a given function multiple algorithms may exist This is true even without expanding the available instruction set available to the programmer Rogers observes that It is  important to distinguish between the notion of algorithm ie procedure and the notion of function computable by algorithm ie mapping yielded by procedure The same function may have several different algorithmsUnfortunately there may be a tradeoff between goodness speed and elegance compactness—an elegant program may take more steps to complete a computation than one less elegant An example that uses Euclids algorithm appears below
Computers and computors models of computation A computer or human computor is a restricted type of machine a discrete deterministic mechanical device that blindly follows its instructions Melzaks and Lambeks primitive models reduced this notion to four elements i discrete distinguishable locations ii discrete indistinguishable counters iii an agent and iv a list of instructions that are effective relative to the capability of the agentMinsky describes a more congenial variation of Lambeks abacus model in his Very Simple Bases for Computability Minskys machine proceeds sequentially through its five or six depending on how one counts instructions unless either a conditional IF–THEN GOTO or an unconditional GOTO changes program flow out of sequence Besides HALT Minskys machine includes three assignment replacement substitution operations ZERO eg the contents of location replaced by 0 L ← 0 SUCCESSOR eg L ← L+1 and DECREMENT eg L ← L − 1 Rarely must a programmer write code with such a limited instruction set But Minsky shows as do Melzak and Lambek that his machine is Turing complete with only four general types of instructions conditional GOTO unconditional GOTO assignmentreplacementsubstitution and HALT  However a few different assignment instructions eg DECREMENT INCREMENT and ZEROCLEAREMPTY for a Minsky machine are also required for Turingcompleteness their exact specification is somewhat up to the designer The unconditional GOTO is a convenience it can be constructed by initializing a dedicated location to zero eg the instruction  Z ← 0  thereafter the instruction IF Z=0 THEN GOTO xxx is unconditional
Simulation of an algorithm computer computor language Knuth advises the reader that the best way to learn an algorithm is to try it    immediately take pen and paper and work through an example But what about a simulation or execution of the real thing The programmer must translate the algorithm into a language that the simulatorcomputercomputor can effectively execute Stone gives an example of this when computing the roots of a quadratic equation the computor must know how to take a square root If they dont then the algorithm to be effective must provide a set of rules for extracting a square rootThis means that the programmer must know a language that is effective relative to the target computing agent computercomputor
But what model should be used for the simulation Van Emde Boas observes even if we base complexity theory on abstract instead of concrete machines arbitrariness of the choice of a model remains It is at this point that the notion of simulation enters When speed is being measured the instruction set matters For example the subprogram in Euclids algorithm to compute the remainder would execute much faster if the programmer had a modulus instruction available rather than just subtraction or worse just Minskys decrement
Structured programming canonical structures Per the Church–Turing thesis any algorithm can be computed by a model known to be Turing complete and per Minskys demonstrations Turing completeness requires only four instruction types—conditional GOTO unconditional GOTO assignment HALT Kemeny and Kurtz observe that while undisciplined use of unconditional GOTOs and conditional IFTHEN GOTOs can result in spaghetti code a programmer can write structured programs using only these instructions on the other hand it is also possible and not too hard to write badly structured programs in a structured language Tausworthe augments the three BöhmJacopini canonical structures SEQUENCE IFTHENELSE and WHILEDO with two more DOWHILE and CASE An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical inductionCanonical flowchart symbols The graphical aide called a flowchart offers a way to describe and document an algorithm and a computer program of one Like the program flow of a Minsky machine a flowchart always starts at the top of a page and proceeds down Its primary symbols are only four the directed arrow showing program flow the rectangle SEQUENCE GOTO the diamond IFTHENELSE and the dot ORtie The Böhm–Jacopini canonical structures are made of these primitive shapes Substructures can nest in rectangles but only if a single exit occurs from the superstructure The symbols and their use to build the canonical structures are shown in the diagram


==========


==========

One of the simplest algorithms is to find the largest number in a list of numbers of random order Finding the solution requires looking at every number in the list From this follows a simple algorithm which can be stated in a highlevel description in English prose as
Highlevel description

If there are no numbers in the set then there is no highest number
Assume the first number in the set is the largest number in the set
For each remaining number in the set if this number is larger than the current largest number consider this number to be the largest number in the set
When there are no numbers left in the set to iterate over consider the current largest number to be the largest number of the setQuasiformal description
Written in prose but much closer to the highlevel language of a computer program the following is the more formal coding of the algorithm in pseudocode or pidgin code


==========

Euclids algorithm to compute the greatest common divisor GCD to two numbers appears as Proposition II in Book VII Elementary Number Theory of his Elements Euclid poses the problem thus Given two numbers not prime to one another to find their greatest common measure He defines A number to be a multitude composed of units a counting number a positive integer not including zero To measure is to place a shorter measuring length s successively q times along longer length l until the remaining portion r is less than the shorter length s In modern words remainder r = l − q×s q being the quotient or remainder r is the modulus the integerfractional part left over after the divisionFor Euclids method to succeed the starting lengths must satisfy two requirements i the lengths must not be zero AND ii the subtraction must be “proper” ie a test must guarantee that the smaller of the two numbers is subtracted from the larger alternately the two can be equal so their subtraction yields zero
Euclids original proof adds a third requirement the two lengths must not be prime to one another Euclid stipulated this so that he could construct a reductio ad absurdum proof that the two numbers common measure is in fact the greatest While Nicomachus algorithm is the same as Euclids when the numbers are prime to one another it yields the number 1 for their common measure So to be precise the following is really Nicomachus algorithm


============
Only a few instruction types are required to execute Euclids algorithm—some logical tests conditional GOTO unconditional GOTO assignment replacement and subtraction

A location is symbolized by upper case letters eg S A etc
The varying quantity number in a location is written in lower case letters and usually associated with the locations name For example location L at the start might contain the number l = 3009


============

The following algorithm is framed as Knuths fourstep version of Euclids and Nicomachus but rather than using division to find the remainder it uses successive subtractions of the shorter length s from the remaining length r until r is less than s The highlevel description shown in boldface is adapted from Knuth 19732–4
INPUT

1 Into two locations L and S put the numbers l and s that represent the two lengths
  INPUT L S
2 Initialize R make the remaining length r equal to the startinginitialinput length l
  R ← L

E0 Ensure r ≥ s

3 Ensure the smaller of the two numbers is in S and the larger in R
  IF R  S THEN
    the contents of L is the larger number so skip over the exchangesteps 4 5 and 6
    GOTO step 6
  ELSE
    swap the contents of R and S
4   L ← R this first step is redundant but is useful for later discussion
5   R ← S
6   S ← L

E1 Find remainder Until the remaining length r in R is less than the shorter length s in S repeatedly subtract the measuring number s in S from the remaining length r in R

7 IF S  R THEN
    done measuring so
    GOTO 10
  ELSE
    measure again
8   R ← R − S
9   Remainderloop
    GOTO 7

E2 Is the remainder zero EITHER i the last measure was exact the remainder in R is zero and the program can halt OR ii the algorithm must continue the last measure left a remainder in R less than measuring number in S

10 IF R = 0 THEN
     done so
     GOTO step 15
   ELSE
     CONTINUE TO step 11

E3 Interchange s and r The nut of Euclids algorithm Use remainder r to measure what was previously smaller number s L serves as a temporary location

11  L ← R
12  R ← S
13  S ← L
14  Repeat the measuring process
    GOTO 7

OUTPUT

15 Done S contains the greatest common divisor
   PRINT S

DONE

16 HALT END STOP


============
The following version of Euclids algorithm requires only six core instructions to do what thirteen are required to do by Inelegant worse Inelegant requires more types of instructions The flowchart of Elegant can be found at the top of this article In the unstructured Basic language the steps are numbered and the instruction LET  =  is the assignment instruction symbolized by ←

How Elegant works In place of an outer Euclid loop Elegant shifts back and forth between two coloops an A  B loop that computes A ← A − B and a B ≤ A loop that computes B ← B − A This works because when at last the minuend M is less than or equal to the subtrahend S  Difference = Minuend − Subtrahend the minuend can become s the new measuring length and the subtrahend can become the new r the length to be measured in other words the sense of the subtraction reverses
The following version can be used with Object Oriented languages


==========
Does an algorithm do what its author wants it to do A few test cases usually give some confidence in the core functionality But tests are not enough For test cases one source uses 3009 and 884 Knuth suggested 40902 24140 Another interesting case is the two relatively prime numbers 14157 and 5950
But exceptional cases must be identified and tested Will Inelegant perform properly when R  S S  R R = S Ditto for Elegant B  A A  B A = B Yes to all What happens when one number is zero both numbers are zero Inelegant computes forever in all cases Elegant computes forever when A = 0 What happens if negative numbers are entered Fractional numbers If the input numbers ie the domain of the function computed by the algorithmprogram is to include only positive integers including zero then the failures at zero indicate that the algorithm and the program that instantiates it is a partial function rather than a total function A notable failure due to exceptions is the Ariane 5 Flight 501 rocket failure June 4 1996
Proof of program correctness by use of mathematical induction Knuth demonstrates the application of mathematical induction to an extended version of Euclids algorithm and he proposes a general method applicable to proving the validity of any algorithm Tausworthe proposes that a measure of the complexity of a program be the length of its correctness proof


==========
Elegance compactness versus goodness speed With only six core instructions Elegant is the clear winner compared to Inelegant at thirteen instructions However Inelegant is faster it arrives at HALT in fewer steps Algorithm analysis indicates why this is the case Elegant does two conditional tests in every subtraction loop whereas Inelegant only does one As the algorithm usually requires many loopthroughs on average much time is wasted doing a B = 0 test that is needed only after the remainder is computed
Can the algorithms be improved Once the programmer judges a program fit and effective—that is it computes the function intended by its author—then the question becomes can it be improved
The compactness of Inelegant can be improved by the elimination of five steps But Chaitin proved that compacting an algorithm cannot be automated by a generalized algorithm rather it can only be done heuristically ie by exhaustive search examples to be found at Busy beaver trial and error cleverness insight application of inductive reasoning etc Observe that steps 4 5 and 6 are repeated in steps 11 12 and 13 Comparison with Elegant provides a hint that these steps together with steps 2 and 3 can be eliminated This reduces the number of core instructions from thirteen to eight which makes it more elegant than Elegant at nine steps
The speed of Elegant can be improved by moving the B=0 test outside of the two subtraction loops This change calls for the addition of three instructions B = 0 A = 0 GOTO Now Elegant computes the examplenumbers faster whether this is always the case for any given A B and R S would require a detailed analysis


==========

It is frequently important to know how much of a particular resource such as time or storage is theoretically required for a given algorithm Methods have been developed for the analysis of algorithms to obtain such quantitative answers estimates for example the sorting algorithm above has a time requirement of On using the big O notation with n as the length of the list At all times the algorithm only needs to remember two values the largest number found so far and its current position in the input list Therefore it is said to have a space requirement of O1 if the space required to store the input numbers is not counted or On if it is counted
Different algorithms may complete the same task with a different set of instructions in less or more time space or effort than others For example a binary search algorithm with cost Olog n  outperforms a sequential search cost On  when used for table lookups on sorted lists or arrays


==========

The analysis and study of algorithms is a discipline of computer science and is often practiced abstractly without the use of a specific programming language or implementation In this sense algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation Usually pseudocode is used for analysis as it is the simplest and most general representation However ultimately most algorithms are usually implemented on particular hardwaresoftware platforms and their algorithmic efficiency is eventually put to the test using real code For the solution of a one off problem the efficiency of a particular algorithm may not have significant consequences unless n is extremely large but for algorithms designed for fast interactive commercial or long life scientific usage it may be critical Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign
Empirical testing is useful because it may uncover unexpected interactions that affect performance Benchmarks may be used to compare beforeafter potential improvements to an algorithm after program optimization
Empirical tests cannot replace formal analysis though and are not trivial to perform in a fair manner


==========

To illustrate the potential improvements possible even in wellestablished algorithms a recent significant innovation relating to FFT algorithms used heavily in the field of image processing can decrease processing time up to 1000 times for applications like medical imaging In general speed improvements depend on special properties of the problem which are very common in practical applications Speedups of this magnitude enable computing devices that make extensive use of image processing like digital cameras and medical equipment to consume less power


==========
There are various ways to classify algorithms each with its own merits


==========
One way to classify algorithms is by implementation means

Recursion
A recursive algorithm is one that invokes makes reference to itself repeatedly until a certain condition also known as termination condition matches which is a method common to functional programming Iterative algorithms use repetitive constructs like loops and sometimes additional data structures like stacks to solve the given problems Some problems are naturally suited for one implementation or the other For example towers of Hanoi is well understood using recursive implementation Every recursive version has an equivalent but possibly more or less complex iterative version and vice versa
Logical
An algorithm may be viewed as controlled logical deduction This notion may be expressed as Algorithm = logic + control The logic component expresses the axioms that may be used in the computation and the control component determines the way in which deduction is applied to the axioms This is the basis for the logic programming paradigm In pure logic programming languages the control component is fixed and algorithms are specified by supplying only the logic component The appeal of this approach is the elegant semantics a change in the axioms produces a welldefined change in the algorithm
Serial parallel or distributed
Algorithms are usually discussed with the assumption that computers execute one instruction of an algorithm at a time Those computers are sometimes called serial computers An algorithm designed for such an environment is called a serial algorithm as opposed to parallel algorithms or distributed algorithms Parallel algorithms take advantage of computer architectures where several processors can work on a problem at the same time whereas distributed algorithms utilize multiple machines connected with a computer network Parallel or distributed algorithms divide the problem into more symmetrical or asymmetrical subproblems and collect the results back together The resource consumption in such algorithms is not only processor cycles on each processor but also the communication overhead between the processors Some sorting algorithms can be parallelized efficiently but their communication overhead is expensive Iterative algorithms are generally parallelizable Some problems have no parallel algorithms and are called inherently serial problems
Deterministic or nondeterministic
Deterministic algorithms solve the problem with exact decision at every step of the algorithm whereas nondeterministic algorithms solve problems via guessing although typical guesses are made more accurate through the use of heuristics
Exact or approximate
While many algorithms reach an exact solution approximation algorithms seek an approximation that is closer to the true solution The approximation can be reached by either using a deterministic or a random strategy Such algorithms have practical value for many hard problems One of the examples of an approximate algorithm is the Knapsack problem where there is a set of given items Its goal is to pack the knapsack to get the maximum total value Each item has some weight and some value Total weight that can be carried is no more than some fixed number X So the solution must consider weights of items as well as their value
Quantum algorithm
They run on a realistic model of quantum computation The term is usually used for those algorithms which seem inherently quantum or use some essential feature of Quantum computing such as quantum superposition or quantum entanglement


==========
Another way of classifying algorithms is by their design methodology or paradigm There is a certain number of paradigms each different from the other Furthermore each of these categories includes many different types of algorithms Some common paradigms are

Bruteforce or exhaustive search
This is the naive method of trying every possible solution to see which is best
Divide and conquer
A divide and conquer algorithm repeatedly reduces an instance of a problem to one or more smaller instances of the same problem usually recursively until the instances are small enough to solve easily One such example of divide and conquer is merge sorting Sorting can be done on each segment of data after dividing data into segments and sorting of entire data can be obtained in the conquer phase by merging the segments A simpler variant of divide and conquer is called a decrease and conquer algorithm that solves an identical subproblem and uses the solution of this subproblem to solve the bigger problem Divide and conquer divides the problem into multiple subproblems and so the conquer stage is more complex than decrease and conquer algorithms An example of a decrease and conquer algorithm is the binary search algorithm
Search and enumeration
Many problems such as playing chess can be modeled as problems on graphs A graph exploration algorithm specifies rules for moving around a graph and is useful for such problems This category also includes search algorithms branch and bound enumeration and backtracking
Randomized algorithm
Such algorithms make some choices randomly or pseudorandomly They can be very useful in finding approximate solutions for problems where finding exact solutions can be impractical see heuristic method below For some of these problems it is known that the fastest approximations must involve some randomness Whether randomized algorithms with polynomial time complexity can be the fastest algorithms for some problems is an open question known as the P versus NP problem There are two large classes of such algorithmsMonte Carlo algorithms return a correct answer with highprobability Eg RP is the subclass of these that run in polynomial time
Las Vegas algorithms always return the correct answer but their running time is only probabilistically bound eg ZPPReduction of complexity
This technique involves solving a difficult problem by transforming it into a betterknown problem for which we have hopefully asymptotically optimal algorithms The goal is to find a reducing algorithm whose complexity is not dominated by the resulting reduced algorithms For example one selection algorithm for finding the median in an unsorted list involves first sorting the list the expensive portion and then pulling out the middle element in the sorted list the cheap portion This technique is also known as transform and conquer
Back tracking
In this approach multiple solutions are built incrementally and abandoned when it is determined that they cannot lead to a valid full solution


==========
For optimization problems there is a more specific classification of algorithms an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following

Linear programming
When searching for optimal solutions to a linear function bound to linear equality and inequality constraints the constraints of the problem can be used directly in producing the optimal solutions There are algorithms that can solve any problem in this category such as the popular simplex algorithm Problems that can be solved with linear programming include the maximum flow problem for directed graphs If a problem additionally requires that one or more of the unknowns must be an integer then it is classified in integer programming A linear programming algorithm can solve such a problem if it can be proved that all restrictions for integer values are superficial ie the solutions satisfy these restrictions anyway In the general case a specialized algorithm or an algorithm that finds approximate solutions is used depending on the difficulty of the problem
Dynamic programming
When a problem shows optimal substructures—meaning the optimal solution to a problem can be constructed from optimal solutions to subproblems—and overlapping subproblems meaning the same subproblems are used to solve many different problem instances a quicker approach called dynamic programming avoids recomputing solutions that have already been computed For example Floyd–Warshall algorithm the shortest path to a goal from a vertex in a weighted graph can be found by using the shortest path to the goal from all adjacent vertices Dynamic programming and memoization go together The main difference between dynamic programming and divide and conquer is that subproblems are more or less independent in divide and conquer whereas subproblems overlap in dynamic programming The difference between dynamic programming and straightforward recursion is in caching or memoization of recursive calls When subproblems are independent and there is no repetition memoization does not help hence dynamic programming is not a solution for all complex problems By using memoization or maintaining a table of subproblems already solved dynamic programming reduces the exponential nature of many problems to polynomial complexity
The greedy method
A greedy algorithm is similar to a dynamic programming algorithm in that it works by examining substructures in this case not of the problem but of a given solution Such algorithms start with some solution which may be given or have been constructed in some way and improve it by making small modifications For some problems they can find the optimal solution while for others they stop at local optima that is at solutions that cannot be improved by the algorithm but are not optimum The most popular use of greedy algorithms is for finding the minimal spanning tree where finding the optimal solution is possible with this method Huffman Tree Kruskal Prim Sollin are greedy algorithms that can solve this optimization problem
The heuristic method
In optimization problems heuristic algorithms can be used to find a solution close to the optimal solution in cases where finding the optimal solution is impractical These algorithms work by getting closer and closer to the optimal solution as they progress In principle if run for an infinite amount of time they will find the optimal solution Their merit is that they can find a solution very close to the optimal solution in a relatively short time Such algorithms include local search tabu search simulated annealing and genetic algorithms Some of them like simulated annealing are nondeterministic algorithms while others like tabu search are deterministic When a bound on the error of the nonoptimal solution is known the algorithm is further categorized as an approximation algorithm


==========

Every field of science has its own problems and needs efficient algorithms Related problems in one field are often studied together Some example classes are search algorithms sorting algorithms merge algorithms numerical algorithms graph algorithms string algorithms computational geometric algorithms combinatorial algorithms medical algorithms machine learning cryptography data compression algorithms and parsing techniques
Fields tend to overlap with each other and algorithm advances in one field may improve those of other sometimes completely unrelated fields For example dynamic programming was invented for optimization of resource consumption in industry but is now used in solving a broad range of problems in many fields


==========

Algorithms can be classified by the amount of time they need to complete compared to their input size

Constant time if the time needed by the algorithm is the same regardless of the input size Eg an access to an array element
Linear time if the time is proportional to the input size Eg the traverse of a list
Logarithmic time if the time is a logarithmic function of the input size Eg binary search algorithm
Polynomial time if the time is a power of the input size Eg the bubble sort algorithm has quadratic time complexity
Exponential time if the time is an exponential function of the input size Eg Bruteforce searchSome problems may have multiple algorithms of differing complexity while other problems might have no algorithms or no known efficient algorithms There are also mappings from some problems to other problems Owing to this it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best possible algorithms for them


==========
The adjective continuous when applied to the word algorithm can mean

An algorithm operating on data that represents continuous quantities even though this data is represented by discrete approximations—such algorithms are studied in numerical analysis or
An algorithm in the form of a differential equation that operates continuously on the data running on an analog computer


==========

Algorithms by themselves are not usually patentable In the United States a claim consisting solely of simple manipulations of abstract concepts numbers or signals does not constitute processes USPTO 2006 and hence algorithms are not patentable as in Gottschalk v Benson However practical applications of algorithms are sometimes patentable For example in Diamond v Diehr the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable The patenting of software is highly controversial and there are highly criticized patents involving algorithms especially data compression algorithms such as Unisys LZW patent
Additionally some cryptographic algorithms have export restrictions see export of cryptography


==========


==========
The earliest  evidence of algorithms is found in the Babylonian mathematics of ancient Mesopotamia modern Iraq A Sumerian clay tablet found in Shuruppak near Baghdad and dated to circa 2500 BC described the earliest division algorithm During the Hammurabi dynasty circa 18001600 BC Babylonian clay tablets described algorithms for computing formulas Algorithms were also used in Babylonian astronomy Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical eventsAlgorithms for arithmetic are also found in ancient Egyptian mathematics dating back to the Rhind Mathematical Papyrus circa 1550 BC Algorithms were later used in ancient Hellenistic mathematics Two examples are the Sieve of Eratosthenes which was described in the Introduction to Arithmetic by Nicomachus and the Euclidean algorithm which was first described in Euclids Elements c 300 BC


==========
Tallymarks To keep track of their flocks their sacks of grain and their money the ancients used tallying accumulating stones or marks scratched on sticks or making discrete symbols in clay Through the Babylonian and Egyptian use of marks and symbols eventually Roman numerals and the abacus evolved Dilson p 16–41 Tally marks appear prominently in unary numeral system arithmetic used in Turing machine and Post–Turing machine computations


==========
Muhammad ibn Mūsā alKhwārizmī a Persian mathematician wrote the Aljabr in the 9th century The terms algorism and algorithm are derived from the name alKhwārizmī while the term algebra is derived from the book Aljabr In Europe the word algorithm was originally used to refer to the sets of rules and techniques used by AlKhwarizmi to solve algebraic equations before later being generalized to refer to any set of rules or techniques This eventually culminated in Leibnizs notion of the calculus ratiocinator ca 1680

A good century and a half ahead of his time Leibniz proposed an algebra of logic an algebra that would specify the rules for manipulating logical concepts in the manner that ordinary algebra specifies the rules for manipulating numbers


==========
The first cryptographic algorithm for deciphering encrypted code was developed by AlKindi a 9thcentury Arab mathematician in A Manuscript On Deciphering Cryptographic Messages He gave the first description of cryptanalysis by frequency analysis the earliest codebreaking algorithm


==========
The clock Bolter credits the invention of the weightdriven clock as The key invention of Europe in the Middle Ages in particular the verge escapement that provides us with the tick and tock of a mechanical clock The accurate automatic machine led immediately to mechanical automata beginning in the 13th century and finally to computational machines—the difference engine and analytical engines of Charles Babbage and Countess Ada Lovelace mid19th century Lovelace is credited with the first creation of an algorithm intended for processing on a computer—Babbages analytical engine the first device considered a real Turingcomplete computer instead of just a calculator—and is sometimes called historys first programmer as a result though a full implementation of Babbages second device would not be realized until decades after her lifetime
Logical machines 1870 – Stanley Jevons logical abacus and logical machine The technical problem was to reduce Boolean equations when presented in a form similar to what is now known as Karnaugh maps Jevons 1880 describes first a simple abacus of slips of wood furnished with pins contrived so that any part or class of the logical combinations can be picked out mechanically  More recently however I have reduced the system to a completely mechanical form and have thus embodied the whole of the indirect process of inference in what may be called a Logical Machine His machine came equipped with certain moveable wooden rods and at the foot are 21 keys like those of a piano etc  With this machine he could analyze a syllogism or any other simple logical argumentThis machine he displayed in 1870 before the Fellows of the Royal Society Another logician John Venn however in his 1881 Symbolic Logic turned a jaundiced eye to this effort I have no high estimate myself of the interest or importance of what are sometimes called logical machines  it does not seem to me that any contrivances at present known or likely to be discovered really deserve the name of logical machines see more at Algorithm characterizations But not to be outdone he too presented a plan somewhat analogous I apprehend to Prof Jevons abacus  And again corresponding to Prof Jevonss logical machine the following contrivance may be described I prefer to call it merely a logicaldiagram machine  but I suppose that it could do very completely all that can be rationally expected of any logical machineJacquard loom Hollerith punch cards telegraphy and telephony – the electromechanical relay Bell and Newell 1971 indicate that the Jacquard loom 1801 precursor to Hollerith cards punch cards 1887 and telephone switching technologies were the roots of a tree leading to the development of the first computers By the mid19th century the telegraph the precursor of the telephone was in use throughout the world its discrete and distinguishable encoding of letters as dots and dashes a common sound By the late 19th century the ticker tape ca 1870s was in use as was the use of Hollerith cards in the 1890 US census Then came the teleprinter ca 1910 with its punchedpaper use of Baudot code on tape
Telephoneswitching networks of electromechanical relays invented 1835 was behind the work of George Stibitz 1937 the inventor of the digital adding device As he worked in Bell Laboratories he observed the burdensome use of mechanical calculators with gears He went home one evening in 1937 intending to test his idea When the tinkering was over Stibitz had constructed a binary adding deviceDavis 2000 observes the particular importance of the electromechanical relay with its two binary states open and closed

It was only with the development beginning in the 1930s of electromechanical calculators using electrical relays that machines were built having the scope Babbage had envisioned


==========
Symbols and rules In rapid succession the mathematics of George Boole 1847 1854 Gottlob Frege 1879 and Giuseppe Peano 1888–1889 reduced arithmetic to a sequence of symbols manipulated by rules Peanos The principles of arithmetic presented by a new method 1888 was the first attempt at an axiomatization of mathematics in a symbolic languageBut Heijenoort gives Frege 1879 this kudos Freges is perhaps the most important single work ever written in logic  in which we see a  formula language that is a lingua characterica a language written with special symbols for pure thought that is free from rhetorical embellishments  constructed from specific symbols that are manipulated according to definite rules The work of Frege was further simplified and amplified by Alfred North Whitehead and Bertrand Russell in their Principia Mathematica 1910–1913
The paradoxes At the same time a number of disturbing paradoxes appeared in the literature in particular the BuraliForti paradox 1897 the Russell paradox 1902–03 and the Richard Paradox The resultant considerations led to Kurt Gödels paper 1931—he specifically cites the paradox of the liar—that completely reduces rules of recursion to numbers
Effective calculability In an effort to solve the Entscheidungsproblem defined precisely by Hilbert in 1928 mathematicians first set about to define what was meant by an effective method or effective calculation or effective calculability ie a calculation that would succeed In rapid succession the following appeared Alonzo Church Stephen Kleene and JB Rossers λcalculus a finely honed definition of general recursion from the work of Gödel acting on suggestions of Jacques Herbrand cf Gödels Princeton lectures of 1934 and subsequent simplifications by Kleene Churchs proof that the Entscheidungsproblem was unsolvable Emil Posts definition of effective calculability as a worker mindlessly following a list of instructions to move left or right through a sequence of rooms and while there either mark or erase a paper or observe the paper and make a yesno decision about the next instruction Alan Turings proof of that the Entscheidungsproblem was unsolvable by use of his a automatic machine—in effect almost identical to Posts formulation J Barkley Rossers definition of effective method in terms of a machine SC Kleenes proposal of a precursor to Church thesis that he called Thesis I and a few years later Kleenes renaming his Thesis Churchs Thesis and proposing Turings Thesis


==========
Emil Post 1936 described the actions of a computer human being as follows

two concepts are involved that of a symbol space in which the work leading from problem to answer is to be carried out and a fixed unalterable set of directionsHis symbol space would be

a twoway infinite sequence of spaces or boxes The problem solver or worker is to move and work in this symbol space being capable of being in and operating in but one box at a time a box is to admit of but two possible conditions ie being empty or unmarked and having a single mark in it say a vertical strokeOne box is to be singled out and called the starting point a specific problem is to be given in symbolic form by a finite number of boxes ie INPUT being marked with a stroke Likewise the answer ie OUTPUT is to be given in symbolic form by such a configuration of marked boxesA set of directions applicable to a general problem sets up a deterministic process when applied to each specific problem This process terminates only when it comes to the direction of type C  ie STOP See more at Post–Turing machine
Alan Turings work preceded that of Stibitz 1937 it is unknown whether Stibitz knew of the work of Turing Turings biographer believed that Turings use of a typewriterlike model derived from a youthful interest Alan had dreamt of inventing typewriters as a boy Mrs Turing had a typewriter and he could well have begun by asking himself what was meant by calling a typewriter mechanical Given the prevalence of Morse code and telegraphy ticker tape machines and teletypewriters we might conjecture that all were influences
Turing—his model of computation is now called a Turing machine—begins as did Post with an analysis of a human computer that he whittles down to a simple set of basic motions and states of mind But he continues a step further and creates a machine as a model of computation of numbers
Computing is normally done by writing certain symbols on paper We may suppose this paper is divided into squares like a childs arithmetic bookI assume then that the computation is carried out on onedimensional paper ie on a tape divided into squares I shall also suppose that the number of symbols which may be printed is finiteThe behavior of the computer at any moment is determined by the symbols which he is observing and his state of mind at that moment We may suppose that there is a bound B to the number of symbols or squares which the computer can observe at one moment If he wishes to observe more he must use successive observations We will also suppose that the number of states of mind which need be taken into account is finiteLet us imagine that the operations performed by the computer to be split up into simple operations which are so elementary that it is not easy to imagine them further dividedTurings reduction yields the following

The simple operations must therefore include
a Changes of the symbol on one of the observed squares
b Changes of one of the squares observed to another square within L squares of one of the previously observed squaresIt may be that some of these change necessarily invoke a change of state of mind The most general single operation must therefore be taken to be one of the following

A A possible change a of symbol together with a possible change of state of mind
B A possible change b of observed squares together with a possible change of state of mindWe may now construct a machine to do the work of this computerA few years later Turing expanded his analysis thesis definition with this forceful expression of it

A function is said to be effectively calculable if its values can be found by some purely mechanical process Though it is fairly easy to get an intuitive grasp of this idea it is nevertheless desirable to have some more definite mathematical expressible definition  he discusses the history of the definition pretty much as presented above with respect to Gödel Herbrand Kleene Church Turing and Post  We may take this statement literally understanding by a purely mechanical process one which could be carried out by a machine It is possible to give a mathematical description in a certain normal form of the structures of these machines The development of these ideas leads to the authors definition of a computable function and to an identification of computability † with effective calculability  
† We shall use the expression computable function to mean a function calculable by a machine and we let effectively calculable refer to the intuitive idea without particular identification with any one of these definitions


==========
J Barkley Rosser defined an effective mathematical method in the following manner italicization added

Effective method is used here in the rather special sense of a method each step of which is precisely determined and which is certain to produce the answer in a finite number of steps With this special meaning three different precise definitions have been given to date his footnote 5 see discussion immediately below The simplest of these to state due to Post and Turing says essentially that an effective method of solving certain sets of problems exists if one can build a machine which will then solve any problem of the set with no human intervention beyond inserting the question and later reading the answer All three definitions are equivalent so it doesnt matter which one is used Moreover the fact that all three are equivalent is a very strong argument for the correctness of any one Rosser 1939225–226Rossers footnote No 5 references the work of 1 Church and Kleene and their definition of λdefinability in particular Churchs use of it in his An Unsolvable Problem of Elementary Number Theory 1936 2 Herbrand and Gödel and their use of recursion in particular Gödels use in his famous paper On Formally Undecidable Propositions of Principia Mathematica and Related Systems I 1931 and 3 Post 1936 and Turing 1936–37 in their mechanismmodels of computation
Stephen C Kleene defined as his nowfamous Thesis I known as the Church–Turing thesis But he did this in the following context boldface in original

12 Algorithmic theories In setting up a complete algorithmic theory what we do is to describe a procedure performable for each set of values of the independent variables which procedure necessarily terminates and in such manner that from the outcome we can read a definite answer yes or no to the question is the predicate value true Kleene 1943273


==========
A number of efforts have been directed toward further refinement of the definition of algorithm and activity is ongoing because of issues surrounding in particular foundations of mathematics especially the Church–Turing thesis and philosophy of mind especially arguments about artificial intelligence For more see Algorithm characterizations


==========


==========


==========


==========


==========
Hazewinkel Michiel ed 2001 1994 Algorithm Encyclopedia of Mathematics Springer Science+Business Media BV  Kluwer Academic Publishers ISBN 9781556080104
Algorithms at Curlie
Weisstein Eric W Algorithm MathWorld
Dictionary of Algorithms and Data Structures – National Institute of Standards and TechnologyAlgorithm repositoriesThe Stony Brook Algorithm Repository – State University of New York at Stony Brook
Collected Algorithms of the ACM – Association for Computing Machinery
The Stanford GraphBase – Stanford UniversityIn mathematics and computer science an algorithm  listen is a set of instructions typically to solve a class of problems or perform a computation Algorithms are unambiguous specifications for performing calculation data processing automated reasoning and other tasks
As an effective method an algorithm can be expressed within a finite amount of space and time and in a welldefined formal language for calculating a function Starting from an initial state and initial input perhaps empty the instructions describe a computation that when executed proceeds through a finite number of welldefined successive states eventually producing output and terminating at a final ending state The transition from one state to the next is not necessarily deterministic some algorithms known as randomized algorithms incorporate random inputThe concept of algorithm has existed for centuries Greek mathematicians used algorithms in the sieve of Eratosthenes for finding prime numbers and the Euclidean algorithm for finding the greatest common divisor of two numbersThe word algorithm itself is derived from the 9th century mathematician Muḥammad ibn Mūsā alKhwārizmī Latinized Algoritmi A partial formalization of what would become the modern concept of algorithm began with attempts to solve the Entscheidungsproblem decision problem posed by David Hilbert in 1928 Later formalizations were framed as attempts to define effective calculability or effective method Those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930 1934 and 1935 Alonzo Churchs lambda calculus of 1936 Emil Posts Formulation 1 of 1936 and Alan Turings Turing machines of 1936–37 and 1939


==========
The word algorithm has its roots in Latinizing the name of Muhammad ibn Musa alKhwarizmi in a first step to algorismus AlKhwārizmī Arabic الخوارزمي‎  Persian خوارزمی‎ c 780–850 was a Persian  mathematician astronomer geographer and scholar in the House of Wisdom in Baghdad whose name means the native of Khwarazm a region that was part of Greater Iran and is now in UzbekistanAbout 825 alKhwarizmi wrote an Arabic language treatise on the Hindu–Arabic numeral system which was translated into Latin during the 12th century under the title Algoritmi de numero Indorum This title means Algoritmi on the numbers of the Indians where Algoritmi was the translators Latinization of AlKhwarizmis name AlKhwarizmi was the most widely read mathematician in Europe in the late Middle Ages primarily through another of his books the Algebra In late medieval Latin algorismus English algorism the corruption of his name simply meant the decimal number system In the 15th century under the influence of the Greek word ἀριθμός number cf arithmetic the Latin word was altered to algorithmus and the corresponding English term algorithm is first attested in the 17th century the modern sense was introduced in the 19th centuryIn English it was first used in about 1230 and then by Chaucer in 1391 English adopted the French term but it wasnt until the late 19th century that algorithm took on the meaning that it has in modern English
Another early use of the word is from 1240 in a manual titled Carmen de Algorismo composed by Alexandre de Villedieu It begins thus

Haec algorismus ars praesens dicitur in qua  Talibus Indorum fruimur bis quinque figuris

which translates as

Algorism is the art by which at present we use those Indian figures which number two times five

The poem is a few hundred lines long and summarizes the art of calculating with the new style of Indian dice or Talibus Indorum or Hindu numerals


==========

An informal definition could be a set of rules that precisely defines a sequence of operations which would include all computer programs including programs that do not perform numeric calculations and for example any prescribed bureaucratic procedure
Generally a program is only an algorithm if it stops eventuallyA prototypical example of an algorithm is the Euclidean algorithm to determine the maximum common divisor of two integers an example there are others is described by the flowchart above and as an example in a later section
Boolos Jeffrey  1974 1999 offer an informal meaning of the word in the following quotation

No human being can write fast enough or long enough or small enough†  †smaller and smaller without limit youd be trying to write on molecules on atoms on electrons to list all members of an enumerably infinite set by writing out their names one after another in some notation But humans can do something equally useful in the case of certain enumerably infinite sets They can give explicit instructions for determining the nth member of the set for arbitrary finite n Such instructions are to be given quite explicitly in a form in which they could be followed by a computing machine or by a human who is capable of carrying out only very elementary operations on symbols
An enumerably infinite set is one whose elements can be put into onetoone correspondence with the integers Thus Boolos and Jeffrey are saying that an algorithm implies instructions for a process that creates output integers from an arbitrary input integer or integers that in theory can be arbitrarily large Thus an algorithm can be an algebraic equation such as y = m + n – two arbitrary input variables m and n that produce an output y But various authors attempts to define the notion indicate that the word implies much more than this something on the order of for the addition example

Precise instructions in language understood by the computer for a fast efficient good process that specifies the moves of the computer machine or human equipped with the necessary internally contained information and capabilities to find decode and then process arbitrary input integerssymbols m and n symbols + and =  and effectively produce in a reasonable time outputinteger y at a specified place and in a specified formatThe concept of algorithm is also used to define the notion of decidability That notion is central for explaining how formal systems come into being starting from a small set of axioms and rules In logic the time that an algorithm requires to complete cannot be measured as it is not apparently related to our customary physical dimension From such uncertainties that characterize ongoing work stems the unavailability of a definition of algorithm that suits both concrete in some sense and abstract usage of the term


==========
Algorithms are essential to the way computers process data Many computer programs contain algorithms that detail the specific instructions a computer should perform in a specific order to carry out a specified task such as calculating employees paychecks or printing students report cards Thus an algorithm can be considered to be any sequence of operations that can be simulated by a Turingcomplete system Authors who assert this thesis include Minsky 1967 Savage 1987 and Gurevich 2000

 Minsky But we will also maintain with Turing  that any procedure which could naturally be called effective can in fact be realized by a simple machine Although this may seem extreme the arguments  in its favor are hard to refute
 Gurevich Turings informal argument in favor of his thesis justifies a stronger thesis every algorithm can be simulated by a Turing machine  according to Savage 1987 an algorithm is a computational process defined by a Turing machine
Turing machines can define computational processes that do not terminate The informal definitions of algorithms generally require that the algorithm always terminates This requirement renders the task of deciding whether a formal procedure is an algorithm impossible in the general case This is because of a major theorem of Computability Theory known as the Halting Problem
Typically when an algorithm is associated with processing information data can be read from an input source written to an output device and stored for further processing Stored data are regarded as part of the internal state of the entity performing the algorithm In practice the state is stored in one or more data structures
For some such computational process the algorithm must be rigorously defined specified in the way it applies in all possible circumstances that could arise That is any conditional steps must be systematically dealt with casebycase the criteria for each case must be clear and computable
Because an algorithm is a precise list of precise steps the order of computation is always crucial to the functioning of the algorithm Instructions are usually assumed to be listed explicitly and are described as starting from the top and going down to the bottom an idea that is described more formally by flow of control
So far this discussion of the formalization of an algorithm has assumed the premises of imperative programming This is the most common conception and it attempts to describe a task in discrete mechanical means Unique to this conception of formalized algorithms is the assignment operation setting the value of a variable It derives from the intuition of memory as a scratchpad There is an example below of such an assignment
For some alternate conceptions of what constitutes an algorithm see functional programming and logic programming


==========
Algorithms can be expressed in many kinds of notation including natural languages pseudocode flowcharts drakoncharts programming languages or control tables processed by interpreters Natural language expressions of algorithms tend to be verbose and ambiguous and are rarely used for complex or technical algorithms Pseudocode flowcharts drakoncharts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in natural language statements Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer but are often used as a way to define or document algorithms
There is a wide variety of representations possible and one can express a given Turing machine program as a sequence of machine tables see more at finitestate machine state transition table and control table as flowcharts and drakoncharts see more at state diagram or as a form of rudimentary machine code or assembly code called sets of quadruples see more at Turing machine
Representations of algorithms can be classed into three accepted levels of Turing machine description
1 Highlevel description
prose to describe an algorithm ignoring the implementation details At this level we do not need to mention how the machine manages its tape or head
2 Implementation description
prose used to define the way the Turing machine uses its head and the way that it stores data on its tape At this level we do not give details of states or transition function
3 Formal description
Most detailed lowest level gives the Turing machines state tableFor an example of the simple algorithm Add m+n described in all three levels see AlgorithmExamples


==========

Algorithm design refers to a method or mathematical process for problemsolving and engineering algorithms The design of algorithms is part of many solution theories of operation research such as dynamic programming and divideandconquer Techniques for designing and implementing algorithm designs are also called algorithm design patterns such as the template method pattern and decorator pattern
One of the most important aspects of algorithm design is creating an algorithm that has an efficient runtime also known as its Big O
Typical steps in the development of algorithms

Problem definition
Development of a model
Specification of the algorithm
Designing an algorithm
Checking the correctness of the algorithm
Analysis of algorithm
Implementation of algorithm
Program testing
Documentation preparation


==========

Most algorithms are intended to be implemented as computer programs However algorithms are also implemented by other means such as in a biological neural network for example the human brain implementing arithmetic or an insect looking for food in an electrical circuit or in a mechanical device


==========

In computer systems an algorithm is basically an instance of logic written in software by software developers to be effective for the intended target computers to produce output from given perhaps null input An optimal algorithm even running in old hardware would produce faster results than a nonoptimal higher time complexity algorithm for the same purpose running in more efficient hardware that is why algorithms like computer hardware are considered technology
Elegant compact programs good fast programs  The notion of simplicity and elegance appears informally in Knuth and precisely in Chaitin

Knuth   we want good algorithms in some loosely defined aesthetic sense One criterion  is the length of time taken to perform the algorithm  Other criteria are adaptability of the algorithm to computers its simplicity and elegance etcChaitin   a program is elegant by which I mean that its the smallest possible program for producing the output that it doesChaitin prefaces his definition with Ill show you cant prove that a program is elegant—such a proof would solve the Halting problem ibid
Algorithm versus function computable by an algorithm For a given function multiple algorithms may exist This is true even without expanding the available instruction set available to the programmer Rogers observes that It is  important to distinguish between the notion of algorithm ie procedure and the notion of function computable by algorithm ie mapping yielded by procedure The same function may have several different algorithmsUnfortunately there may be a tradeoff between goodness speed and elegance compactness—an elegant program may take more steps to complete a computation than one less elegant An example that uses Euclids algorithm appears below
Computers and computors models of computation A computer or human computor is a restricted type of machine a discrete deterministic mechanical device that blindly follows its instructions Melzaks and Lambeks primitive models reduced this notion to four elements i discrete distinguishable locations ii discrete indistinguishable counters iii an agent and iv a list of instructions that are effective relative to the capability of the agentMinsky describes a more congenial variation of Lambeks abacus model in his Very Simple Bases for Computability Minskys machine proceeds sequentially through its five or six depending on how one counts instructions unless either a conditional IF–THEN GOTO or an unconditional GOTO changes program flow out of sequence Besides HALT Minskys machine includes three assignment replacement substitution operations ZERO eg the contents of location replaced by 0 L ← 0 SUCCESSOR eg L ← L+1 and DECREMENT eg L ← L − 1 Rarely must a programmer write code with such a limited instruction set But Minsky shows as do Melzak and Lambek that his machine is Turing complete with only four general types of instructions conditional GOTO unconditional GOTO assignmentreplacementsubstitution and HALT  However a few different assignment instructions eg DECREMENT INCREMENT and ZEROCLEAREMPTY for a Minsky machine are also required for Turingcompleteness their exact specification is somewhat up to the designer The unconditional GOTO is a convenience it can be constructed by initializing a dedicated location to zero eg the instruction  Z ← 0  thereafter the instruction IF Z=0 THEN GOTO xxx is unconditional
Simulation of an algorithm computer computor language Knuth advises the reader that the best way to learn an algorithm is to try it    immediately take pen and paper and work through an example But what about a simulation or execution of the real thing The programmer must translate the algorithm into a language that the simulatorcomputercomputor can effectively execute Stone gives an example of this when computing the roots of a quadratic equation the computor must know how to take a square root If they dont then the algorithm to be effective must provide a set of rules for extracting a square rootThis means that the programmer must know a language that is effective relative to the target computing agent computercomputor
But what model should be used for the simulation Van Emde Boas observes even if we base complexity theory on abstract instead of concrete machines arbitrariness of the choice of a model remains It is at this point that the notion of simulation enters When speed is being measured the instruction set matters For example the subprogram in Euclids algorithm to compute the remainder would execute much faster if the programmer had a modulus instruction available rather than just subtraction or worse just Minskys decrement
Structured programming canonical structures Per the Church–Turing thesis any algorithm can be computed by a model known to be Turing complete and per Minskys demonstrations Turing completeness requires only four instruction types—conditional GOTO unconditional GOTO assignment HALT Kemeny and Kurtz observe that while undisciplined use of unconditional GOTOs and conditional IFTHEN GOTOs can result in spaghetti code a programmer can write structured programs using only these instructions on the other hand it is also possible and not too hard to write badly structured programs in a structured language Tausworthe augments the three BöhmJacopini canonical structures SEQUENCE IFTHENELSE and WHILEDO with two more DOWHILE and CASE An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical inductionCanonical flowchart symbols The graphical aide called a flowchart offers a way to describe and document an algorithm and a computer program of one Like the program flow of a Minsky machine a flowchart always starts at the top of a page and proceeds down Its primary symbols are only four the directed arrow showing program flow the rectangle SEQUENCE GOTO the diamond IFTHENELSE and the dot ORtie The Böhm–Jacopini canonical structures are made of these primitive shapes Substructures can nest in rectangles but only if a single exit occurs from the superstructure The symbols and their use to build the canonical structures are shown in the diagram


==========


==========

One of the simplest algorithms is to find the largest number in a list of numbers of random order Finding the solution requires looking at every number in the list From this follows a simple algorithm which can be stated in a highlevel description in English prose as
Highlevel description

If there are no numbers in the set then there is no highest number
Assume the first number in the set is the largest number in the set
For each remaining number in the set if this number is larger than the current largest number consider this number to be the largest number in the set
When there are no numbers left in the set to iterate over consider the current largest number to be the largest number of the setQuasiformal description
Written in prose but much closer to the highlevel language of a computer program the following is the more formal coding of the algorithm in pseudocode or pidgin code


==========

Euclids algorithm to compute the greatest common divisor GCD to two numbers appears as Proposition II in Book VII Elementary Number Theory of his Elements Euclid poses the problem thus Given two numbers not prime to one another to find their greatest common measure He defines A number to be a multitude composed of units a counting number a positive integer not including zero To measure is to place a shorter measuring length s successively q times along longer length l until the remaining portion r is less than the shorter length s In modern words remainder r = l − q×s q being the quotient or remainder r is the modulus the integerfractional part left over after the divisionFor Euclids method to succeed the starting lengths must satisfy two requirements i the lengths must not be zero AND ii the subtraction must be “proper” ie a test must guarantee that the smaller of the two numbers is subtracted from the larger alternately the two can be equal so their subtraction yields zero
Euclids original proof adds a third requirement the two lengths must not be prime to one another Euclid stipulated this so that he could construct a reductio ad absurdum proof that the two numbers common measure is in fact the greatest While Nicomachus algorithm is the same as Euclids when the numbers are prime to one another it yields the number 1 for their common measure So to be precise the following is really Nicomachus algorithm


============
Only a few instruction types are required to execute Euclids algorithm—some logical tests conditional GOTO unconditional GOTO assignment replacement and subtraction

A location is symbolized by upper case letters eg S A etc
The varying quantity number in a location is written in lower case letters and usually associated with the locations name For example location L at the start might contain the number l = 3009


============

The following algorithm is framed as Knuths fourstep version of Euclids and Nicomachus but rather than using division to find the remainder it uses successive subtractions of the shorter length s from the remaining length r until r is less than s The highlevel description shown in boldface is adapted from Knuth 19732–4
INPUT

1 Into two locations L and S put the numbers l and s that represent the two lengths
  INPUT L S
2 Initialize R make the remaining length r equal to the startinginitialinput length l
  R ← L

E0 Ensure r ≥ s

3 Ensure the smaller of the two numbers is in S and the larger in R
  IF R  S THEN
    the contents of L is the larger number so skip over the exchangesteps 4 5 and 6
    GOTO step 6
  ELSE
    swap the contents of R and S
4   L ← R this first step is redundant but is useful for later discussion
5   R ← S
6   S ← L

E1 Find remainder Until the remaining length r in R is less than the shorter length s in S repeatedly subtract the measuring number s in S from the remaining length r in R

7 IF S  R THEN
    done measuring so
    GOTO 10
  ELSE
    measure again
8   R ← R − S
9   Remainderloop
    GOTO 7

E2 Is the remainder zero EITHER i the last measure was exact the remainder in R is zero and the program can halt OR ii the algorithm must continue the last measure left a remainder in R less than measuring number in S

10 IF R = 0 THEN
     done so
     GOTO step 15
   ELSE
     CONTINUE TO step 11

E3 Interchange s and r The nut of Euclids algorithm Use remainder r to measure what was previously smaller number s L serves as a temporary location

11  L ← R
12  R ← S
13  S ← L
14  Repeat the measuring process
    GOTO 7

OUTPUT

15 Done S contains the greatest common divisor
   PRINT S

DONE

16 HALT END STOP


============
The following version of Euclids algorithm requires only six core instructions to do what thirteen are required to do by Inelegant worse Inelegant requires more types of instructions The flowchart of Elegant can be found at the top of this article In the unstructured Basic language the steps are numbered and the instruction LET  =  is the assignment instruction symbolized by ←

How Elegant works In place of an outer Euclid loop Elegant shifts back and forth between two coloops an A  B loop that computes A ← A − B and a B ≤ A loop that computes B ← B − A This works because when at last the minuend M is less than or equal to the subtrahend S  Difference = Minuend − Subtrahend the minuend can become s the new measuring length and the subtrahend can become the new r the length to be measured in other words the sense of the subtraction reverses
The following version can be used with Object Oriented languages


==========
Does an algorithm do what its author wants it to do A few test cases usually give some confidence in the core functionality But tests are not enough For test cases one source uses 3009 and 884 Knuth suggested 40902 24140 Another interesting case is the two relatively prime numbers 14157 and 5950
But exceptional cases must be identified and tested Will Inelegant perform properly when R  S S  R R = S Ditto for Elegant B  A A  B A = B Yes to all What happens when one number is zero both numbers are zero Inelegant computes forever in all cases Elegant computes forever when A = 0 What happens if negative numbers are entered Fractional numbers If the input numbers ie the domain of the function computed by the algorithmprogram is to include only positive integers including zero then the failures at zero indicate that the algorithm and the program that instantiates it is a partial function rather than a total function A notable failure due to exceptions is the Ariane 5 Flight 501 rocket failure June 4 1996
Proof of program correctness by use of mathematical induction Knuth demonstrates the application of mathematical induction to an extended version of Euclids algorithm and he proposes a general method applicable to proving the validity of any algorithm Tausworthe proposes that a measure of the complexity of a program be the length of its correctness proof


==========
Elegance compactness versus goodness speed With only six core instructions Elegant is the clear winner compared to Inelegant at thirteen instructions However Inelegant is faster it arrives at HALT in fewer steps Algorithm analysis indicates why this is the case Elegant does two conditional tests in every subtraction loop whereas Inelegant only does one As the algorithm usually requires many loopthroughs on average much time is wasted doing a B = 0 test that is needed only after the remainder is computed
Can the algorithms be improved Once the programmer judges a program fit and effective—that is it computes the function intended by its author—then the question becomes can it be improved
The compactness of Inelegant can be improved by the elimination of five steps But Chaitin proved that compacting an algorithm cannot be automated by a generalized algorithm rather it can only be done heuristically ie by exhaustive search examples to be found at Busy beaver trial and error cleverness insight application of inductive reasoning etc Observe that steps 4 5 and 6 are repeated in steps 11 12 and 13 Comparison with Elegant provides a hint that these steps together with steps 2 and 3 can be eliminated This reduces the number of core instructions from thirteen to eight which makes it more elegant than Elegant at nine steps
The speed of Elegant can be improved by moving the B=0 test outside of the two subtraction loops This change calls for the addition of three instructions B = 0 A = 0 GOTO Now Elegant computes the examplenumbers faster whether this is always the case for any given A B and R S would require a detailed analysis


==========

It is frequently important to know how much of a particular resource such as time or storage is theoretically required for a given algorithm Methods have been developed for the analysis of algorithms to obtain such quantitative answers estimates for example the sorting algorithm above has a time requirement of On using the big O notation with n as the length of the list At all times the algorithm only needs to remember two values the largest number found so far and its current position in the input list Therefore it is said to have a space requirement of O1 if the space required to store the input numbers is not counted or On if it is counted
Different algorithms may complete the same task with a different set of instructions in less or more time space or effort than others For example a binary search algorithm with cost Olog n  outperforms a sequential search cost On  when used for table lookups on sorted lists or arrays


==========

The analysis and study of algorithms is a discipline of computer science and is often practiced abstractly without the use of a specific programming language or implementation In this sense algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation Usually pseudocode is used for analysis as it is the simplest and most general representation However ultimately most algorithms are usually implemented on particular hardwaresoftware platforms and their algorithmic efficiency is eventually put to the test using real code For the solution of a one off problem the efficiency of a particular algorithm may not have significant consequences unless n is extremely large but for algorithms designed for fast interactive commercial or long life scientific usage it may be critical Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign
Empirical testing is useful because it may uncover unexpected interactions that affect performance Benchmarks may be used to compare beforeafter potential improvements to an algorithm after program optimization
Empirical tests cannot replace formal analysis though and are not trivial to perform in a fair manner


==========

To illustrate the potential improvements possible even in wellestablished algorithms a recent significant innovation relating to FFT algorithms used heavily in the field of image processing can decrease processing time up to 1000 times for applications like medical imaging In general speed improvements depend on special properties of the problem which are very common in practical applications Speedups of this magnitude enable computing devices that make extensive use of image processing like digital cameras and medical equipment to consume less power


==========
There are various ways to classify algorithms each with its own merits


==========
One way to classify algorithms is by implementation means

Recursion
A recursive algorithm is one that invokes makes reference to itself repeatedly until a certain condition also known as termination condition matches which is a method common to functional programming Iterative algorithms use repetitive constructs like loops and sometimes additional data structures like stacks to solve the given problems Some problems are naturally suited for one implementation or the other For example towers of Hanoi is well understood using recursive implementation Every recursive version has an equivalent but possibly more or less complex iterative version and vice versa
Logical
An algorithm may be viewed as controlled logical deduction This notion may be expressed as Algorithm = logic + control The logic component expresses the axioms that may be used in the computation and the control component determines the way in which deduction is applied to the axioms This is the basis for the logic programming paradigm In pure logic programming languages the control component is fixed and algorithms are specified by supplying only the logic component The appeal of this approach is the elegant semantics a change in the axioms produces a welldefined change in the algorithm
Serial parallel or distributed
Algorithms are usually discussed with the assumption that computers execute one instruction of an algorithm at a time Those computers are sometimes called serial computers An algorithm designed for such an environment is called a serial algorithm as opposed to parallel algorithms or distributed algorithms Parallel algorithms take advantage of computer architectures where several processors can work on a problem at the same time whereas distributed algorithms utilize multiple machines connected with a computer network Parallel or distributed algorithms divide the problem into more symmetrical or asymmetrical subproblems and collect the results back together The resource consumption in such algorithms is not only processor cycles on each processor but also the communication overhead between the processors Some sorting algorithms can be parallelized efficiently but their communication overhead is expensive Iterative algorithms are generally parallelizable Some problems have no parallel algorithms and are called inherently serial problems
Deterministic or nondeterministic
Deterministic algorithms solve the problem with exact decision at every step of the algorithm whereas nondeterministic algorithms solve problems via guessing although typical guesses are made more accurate through the use of heuristics
Exact or approximate
While many algorithms reach an exact solution approximation algorithms seek an approximation that is closer to the true solution The approximation can be reached by either using a deterministic or a random strategy Such algorithms have practical value for many hard problems One of the examples of an approximate algorithm is the Knapsack problem where there is a set of given items Its goal is to pack the knapsack to get the maximum total value Each item has some weight and some value Total weight that can be carried is no more than some fixed number X So the solution must consider weights of items as well as their value
Quantum algorithm
They run on a realistic model of quantum computation The term is usually used for those algorithms which seem inherently quantum or use some essential feature of Quantum computing such as quantum superposition or quantum entanglement


==========
Another way of classifying algorithms is by their design methodology or paradigm There is a certain number of paradigms each different from the other Furthermore each of these categories includes many different types of algorithms Some common paradigms are

Bruteforce or exhaustive search
This is the naive method of trying every possible solution to see which is best
Divide and conquer
A divide and conquer algorithm repeatedly reduces an instance of a problem to one or more smaller instances of the same problem usually recursively until the instances are small enough to solve easily One such example of divide and conquer is merge sorting Sorting can be done on each segment of data after dividing data into segments and sorting of entire data can be obtained in the conquer phase by merging the segments A simpler variant of divide and conquer is called a decrease and conquer algorithm that solves an identical subproblem and uses the solution of this subproblem to solve the bigger problem Divide and conquer divides the problem into multiple subproblems and so the conquer stage is more complex than decrease and conquer algorithms An example of a decrease and conquer algorithm is the binary search algorithm
Search and enumeration
Many problems such as playing chess can be modeled as problems on graphs A graph exploration algorithm specifies rules for moving around a graph and is useful for such problems This category also includes search algorithms branch and bound enumeration and backtracking
Randomized algorithm
Such algorithms make some choices randomly or pseudorandomly They can be very useful in finding approximate solutions for problems where finding exact solutions can be impractical see heuristic method below For some of these problems it is known that the fastest approximations must involve some randomness Whether randomized algorithms with polynomial time complexity can be the fastest algorithms for some problems is an open question known as the P versus NP problem There are two large classes of such algorithmsMonte Carlo algorithms return a correct answer with highprobability Eg RP is the subclass of these that run in polynomial time
Las Vegas algorithms always return the correct answer but their running time is only probabilistically bound eg ZPPReduction of complexity
This technique involves solving a difficult problem by transforming it into a betterknown problem for which we have hopefully asymptotically optimal algorithms The goal is to find a reducing algorithm whose complexity is not dominated by the resulting reduced algorithms For example one selection algorithm for finding the median in an unsorted list involves first sorting the list the expensive portion and then pulling out the middle element in the sorted list the cheap portion This technique is also known as transform and conquer
Back tracking
In this approach multiple solutions are built incrementally and abandoned when it is determined that they cannot lead to a valid full solution


==========
For optimization problems there is a more specific classification of algorithms an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following

Linear programming
When searching for optimal solutions to a linear function bound to linear equality and inequality constraints the constraints of the problem can be used directly in producing the optimal solutions There are algorithms that can solve any problem in this category such as the popular simplex algorithm Problems that can be solved with linear programming include the maximum flow problem for directed graphs If a problem additionally requires that one or more of the unknowns must be an integer then it is classified in integer programming A linear programming algorithm can solve such a problem if it can be proved that all restrictions for integer values are superficial ie the solutions satisfy these restrictions anyway In the general case a specialized algorithm or an algorithm that finds approximate solutions is used depending on the difficulty of the problem
Dynamic programming
When a problem shows optimal substructures—meaning the optimal solution to a problem can be constructed from optimal solutions to subproblems—and overlapping subproblems meaning the same subproblems are used to solve many different problem instances a quicker approach called dynamic programming avoids recomputing solutions that have already been computed For example Floyd–Warshall algorithm the shortest path to a goal from a vertex in a weighted graph can be found by using the shortest path to the goal from all adjacent vertices Dynamic programming and memoization go together The main difference between dynamic programming and divide and conquer is that subproblems are more or less independent in divide and conquer whereas subproblems overlap in dynamic programming The difference between dynamic programming and straightforward recursion is in caching or memoization of recursive calls When subproblems are independent and there is no repetition memoization does not help hence dynamic programming is not a solution for all complex problems By using memoization or maintaining a table of subproblems already solved dynamic programming reduces the exponential nature of many problems to polynomial complexity
The greedy method
A greedy algorithm is similar to a dynamic programming algorithm in that it works by examining substructures in this case not of the problem but of a given solution Such algorithms start with some solution which may be given or have been constructed in some way and improve it by making small modifications For some problems they can find the optimal solution while for others they stop at local optima that is at solutions that cannot be improved by the algorithm but are not optimum The most popular use of greedy algorithms is for finding the minimal spanning tree where finding the optimal solution is possible with this method Huffman Tree Kruskal Prim Sollin are greedy algorithms that can solve this optimization problem
The heuristic method
In optimization problems heuristic algorithms can be used to find a solution close to the optimal solution in cases where finding the optimal solution is impractical These algorithms work by getting closer and closer to the optimal solution as they progress In principle if run for an infinite amount of time they will find the optimal solution Their merit is that they can find a solution very close to the optimal solution in a relatively short time Such algorithms include local search tabu search simulated annealing and genetic algorithms Some of them like simulated annealing are nondeterministic algorithms while others like tabu search are deterministic When a bound on the error of the nonoptimal solution is known the algorithm is further categorized as an approximation algorithm


==========

Every field of science has its own problems and needs efficient algorithms Related problems in one field are often studied together Some example classes are search algorithms sorting algorithms merge algorithms numerical algorithms graph algorithms string algorithms computational geometric algorithms combinatorial algorithms medical algorithms machine learning cryptography data compression algorithms and parsing techniques
Fields tend to overlap with each other and algorithm advances in one field may improve those of other sometimes completely unrelated fields For example dynamic programming was invented for optimization of resource consumption in industry but is now used in solving a broad range of problems in many fields


==========

Algorithms can be classified by the amount of time they need to complete compared to their input size

Constant time if the time needed by the algorithm is the same regardless of the input size Eg an access to an array element
Linear time if the time is proportional to the input size Eg the traverse of a list
Logarithmic time if the time is a logarithmic function of the input size Eg binary search algorithm
Polynomial time if the time is a power of the input size Eg the bubble sort algorithm has quadratic time complexity
Exponential time if the time is an exponential function of the input size Eg Bruteforce searchSome problems may have multiple algorithms of differing complexity while other problems might have no algorithms or no known efficient algorithms There are also mappings from some problems to other problems Owing to this it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best possible algorithms for them


==========
The adjective continuous when applied to the word algorithm can mean

An algorithm operating on data that represents continuous quantities even though this data is represented by discrete approximations—such algorithms are studied in numerical analysis or
An algorithm in the form of a differential equation that operates continuously on the data running on an analog computer


==========

Algorithms by themselves are not usually patentable In the United States a claim consisting solely of simple manipulations of abstract concepts numbers or signals does not constitute processes USPTO 2006 and hence algorithms are not patentable as in Gottschalk v Benson However practical applications of algorithms are sometimes patentable For example in Diamond v Diehr the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable The patenting of software is highly controversial and there are highly criticized patents involving algorithms especially data compression algorithms such as Unisys LZW patent
Additionally some cryptographic algorithms have export restrictions see export of cryptography


==========


==========
The earliest  evidence of algorithms is found in the Babylonian mathematics of ancient Mesopotamia modern Iraq A Sumerian clay tablet found in Shuruppak near Baghdad and dated to circa 2500 BC described the earliest division algorithm During the Hammurabi dynasty circa 18001600 BC Babylonian clay tablets described algorithms for computing formulas Algorithms were also used in Babylonian astronomy Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical eventsAlgorithms for arithmetic are also found in ancient Egyptian mathematics dating back to the Rhind Mathematical Papyrus circa 1550 BC Algorithms were later used in ancient Hellenistic mathematics Two examples are the Sieve of Eratosthenes which was described in the Introduction to Arithmetic by Nicomachus and the Euclidean algorithm which was first described in Euclids Elements c 300 BC


==========
Tallymarks To keep track of their flocks their sacks of grain and their money the ancients used tallying accumulating stones or marks scratched on sticks or making discrete symbols in clay Through the Babylonian and Egyptian use of marks and symbols eventually Roman numerals and the abacus evolved Dilson p 16–41 Tally marks appear prominently in unary numeral system arithmetic used in Turing machine and Post–Turing machine computations


==========
Muhammad ibn Mūsā alKhwārizmī a Persian mathematician wrote the Aljabr in the 9th century The terms algorism and algorithm are derived from the name alKhwārizmī while the term algebra is derived from the book Aljabr In Europe the word algorithm was originally used to refer to the sets of rules and techniques used by AlKhwarizmi to solve algebraic equations before later being generalized to refer to any set of rules or techniques This eventually culminated in Leibnizs notion of the calculus ratiocinator ca 1680

A good century and a half ahead of his time Leibniz proposed an algebra of logic an algebra that would specify the rules for manipulating logical concepts in the manner that ordinary algebra specifies the rules for manipulating numbers


==========
The first cryptographic algorithm for deciphering encrypted code was developed by AlKindi a 9thcentury Arab mathematician in A Manuscript On Deciphering Cryptographic Messages He gave the first description of cryptanalysis by frequency analysis the earliest codebreaking algorithm


==========
The clock Bolter credits the invention of the weightdriven clock as The key invention of Europe in the Middle Ages in particular the verge escapement that provides us with the tick and tock of a mechanical clock The accurate automatic machine led immediately to mechanical automata beginning in the 13th century and finally to computational machines—the difference engine and analytical engines of Charles Babbage and Countess Ada Lovelace mid19th century Lovelace is credited with the first creation of an algorithm intended for processing on a computer—Babbages analytical engine the first device considered a real Turingcomplete computer instead of just a calculator—and is sometimes called historys first programmer as a result though a full implementation of Babbages second device would not be realized until decades after her lifetime
Logical machines 1870 – Stanley Jevons logical abacus and logical machine The technical problem was to reduce Boolean equations when presented in a form similar to what is now known as Karnaugh maps Jevons 1880 describes first a simple abacus of slips of wood furnished with pins contrived so that any part or class of the logical combinations can be picked out mechanically  More recently however I have reduced the system to a completely mechanical form and have thus embodied the whole of the indirect process of inference in what may be called a Logical Machine His machine came equipped with certain moveable wooden rods and at the foot are 21 keys like those of a piano etc  With this machine he could analyze a syllogism or any other simple logical argumentThis machine he displayed in 1870 before the Fellows of the Royal Society Another logician John Venn however in his 1881 Symbolic Logic turned a jaundiced eye to this effort I have no high estimate myself of the interest or importance of what are sometimes called logical machines  it does not seem to me that any contrivances at present known or likely to be discovered really deserve the name of logical machines see more at Algorithm characterizations But not to be outdone he too presented a plan somewhat analogous I apprehend to Prof Jevons abacus  And again corresponding to Prof Jevonss logical machine the following contrivance may be described I prefer to call it merely a logicaldiagram machine  but I suppose that it could do very completely all that can be rationally expected of any logical machineJacquard loom Hollerith punch cards telegraphy and telephony – the electromechanical relay Bell and Newell 1971 indicate that the Jacquard loom 1801 precursor to Hollerith cards punch cards 1887 and telephone switching technologies were the roots of a tree leading to the development of the first computers By the mid19th century the telegraph the precursor of the telephone was in use throughout the world its discrete and distinguishable encoding of letters as dots and dashes a common sound By the late 19th century the ticker tape ca 1870s was in use as was the use of Hollerith cards in the 1890 US census Then came the teleprinter ca 1910 with its punchedpaper use of Baudot code on tape
Telephoneswitching networks of electromechanical relays invented 1835 was behind the work of George Stibitz 1937 the inventor of the digital adding device As he worked in Bell Laboratories he observed the burdensome use of mechanical calculators with gears He went home one evening in 1937 intending to test his idea When the tinkering was over Stibitz had constructed a binary adding deviceDavis 2000 observes the particular importance of the electromechanical relay with its two binary states open and closed

It was only with the development beginning in the 1930s of electromechanical calculators using electrical relays that machines were built having the scope Babbage had envisioned


==========
Symbols and rules In rapid succession the mathematics of George Boole 1847 1854 Gottlob Frege 1879 and Giuseppe Peano 1888–1889 reduced arithmetic to a sequence of symbols manipulated by rules Peanos The principles of arithmetic presented by a new method 1888 was the first attempt at an axiomatization of mathematics in a symbolic languageBut Heijenoort gives Frege 1879 this kudos Freges is perhaps the most important single work ever written in logic  in which we see a  formula language that is a lingua characterica a language written with special symbols for pure thought that is free from rhetorical embellishments  constructed from specific symbols that are manipulated according to definite rules The work of Frege was further simplified and amplified by Alfred North Whitehead and Bertrand Russell in their Principia Mathematica 1910–1913
The paradoxes At the same time a number of disturbing paradoxes appeared in the literature in particular the BuraliForti paradox 1897 the Russell paradox 1902–03 and the Richard Paradox The resultant considerations led to Kurt Gödels paper 1931—he specifically cites the paradox of the liar—that completely reduces rules of recursion to numbers
Effective calculability In an effort to solve the Entscheidungsproblem defined precisely by Hilbert in 1928 mathematicians first set about to define what was meant by an effective method or effective calculation or effective calculability ie a calculation that would succeed In rapid succession the following appeared Alonzo Church Stephen Kleene and JB Rossers λcalculus a finely honed definition of general recursion from the work of Gödel acting on suggestions of Jacques Herbrand cf Gödels Princeton lectures of 1934 and subsequent simplifications by Kleene Churchs proof that the Entscheidungsproblem was unsolvable Emil Posts definition of effective calculability as a worker mindlessly following a list of instructions to move left or right through a sequence of rooms and while there either mark or erase a paper or observe the paper and make a yesno decision about the next instruction Alan Turings proof of that the Entscheidungsproblem was unsolvable by use of his a automatic machine—in effect almost identical to Posts formulation J Barkley Rossers definition of effective method in terms of a machine SC Kleenes proposal of a precursor to Church thesis that he called Thesis I and a few years later Kleenes renaming his Thesis Churchs Thesis and proposing Turings Thesis


==========
Emil Post 1936 described the actions of a computer human being as follows

two concepts are involved that of a symbol space in which the work leading from problem to answer is to be carried out and a fixed unalterable set of directionsHis symbol space would be

a twoway infinite sequence of spaces or boxes The problem solver or worker is to move and work in this symbol space being capable of being in and operating in but one box at a time a box is to admit of but two possible conditions ie being empty or unmarked and having a single mark in it say a vertical strokeOne box is to be singled out and called the starting point a specific problem is to be given in symbolic form by a finite number of boxes ie INPUT being marked with a stroke Likewise the answer ie OUTPUT is to be given in symbolic form by such a configuration of marked boxesA set of directions applicable to a general problem sets up a deterministic process when applied to each specific problem This process terminates only when it comes to the direction of type C  ie STOP See more at Post–Turing machine
Alan Turings work preceded that of Stibitz 1937 it is unknown whether Stibitz knew of the work of Turing Turings biographer believed that Turings use of a typewriterlike model derived from a youthful interest Alan had dreamt of inventing typewriters as a boy Mrs Turing had a typewriter and he could well have begun by asking himself what was meant by calling a typewriter mechanical Given the prevalence of Morse code and telegraphy ticker tape machines and teletypewriters we might conjecture that all were influences
Turing—his model of computation is now called a Turing machine—begins as did Post with an analysis of a human computer that he whittles down to a simple set of basic motions and states of mind But he continues a step further and creates a machine as a model of computation of numbers
Computing is normally done by writing certain symbols on paper We may suppose this paper is divided into squares like a childs arithmetic bookI assume then that the computation is carried out on onedimensional paper ie on a tape divided into squares I shall also suppose that the number of symbols which may be printed is finiteThe behavior of the computer at any moment is determined by the symbols which he is observing and his state of mind at that moment We may suppose that there is a bound B to the number of symbols or squares which the computer can observe at one moment If he wishes to observe more he must use successive observations We will also suppose that the number of states of mind which need be taken into account is finiteLet us imagine that the operations performed by the computer to be split up into simple operations which are so elementary that it is not easy to imagine them further dividedTurings reduction yields the following

The simple operations must therefore include
a Changes of the symbol on one of the observed squares
b Changes of one of the squares observed to another square within L squares of one of the previously observed squaresIt may be that some of these change necessarily invoke a change of state of mind The most general single operation must therefore be taken to be one of the following

A A possible change a of symbol together with a possible change of state of mind
B A possible change b of observed squares together with a possible change of state of mindWe may now construct a machine to do the work of this computerA few years later Turing expanded his analysis thesis definition with this forceful expression of it

A function is said to be effectively calculable if its values can be found by some purely mechanical process Though it is fairly easy to get an intuitive grasp of this idea it is nevertheless desirable to have some more definite mathematical expressible definition  he discusses the history of the definition pretty much as presented above with respect to Gödel Herbrand Kleene Church Turing and Post  We may take this statement literally understanding by a purely mechanical process one which could be carried out by a machine It is possible to give a mathematical description in a certain normal form of the structures of these machines The development of these ideas leads to the authors definition of a computable function and to an identification of computability † with effective calculability  
† We shall use the expression computable function to mean a function calculable by a machine and we let effectively calculable refer to the intuitive idea without particular identification with any one of these definitions


==========
J Barkley Rosser defined an effective mathematical method in the following manner italicization added

Effective method is used here in the rather special sense of a method each step of which is precisely determined and which is certain to produce the answer in a finite number of steps With this special meaning three different precise definitions have been given to date his footnote 5 see discussion immediately below The simplest of these to state due to Post and Turing says essentially that an effective method of solving certain sets of problems exists if one can build a machine which will then solve any problem of the set with no human intervention beyond inserting the question and later reading the answer All three definitions are equivalent so it doesnt matter which one is used Moreover the fact that all three are equivalent is a very strong argument for the correctness of any one Rosser 1939225–226Rossers footnote No 5 references the work of 1 Church and Kleene and their definition of λdefinability in particular Churchs use of it in his An Unsolvable Problem of Elementary Number Theory 1936 2 Herbrand and Gödel and their use of recursion in particular Gödels use in his famous paper On Formally Undecidable Propositions of Principia Mathematica and Related Systems I 1931 and 3 Post 1936 and Turing 1936–37 in their mechanismmodels of computation
Stephen C Kleene defined as his nowfamous Thesis I known as the Church–Turing thesis But he did this in the following context boldface in original

12 Algorithmic theories In setting up a complete algorithmic theory what we do is to describe a procedure performable for each set of values of the independent variables which procedure necessarily terminates and in such manner that from the outcome we can read a definite answer yes or no to the question is the predicate value true Kleene 1943273


==========
A number of efforts have been directed toward further refinement of the definition of algorithm and activity is ongoing because of issues surrounding in particular foundations of mathematics especially the Church–Turing thesis and philosophy of mind especially arguments about artificial intelligence For more see Algorithm characterizations


==========


==========


==========


==========


==========
Hazewinkel Michiel ed 2001 1994 Algorithm Encyclopedia of Mathematics Springer Science+Business Media BV  Kluwer Academic Publishers ISBN 9781556080104
Algorithms at Curlie
Weisstein Eric W Algorithm MathWorld
Dictionary of Algorithms and Data Structures – National Institute of Standards and TechnologyAlgorithm repositoriesThe Stony Brook Algorithm Repository – State University of New York at Stony Brook
Collected Algorithms of the ACM – Association for Computing Machinery
The Stanford GraphBase – Stanford UniversityIn computer science algorithmic efficiency is a property of an algorithm which relates to the number of computational resources used by the algorithm An algorithm must be analyzed to determine its resource usage and the efficiency of an algorithm can be measured based on usage of different resources Algorithmic efficiency can be thought of as analogous to engineering productivity for a repeating or continuous process
For maximum efficiency we wish to minimize resource usage However different resources such as time and space complexity cannot be compared directly so which of two algorithms is considered to be more efficient often depends on which measure of efficiency is considered most important
For example bubble sort and timsort are both algorithms to sort a list of items from smallest to largest Bubble sort sorts the list in time proportional to the number of elements squared 
  
    
      
        
          
            
              
                O
              
            
            
              
              
                n
                
                  2
                
              
              
            
          
        
      
    
    displaystyle scriptstyle mathcal Oleftn2right
   see Big O notation but only requires a small amount of extra memory which is constant with respect to the length of the list 
  
    
      
        
          
            
              
                O
              
            
            
              
              1
              
            
          
        
      
    
    textstyle scriptstyle mathcal Oleft1right
   Timsort sorts the list in time linearithmic proportional to a quantity times its logarithm in the lists length 
  
    
      
        
          
            
              O
              
                
                
                  n
                  log
                  ⁡
                  n
                
                
              
            
          
        
      
    
    textstyle scriptstyle mathcal Oleftnlog nright
   but has a space requirement linear in the length of the list 
  
    
      
        
          
            
              O
              
                
                n
                
              
            
          
        
      
    
    textstyle scriptstyle mathcal Oleftnright
   If large lists must be sorted at high speed for a given application timsort is a better choice however if minimizing the memory footprint of the sorting is more important bubble sort is a better choice


==========
The importance of efficiency with respect to time was emphasised by Ada Lovelace in 1843 as applying to Charles Babbages mechanical analytical engine

In almost every computation a great variety of arrangements for the succession of the processes is possible and various considerations must influence the selections amongst them for the purposes of a calculating engine One essential object is to choose that arrangement which shall tend to reduce to a minimum the time necessary for completing the calculation
Early electronic computers were severely limited both by the speed of operations and the amount of memory available In some cases it was realized that there was a space–time tradeoff whereby a task could be handled either by using a fast algorithm which used quite a lot of working memory or by using a slower algorithm which used very little working memory The engineering tradeoff was then to use the fastest algorithm which would fit in the available memory
Modern computers are significantly faster than the early computers and have a much larger amount of memory available Gigabytes instead of Kilobytes Nevertheless Donald Knuth emphasised that efficiency is still an important consideration

  In established engineering disciplines a 12 improvement easily obtained is never considered marginal and I believe the same viewpoint should prevail in software engineering


==========
An algorithm is considered efficient if its resource consumption also known as computational cost is at or below some acceptable level Roughly speaking acceptable means  it will run in a reasonable amount of time or space on an available computer typically as a function of the size of the input Since the 1950s computers have seen dramatic increases in both the available computational power and in the available amount of memory so current acceptable levels would have been unacceptable even 10 years ago In fact thanks to the approximate doubling of computer power every 2 years tasks that are acceptably efficient on modern smartphones and embedded systems may have been unacceptably inefficient for industrial servers 10 years ago
Computer manufacturers frequently bring out new models often with higher performance Software costs can be quite high so in some cases the simplest and cheapest way of getting higher performance might be to just buy a faster computer provided it is compatible with an existing computer
There are many ways in which the resources used by an algorithm can be measured the two most common measures are speed and memory usage other measures could include transmission speed temporary disk usage longterm disk usage power consumption total cost of ownership response time to external stimuli etc Many of these measures depend on the size of the input to the algorithm ie the amount of data to be processed They might also depend on the way in which the data is arranged for example some sorting algorithms perform poorly on data which is already sorted or which is sorted in reverse order
In practice there are other factors which can affect the efficiency of an algorithm such as requirements for accuracy andor reliability As detailed below the way in which an algorithm is implemented can also have a significant effect on actual efficiency though many aspects of this relate to optimization issues


==========
In the theoretical analysis of algorithms the normal practice is to estimate their complexity in the asymptotic sense The most commonly used notation to describe resource consumption or complexity is Donald Knuths Big O notation representing the complexity of an algorithm as a function of the size of the input 
  
    
      
        
          n
        
      
    
    textstyle scriptstyle n
   Big O notation is an asymptotic measure of function complexity where 
  
    
      
        
          
            f
            
              
              n
              
            
            
            =
            
            
              
                O
              
            
            
              
              
                g
                
                  
                  n
                  
                
              
              
            
          
        
      
    
    textstyle scriptstyle fleftnright=mathcal Oleftgleftnrightright
  roughly means the time requirement for an algorithm is proportional to 
  
    
      
        
          
            g
            
              
              n
              
            
          
        
      
    
    displaystyle scriptstyle gleftnright
   omitting lowerorder terms that contribute less than 
  
    
      
        
          
            g
            
              
              n
              
            
          
        
      
    
    displaystyle scriptstyle gleftnright
  to the growth of the function as 
  
    
      
        
          n
        
      
    
    displaystyle scriptstyle n
  grows arbitrarily large This estimate may be misleading when 
  
    
      
        
          n
        
      
    
    textstyle scriptstyle n
  is small but is generally sufficiently accurate when 
  
    
      
        
          n
        
      
    
    textstyle scriptstyle n
  is large as the notation is asymptotic For example bubble sort may be faster than merge sort when only a few items are to be sorted however either implementation is likely to meet performance requirements for a small list Typically programmers are interested in algorithms that scale efficiently to large input sizes and merge sort is preferred over bubble sort for lists of length encountered in most dataintensive programs
Some examples of Big O notation applied to algorithms asymptotic time complexity include


==========
For new versions of software or to provide comparisons with competitive systems benchmarks are sometimes used which assist with gauging an algorithms relative performance If a new sort algorithm is produced for example it can be compared with its predecessors to ensure that at least it is efficient as before with known data taking into consideration any functional improvements Benchmarks can be used by customers when comparing various products from alternative suppliers to estimate which product will best suit their specific requirements in terms of functionality and performance For example in the mainframe world certain proprietary sort products from independent software companies such as Syncsort compete with products from the major suppliers such as IBM for speed
Some benchmarks provide opportunities for producing an analysis comparing the relative speed of various compiled and interpreted languages for example
and The Computer Language Benchmarks Game compares the performance of implementations of typical programming problems in several programming languages
Even creating do it yourself benchmarks can demonstrate the relative performance of different programming languages using a variety of user specified criteria This is quite simple as a Nine language performance roundup by Christopher W CowellShah demonstrates by example


==========
Implementation issues can also have an effect on efficiency such as the choice of programming language or the way in which the algorithm is actually coded or the choice of a compiler for a particular language or the compilation options used or even the operating system being used In many cases a language implemented by an interpreter may be much slower than a language implemented by a compiler See the articles on justintime compilation and interpreted languages
There are other factors which may affect time or space issues but which may be outside of a programmers control these include data alignment data granularity cache locality cache coherency garbage collection instructionlevel parallelism multithreading at either a hardware or software level simultaneous multitasking and subroutine callsSome processors have capabilities for vector processing which allow a single instruction to operate on multiple operands it may or may not be easy for a programmer or compiler to use these capabilities Algorithms designed for sequential processing may need to be completely redesigned to make use of parallel processing or they could be easily reconfigured As parallel and distributed computing grow in importance in the late 2010s more investments are being made into efficient highlevel Application programming interfaces for parallel and distributed computing systems such as CUDA TensorFlow Hadoop OpenMP and MPI
Another problem which can arise in programming is that processors compatible with the same instruction set such as x8664 or ARM may implement an instruction in different ways so that instructions which are relatively fast on some models may be relatively slow on other models This often presents challenges to optimizing compilers which must have a great amount of knowledge of the specific CPU and other hardware available on the compilation target to best optimize a program for performance In the extreme case a compiler may be forced to emulate instructions not supported on a compilation target platform forcing it to generate code or link an external library call to produce a result that is otherwise incomputable on that platform even if it is natively supported and more efficient in hardware on other platforms This is often the case in embedded systems with respect to floatingpoint arithmetic where small and lowpower microcontrollers often lack hardware support for floatingpoint arithmetic and thus require computationally expensive software routines to produce floating point calculations


==========
Measures are normally expressed as a function of the size of the input 
  
    
      
        
          
            n
          
        
      
    
    displaystyle scriptstyle n
  
The two most common measures are

Time how long does the algorithm take to complete
Space how much working memory typically RAM is needed by the algorithm This has two aspects the amount of memory needed by the code auxiliary space usage and the amount of memory needed for the data on which the code operates intrinsic space usageFor computers whose power is supplied by a battery eg laptops and smartphones or for very longlarge calculations eg supercomputers other measures of interest are

Direct power consumption power needed directly to operate the computer
Indirect power consumption power needed for cooling lighting etcAs of 2018 power consumption is growing as an important metric for computational tasks of all types and at all scales ranging from embedded Internet of things devices to systemonchip devices to server farms This trend is often referred to as green computing
Less common measures of computational efficiency may also be relevant in some cases

Transmission size bandwidth could be a limiting factor Data compression can be used to reduce the amount of data to be transmitted Displaying a picture or image eg Google logo can result in transmitting tens of thousands of bytes 48K in this case compared with transmitting six bytes for the text Google This is important for IO bound computing tasks
External space space needed on a disk or other external memory device this could be for temporary storage while the algorithm is being carried out or it could be longterm storage needed to be carried forward for future reference
Response time latency this is particularly relevant in a realtime application when the computer system must respond quickly to some external event
Total cost of ownership particularly if a computer is dedicated to one particular algorithm


==========


============
Analyze the algorithm typically using time complexity analysis to get an estimate of the running time as a function of the size of the input data The result is normally expressed using Big O notation This is useful for comparing algorithms especially when a large amount of data is to be processed More detailed estimates are needed to compare algorithm performance when the amount of data is small although this is likely to be of less importance Algorithms which include parallel processing may be more difficult to analyze


============
Use a benchmark to time the use of an algorithm Many programming languages have an available function which provides CPU time usage For longrunning algorithms the elapsed time could also be of interest Results should generally be averaged over several tests
Runbased profiling can be very sensitive to hardware configuration and the possibility of other programs or tasks running at the same time in a multiprocessing and multiprogramming environment
This sort of test also depends heavily on the selection of a particular programming language compiler and compiler options so algorithms being compared must all be implemented under the same conditions


==========
This section is concerned with the use of memory resources registers cache RAM virtual memory secondary memory while the algorithm is being executed As for time analysis above analyze the algorithm typically using space complexity analysis to get an estimate of the runtime memory needed as a function as the size of the input data The result is normally expressed using Big O notation
There are up to four aspects of memory usage to consider

The amount of memory needed to hold the code for the algorithm
The amount of memory needed for the input data
The amount of memory needed for any output data
Some algorithms such as sorting often rearrange the input data and dont need any additional space for output data This property is referred to as inplace operation
The amount of memory needed as working space during the calculation
This includes local variables and any stack space needed by routines called during a calculation this stack space can be significant for algorithms which use recursive techniquesEarly electronic computers and early home computers had relatively small amounts of working memory For example the 1949 Electronic Delay Storage Automatic Calculator EDSAC had a maximum working memory of 1024 17bit words while the 1980 Sinclair ZX80 came initially with 1024 8bit bytes of working memory In the late 2010s it is typical for personal computers to have between 4 and 32 GB of RAM an increase of over 300 million times as much memory


============

Current computers can have relatively large amounts of memory possibly Gigabytes so having to squeeze an algorithm into a confined amount of memory is much less of a problem than it used to be But the presence of four different categories of memory can be significant

Processor registers the fastest of computer memory technologies with the least amount of storage space Most direct computation on modern computers occurs with source and destination operands in registers before being updated to the cache main memory and virtual memory if needed On a processor core there are typically on the order of hundreds of bytes or fewer of register availability although a register file may contain more physical registers than architectural registers defined in the instruction set architecture
Cache memory is the second fastest and second smallest memory available in the memory hierarchy Caches are present in CPUs GPUs hard disk drives and external peripherals and are typically implemented in static RAM Memory caches are multileveled lower levels are larger slower and typically shared between processor cores in multicore processors In order to process operands in cache memory a processing unit must fetch the data from the cache perform the operation in registers and write the data back to the cache This operates at speeds comparable about 210 times slower with the CPU or GPUs arithmetic logic unit or floatingpoint unit if in the L1 cache It is about 10 times slower if there is an L1 cache miss and it must be retrieved from and written to the L2 cache and a further 10 times slower if there is an L2 cache miss and it must be retrieved from an L3 cache if present
Main physical memory is most often implemented in dynamic RAM DRAM The main memory is much larger typically gigabytes compared to ≈8 megabytes than an L3 CPU cache with read and write latencies typically 10100 times slower As of 2018 RAM is increasingly implemented onchip of processors as CPU or GPU memory
Virtual memory is most often implemented in terms of secondary storage such as a hard disk and is an extension to the memory hierarchy that has much larger storage space but much larger latency typically around 1000 times slower than a cache miss for a value in RAM While originally motivated to create the impression of higher amounts of memory being available than were truly available virtual memory is more important in contemporary usage for its timespace tradeoff and enabling the usage of virtual machines Cache misses from main memory are called page faults and incur huge performance penalties on programsAn algorithm whose memory needs will fit in cache memory will be much faster than an algorithm which fits in main memory which in turn will be very much faster than an algorithm which has to resort to virtual memory Because of this cache replacement policies are extremely important to highperformance computing as are cacheaware programming and data alignment To further complicate the issue some systems have up to three levels of cache memory with varying effective speeds Different systems will have different amounts of these various types of memory so the effect of algorithm memory needs can vary greatly from one system to another
In the early days of electronic computing if an algorithm and its data wouldnt fit in main memory then the algorithm couldnt be used Nowadays the use of virtual memory appears to provide lots of memory but at the cost of performance If an algorithm and its data will fit in cache memory then very high speed can be obtained in this case minimizing space will also help minimize time This is called the principle of locality and can be subdivided into locality of reference spatial locality and temporal locality An algorithm which will not fit completely in cache memory but which exhibits locality of reference may perform reasonably well


==========
David May FRS a British computer scientist and currently Professor of Computer Science at University of Bristol and founder and CTO of XMOS Semiconductor believes one of the problems is that there is a reliance on Moores law to solve inefficiencies He has advanced an alternative to Moores law Mays law stated as follows
Software efficiency halves every 18 months compensating Moores Law

May goes on to state
In ubiquitous systems halving the instructions executed can double the battery life and big data sets bring big opportunities for better software and algorithms Reducing the number of operations from N x N to N x logN has a dramatic effect when N is large  for N = 30 billion this change is as good as 50 years of technology improvements

Software author Adam N Rosenburg in his blog The failure of the Digital computer has described the current state of programming as nearing the Software event horizon alluding to the fictitious shoe event horizon described by Douglas Adams in his Hitchhikers Guide to the Galaxy book He estimates there has been a 70 dB factor loss of productivity or 9999999 percent of its ability to deliver the goods since the 1980s—When Arthur C Clarke compared the reality of computing in 2001 to the computer HAL 9000 in his book 2001 A Space Odyssey he pointed out how wonderfully small and powerful computers were but how disappointing computer programming had become


==========
The following competitions invite entries for the best algorithms based on some arbitrary criteria decided by the judges

Wired magazine


==========
Analysis of algorithms—how to determine the resources needed by an algorithm
Arithmetic coding—a form of variablelength entropy encoding for efficient data compression
Associative array—a data structure that can be made more efficient using Patricia trees or Judy arrays
Benchmark—a method for measuring comparative execution times in defined cases
Best worst and average case—considerations for estimating execution times in three scenarios
Binary search algorithm—a simple and efficient technique for searching sorted arrays
Branch table—a technique for reducing instruction pathlength size of machine code and often also memory
Comparison of programming paradigms—paradigm specific performance considerations
Compiler optimization—compilerderived optimization
Computational complexity of mathematical operations
Computational complexity theory
Computer performance—computer hardware metrics
Data compression—reducing transmission bandwidth and disk storage
Database index—a data structure that improves the speed of data retrieval operations on a database table
Entropy encoding—encoding data efficiently using frequency of occurrence of strings as a criterion for substitution
Garbage collection—automatic freeing of memory after use
Green computing—a move to implement greener technologies consuming less resources
Huffman algorithm—an algorithm for efficient data encoding
Improving Managed code Performance—Microsoft MSDN Library
Locality of reference—for avoidance of caching delays caused by nonlocal memory access
Loop optimization
Memory management
Optimization computer science
Performance analysis—methods of measuring actual performance of an algorithm at runtime
Realtime computing—further examples of timecritical applications
Runtime analysis—estimation of expected runtimes and an algorithms scalability
Simultaneous multithreading
Sorting algorithm § Comparison of algorithms
Speculative execution or Eager execution
Branch prediction
Superthreading
Hyperthreading
Threaded code—similar to virtual method table or branch table
Virtual method table—branch table with dynamically assigned pointers for dispatching


==========


==========
Animation of the BoyerMoore algorithm Java Applet
How algorithms shape our world   A TED conference Talk by Kevin Slavin
Misconceptions about algorithmic efficiency in highschoolsRobert John Downey Jr born April 4 1965 is an American actor producer and singer His career has been characterized by critical and popular notoriety in his youth followed by a period of substance abuse and legal troubles before a resumption of critical repute and resurgence of commercial success in middle age In 2008 Downey was named by Time magazine among the 100 most influential people in the world and from 2013 to 2015 he was listed by Forbes as Hollywoods single highestpaid actor His films have grossed over 58 billion in North America and 144 billion internationally making him the secondhighestgrossing boxoffice actor to date both domestically and worldwideAt the age of five he made his acting debut in Robert Downey Srs film Pound in 1970 His subsequent films until the end of the 80s brought him in association with the group of actors called the Brat Pack to include the teen scifi comedy Weird Science and the drama Less Than Zero In 1992 Downey secured a height of critical regard when he took on the title character in the film Chaplin for which he was nominated for the Academy Award for Best Actor and won the equivalent category at the BAFTA Awards Following a stint at the Corcoran Substance Abuse Treatment Facility sentenced on drug charges he joined the eminent TV series Ally McBeal Playing the love interest of Calista Flockhart he won a Golden Globe Award however in the wake of two drug charges one in late 2000 and one in early 2001 he was fired and his character terminated He stayed in a courtordered drug treatment program shortly after and has maintained his sobriety since 2003
Initially bond completion companies would not insure Downey for roles in feature films Mel Gibson who had been a close friend to Downey since both had costarred in Air America paid the insurance bond for the 2003 film The Singing Detective Downeys performance in that film paved the way for his return to feature films including a role in the black comedy crime film Kiss Kiss Bang Bang 2005 the mystery thriller Zodiac 2007 and the satirical action comedy Tropic Thunder 2008 for the latter he was nominated for an Academy Award for Best Supporting Actor Downey went on to star as the Marvel Comics character Tony Stark in ten films within the Marvel Cinematic Universe beginning with Iron Man 2008 and concluding with Avengers Endgame 2019 He has also played the title character in Guy Ritchies Sherlock Holmes 2009 which earned him his second Golden Globe win and its sequel 2011


==========
Downey was born in Manhattan New York the younger of two children His father Robert Downey Sr is an actor and filmmaker while his mother Elsie Ann née Ford was an actress who appeared in Downey Srs films Downeys father is of half Lithuanian Jewish onequarter Hungarian Jewish and onequarter Irish descent while Downeys mother had Scottish German and Swiss ancestry Roberts original family name was Elias which was changed by his father to enlist in the Army Downey and his older sister Allyson grew up in Greenwich VillageAs a child Downey was surrounded by drugs His father a drug addict allowed Downey to use marijuana at age six an incident which his father later said he regretted Downey later stated that drug use became an emotional bond between him and his father When my dad and I would do drugs together it was like him trying to express his love for me in the only way he knew how Eventually Downey began spending every night abusing alcohol and making a thousand phone calls in pursuit of drugsDuring his childhood Downey had minor roles in his fathers films He made his acting debut at the age of five playing a sick puppy in the absurdist comedy Pound 1970 and then at seven appeared in the surrealist Greasers Palace 1972 At the age of 10 he was living in England and studied classical ballet as part of a larger curriculum He attended the Stagedoor Manor Performing Arts Training Center in upstate New York as a teenager When his parents divorced in 1978 Downey moved to California with his father but in 1982 he dropped out of Santa Monica High School and moved back to New York to pursue an acting career fulltimeDowney and Kiefer Sutherland who shared the screen in the 1988 drama 1969 were roommates for three years when he first moved to Hollywood to pursue his career in acting


==========


==========
Downey began building upon theater roles including in the shortlived offBroadway musical American Passion at the Joyce Theater in 1983 produced by Norman Lear In 1985 he was part of the new younger cast hired for Saturday Night Live but following a year of poor ratings and criticism of the new casts comedic talents he and most of the new crew were dropped and replaced Rolling Stone magazine named Downey the worst SNL cast member in its entire run stating that the Downey Fail sums up everything that makes SNL great That same year Downey had a dramatic acting breakthrough when he played James Spaders characters sidekick in Tuff Turf and then a bully in John Hughess Weird Science He was considered for the role of Duckie in John Hughess film Pretty in Pink 1986 but his first lead role was with Molly Ringwald in The Pickup Artist 1987 Because of these and other comingofage films Downey did during the 1980s he is sometimes named as a member of the Brat PackIn 1987 Downey played Julian Wells a drugaddicted rich boy whose life rapidly spirals out of his control in the film version of the Bret Easton Ellis novel Less Than Zero His performance described by Janet Maslin in The New York Times as desperately moving was widely praised though Downey has said that for him the role was like the ghost of Christmas Future since his drug habit resulted in his becoming an exaggeration of the character in real life Zero drove Downey into films with bigger budgets and names such as Chances Are 1989 with Cybill Shepherd and Ryan ONeal Air America 1990 with Mel Gibson and Soapdish 1991 with Sally Field Kevin Kline and Whoopi GoldbergIn 1992 he starred as Charlie Chaplin in Chaplin a role for which he prepared extensively learning how to play the violin as well as tennis lefthanded He had a personal coach in order to help him imitate Chaplins posture and a way of carrying himself The role garnered Downey an Academy Award nomination for Best Actor at the Academy Awards 65th ceremony losing to Al Pacino in Scent of a WomanIn 1993 he appeared in the films Heart and Souls with Alfre Woodard and Kyra Sedgwick and Short Cuts with Matthew Modine and Julianne Moore along with a documentary that he wrote about the 1992 presidential campaigns titled The Last Party 1993 He starred in the 1994 films Only You with Marisa Tomei and Natural Born Killers with Woody Harrelson He then subsequently appeared in Restoration 1995 Richard III 1995 Two Girls and a Guy 1997 as Special Agent John Royce in US Marshals 1998 and in Black and White 1999


==========

From 1996 through 2001 Downey was arrested numerous times on charges related to drugs including cocaine heroin and marijuana and went through drug treatment programs unsuccessfully explaining in 1999 to a judge Its like I have a shotgun in my mouth and Ive got my finger on the trigger and I like the taste of the gun metal He explained his relapses by claiming to have been addicted to drugs since the age of eight due to the fact that his father also an addict previously had been giving them to himIn April 1996 Downey was arrested for possession of heroin cocaine and an unloaded 357 Magnum handgun while he was speeding down Sunset Boulevard A month later while on parole he trespassed into a neighbors home while under the influence of a controlled substance and fell asleep in one of the beds He received three years of probation and was ordered to undergo compulsory drug testing In 1997 he missed one of the courtordered drug tests and had to spend six months in the Los Angeles County jailAfter Downey missed another required drug test in 1999 he was arrested once more Despite Downeys lawyer John Stewart Holden assembling the same team of lawyers that successfully defended OJ Simpson during his criminal trial for murder Downey was sentenced to a threeyear prison term at the California Substance Abuse Treatment Facility and State Prison in Corcoran California At the time of the 1999 arrest all of Downeys film projects had wrapped and were close to release He had also been hired for voicing the devil on the NBC animated television series God the Devil and Bob but was fired when he failed to show up for rehearsalsAfter spending nearly a year in the California Substance Abuse Treatment Facility and State Prison Downey on condition of posting a 5000 bail was unexpectedly freed when a judge ruled that his collective time in incarceration facilities spawned from the initial 1996 arrests had qualified him for early release A week after his 2000 release Downey joined the cast of the hit television series Ally McBeal playing the new love interest of Calista Flockharts title character His performance was praised and the following year he was nominated for an Emmy Award in the Outstanding Supporting Actor in a Comedy Series category and won a Golden Globe for Best Supporting Actor in a miniseries or television film He also appeared as a writer and singer on Vonda Shepards Ally McBeal For Once in My Life album and he sang with Sting a duet of Every Breath You Take in an episode of the series Despite the apparent success Downey claimed that his performance on the series was overrated and said It was my lowest point in terms of addictions At that stage I didnt give a fuck whether I ever acted again In January 2001 Downey was scheduled to play the role of Hamlet in a Los Angeles stage production directed by Mel GibsonBefore the end of his first season on Ally McBeal over the Thanksgiving 2000 holiday Downey was arrested when his room at Merv Griffins Hotel and Givenchy Spa in Palm Springs California was searched by the police who were responding to an anonymous 911 call Downey was under the influence of a controlled substance and in possession of cocaine and Valium Despite the fact that if convicted he would have faced a prison sentence of up to four years and eight months he signed on to appear in at least eight more Ally McBeal episodesIn April 2001 while he was on parole a Los Angeles police officer found him wandering barefooted in Culver City just outside Los Angeles He was arrested for suspicion of being under the influence of drugs but was released a few hours later even though tests showed he had cocaine in his system After this last arrest producer David E Kelley and other Ally McBeal executives ordered lastminute rewrites and reshoots and fired Downey from the show despite the fact that Downeys character had resuscitated Ally McBeals ratings The Culver City arrest also cost him a role in the highprofile film Americas Sweethearts and the subsequent incarceration prompted Mel Gibson to shut down his planned stage production of Hamlet as well In July 2001 Downey pleaded no contest to the Palm Springs charges avoiding jail time Instead he was sent into drug rehabilitation and received three years of probation benefiting from California Proposition 36 which had been passed the year before with the aim of helping nonviolent drug offenders overcome their addictions instead of sending them to jailThe book Conversations with Woody Allen reports that director Woody Allen wanted to cast Downey and Winona Ryder in his film Melinda and Melinda in 2005 but was unable to do so because he could not get insurance on them stating We couldnt get bonded The completion bonding companies would not bond the picture unless we could insure them We were heartbroken because I had worked with Winona before on Celebrity and thought she was perfect for this and wanted to work with her again And I had always wanted to work with Bob Downey and always thought he was a huge talentIn a December 18 2000 article for People magazine entitled Bad to Worse Downeys stepmother Rosemary told author Alex Tresnlowski that Downey had been diagnosed with bipolar disorder a few years ago and added that his bipolar disorder was the reason he has a hard time staying sober What hasnt been tried is medication and intensive psychotherapy In the same article Dr Manijeh Nikakhtar a Los Angeles psychiatrist and coauthor of Addiction or SelfMedication The Truth claimed she received a letter from Downey in 1999 during his time at Corcoran II asking for advice on his condition She discovered that no one had done a complete psychiatric evaluation on him  I asked him flat out if he thought he was bipolar and he said Oh yeah There are times I spend a lot of money and Im hyperactive and there are other times Im down In an article for the March 2007 issue of Esquire Downey stated that he wanted to address this whole thing about the bipolar after receiving a phone call from the Bipolar Association asking him about being bipolar When Downey denied he had ever said he was bipolar the caller quoted the People article to which Downey replied No Dr Malibusian said I said I was bipolar   and they go Well its been written so were going to quote it Downey flatly denied being depressed or manic and that previous attempts to diagnose him with any kind of psychiatric or mood disorder have always been skewed because the guy I was seeing didnt know I was smokin crack in his bathroom You cant make a diagnosis until somebodys sober


==========

After five years of substance abuse arrests rehab and relapse Downey was ready to work toward a full recovery from drugs and a return to his career In discussing his failed attempts to control his own addictive behavior in the past Downey told Oprah Winfrey in November 2004 that when someone says I really wonder if maybe I should go to rehab Well uh youre a wreck you just lost your job and your wife left you Uh you might want to give it a shot He added that after his last arrest in April 2001 when he knew he would likely be facing another stint in prison or another form of incarceration such as courtordered rehab I said You know what I dont think I can continue doing this And I reached out for help and I ran with it You can reach out for help in kind of a halfassed way and youll get it and you wont take advantage of it Its not that difficult to overcome these seemingly ghastly problems  whats hard is to decide to do itDowney got his first postrehabilitation acting job in August 2001 lipsyncing in the video for Elton Johns single I Want Love Video director Sam TaylorWood shot 16 takes of the video and used the last one because according to John Downey looked completely relaxed and The way he underplays it is fantasticDowney was able to return to the big screen after Mel Gibson who had been a close friend to Downey since both had costarred in Air America paid Downeys insurance bond for the 2003 film The Singing Detective directed by his Back To School costar Keith Gordon Gibsons gamble paved the way for Downeys comeback and Downey returned to mainstream films in the mid2000s with Gothika for which producer Joel Silver withheld 40 of his salary until after production wrapped as insurance against his addictive behavior Similar clauses have become standard in his contracts since Silver who was getting closer to Downey as he dated his assistant Susan Levin also got the actor the leading role in the comedy thriller Kiss Kiss Bang Bang the directorial debut of screenwriter Shane BlackAfter Gothika Downey was cast in a number of leading and supporting roles including wellreceived work in a number of semiindependent films A Guide to Recognizing Your Saints Good Night and Good Luck Richard Linklaters dystopian rotoscoped A Scanner Darkly in which Downey plays the role of a drug addict and Steven Shainbergs fictional biographical film of Diane Arbus Fur where Downeys character represented the two biggest influences on Arbuss professional life Lisette Model and Marvin Israel Downey also received great notice for his roles in more mainstream fare such as Kiss Kiss Bang Bang and Disneys poorly received The Shaggy DogOn November 23 2004 Downey released his debut musical album The Futurist on Sony Classical for which he designed the cover art and designed the track listing label on the CD with his son Indio The album received mixed reviews but Downey stated in 2006 that he probably will not do another album as he felt that the energy he put into doing the album was not compensatedIn 2006 Downey returned to television when he gueststarred on Family Guy in the episode The Fat Guy Strangler Downey had previously telephoned the shows production staff and asked if he could produce or assist in an episode creation as his son Indio is a fan of the show The producers of the show accepted the offer and created the character of Patrick Pewterschmidt Lois Griffins long lost mentally disturbed brother for DowneyDowney signed on with publishers HarperCollins to write a memoir which in 2006 was already being billed as a candid look at the highs and lows of his life and career In 2008 however Downey returned his advance to the publishers and canceled the book without further commentIn 2007 Downey appeared in David Finchers mystery thriller Zodiac which was based on a true story He played the role of San Francisco Chronicle journalist Paul Avery who was reporting the Zodiac Killer case


==========

With all of the critical success Downey had experienced throughout his career he had not appeared in a blockbuster film That changed in 2008 when Downey starred in two critically and commercially successful films Iron Man and Tropic Thunder In the article Ben Stiller wrote for Downeys entry in the 2008 edition of The Time 100 he offered an observation on Downeys commercially successful summer at the box office

Yes Downey is Iron Man but he really is Actor Man  In the realm where box office is irrelevant and talent is king the realm that actually means something he has always ruled and finally this summer he gets to have his cake and let us eat him up all the way to the multiplex where his mastery is in full effect

In 2007 Downey was cast as the title character in the film Iron Man with director Jon Favreau explaining the choice by stating Downey wasnt the most obvious choice but he understood what makes the character tick He found a lot of his own life experience in Tony Stark Favreau insisted on having Downey as he repeatedly claimed that Downey would be to Iron Man what Johnny Depp is to the Pirates of the Caribbean series a lead actor who could both elevate the quality of the film and increase the publics interest in it For the role Downey had to gain more than 20 pounds of muscle in five months to look like he had the power to forge ironIron Man was globally released between April 30 and May 3 2008 grossing over 585 million worldwide and receiving rave reviews which cited Downeys performance as a highlight of the film By October 2008 Downey had agreed to appear as Iron Man in two Iron Man sequels as part of the Iron Man franchise as well as The Avengers featuring the superhero team that Stark joins based on Marvels comic book series The Avengers He first reprised the role in a small appearance as Iron Mans alter ego Tony Stark in the 2008 film The Incredible Hulk as a part of Marvel Studios depicting the same Marvel Universe on film by providing continuity among the moviesAfter Iron Man Downey appeared alongside Ben Stiller and Jack Black in the Stillerdirected Tropic Thunder The three actors play a Hollywood archetype—with Downey playing selfabsorbed multiOscarwinning Australian method actor Kirk Lazarus—as they star in an extremely expensive Vietnamera film called Tropic Thunder Lazarus undergoes a controversial skin pigmentation procedure in order to take on the role of AfricanAmerican platoon sergeant Lincoln Osiris which required Downey to wear dark makeup and a wig Both Stiller and Downey feared Downeys portrayal of the character could become controversial

Stiller says that he and Downey always stayed focused on the fact that they were skewering insufferable actors not African Americans I was trying to push it as far as you can within reality Stiller explains I had no idea how people would respond to it Stiller screened a rough cut of the film in March 2008 and it scored high with African Americans He was relieved at the reaction It seems people really embrace it he said
When asked by Harry Smith on CBSs The Early Show who his model was for Lazarus Downey laughed before responding Sadly my sorryass selfReleased in the United States on August 13 2008 Tropic Thunder received good reviews with 83 of reviews positive and an average normalized score of 71 according to the review aggregator websites Rotten Tomatoes and Metacritic respectively It earned US26 million in its North American opening weekend and retained the number one position for its first three weekends of release The film grossed 180 million in theaters before its release on home video on November 18 2008 Downey was nominated for the Academy Award for Best Supporting Actor for his portrayal of Lazarus

Opening in late April 2009 was a film Downey finished in mid2008 The Soloist The film was delayed from a November 2008 release by Paramount Pictures due to the studios tight endofyear release schedule Critics who had seen the film in 2008 were mentioning it as a possible Academy Award candidate Downey picked up an Academy Award nomination for the 2008 release year for his role in Tropic ThunderThe first role Downey accepted after Iron Man was the title character in Guy Ritchies Sherlock Holmes Warner Bros released it on December 25 2009 The film set several box office records in the United States for a Christmas Day release beating the previous record holder 2008s Marley  Me by nearly 10M and finished second to Avatar in a recordsetting Christmas weekend box office Sherlock Holmes ended up being the 8th highestgrossing film of 2009 When Downey won the Golden Globe for Best Actor in a Motion Picture Musical or Comedy from the Hollywood Foreign Press Association for his role as Sherlock Holmes he noted in his acceptance speech that he had prepared no remarks because Susan Downey his wife and Sherlock Holmes producer told me that Matt Damon nominated for his role in The Informant was going to win so dont bother preparing a speechDowney returned as Tony Stark in the first of two planned sequels to Iron Man Iron Man 2 which released in May 2010 Iron Man 2 grossed over 623M worldwide becoming the 7th highestgrossing film of 2010Downeys other commercial film release of 2010 was the comedy road film Due Date The movie costarring Zach Galifianakis was released in November 2010 and grossed over 211M worldwide making it the 36th highestgrossing movie of 2010 Downeys sole 2011 film credit was the sequel to the 2009 version of Sherlock Holmes Sherlock Holmes A Game of Shadows which opened worldwide on December 16 2011In 2012 Downey reprised the role of Tony Stark in The Avengers The film received positive reviews and was highly successful at the box office becoming the third highestgrossing film of all time both in the United States and worldwide His film the David Dobkindirected dramedy The Judge a project coproduced by his production company Team Downey was the opening film at the Toronto International Film Festival in 2014 Downey played Tony Stark again in Iron Man 3 2013 Avengers Age of Ultron 2015 Captain America Civil War 2016 SpiderMan Homecoming 2017 Avengers Infinity War 2018 and Avengers Endgame 2019


==========
Downey is scheduled to star in The Voyage of Doctor Dolittle Downey will reprise his role as Holmes in a third film scheduled for release on December 25 2020


==========


==========
Downey has sung on several soundtracks for his films including for Chaplin Too Much Sun Two Girls and a Guy Friends and Lovers The Singing Detective and Kiss Kiss Bang Bang In 2001 he appeared in the music video for Elton Johns song I Want Love He released a CD in 2004 called The Futurist and while promoting his film Tropic Thunder he and his costars Ben Stiller and Jack Black were backup singers The Pips to Gladys Knight singing Midnight Train to GeorgiaDowneys most commercially successful recording venture to date combining sales and radio airplay has been his remake of the 1973 Joni Mitchell Christmas song River which was included on the Ally McBeal tiein album Ally McBeal A Very Ally Christmas released in 2000 Downeys character Larry Paul performs the song in the Ally McBeal episode Tis the Season


==========
On June 14 2010 Downey and his wife Susan opened their own production company called Team Downey Their first project was The Judge


==========


==========
Downey started dating actress Sarah Jessica Parker after meeting her on the set of Firstborn The couple later separated due to his drug addictionHe married actress and singer Deborah Falconer on May 29 1992 after a 42day courtship Their son Indio Falconer Downey was born in September 1993 The strain on their marriage from Downeys repeated trips to rehab and jail finally reached a breaking point in 2001 in the midst of Downeys last arrest and sentencing to an extended stay in rehab Falconer left Downey and took their son with her Downey and Falconer finalized their divorce on April 26 2004

In 2003 Downey met producer Susan Levin an Executive Vice President of Production at Joel Silvers film company Silver Pictures on the set of Gothika Though Susan twice turned down his amorous advances she and Downey did quietly strike up a romance during production Despite Susans worries that the romance would not last after the completion of shooting because hes an actor I have a real job the couples relationship continued after production wrapped on Gothika and Downey proposed to Susan on the night before her thirtieth birthday In August 2005 the couple were married in a Jewish ceremony at Amagansett New York A tattoo on one of his biceps reads Suzie Q in tribute to her Their first child a son was born in February 2012 and their second a daughter was born in November 2014Downey has been a close friend of Mel Gibson since they starred in Air America Downey defended Gibson during the controversy surrounding The Passion of the Christ and said nobodys perfect in reference to Gibsons DUI Gibson said of Downey He was one of the first people to call and offer the hand of friendship He just said Hey welcome to the club Lets go see what we can do to work on ourselves In October 2011 Downey was being honored at the 25th American Cinematheque Awards Downey chose Gibson to present him with his award for his lifes work and used his air time to say a few kind words about Gibson and explain why he chose him to present the award


==========
Downey maintains that he has been drugfree since July 2003 and has credited his wife with helping him overcome his drug and alcohol habits along with his family therapy meditation twelvestep recovery programs yoga and the practice of Wing Chun kung fu the martial art he learned from Eric Oram who is also a fight consultant in several of Downeys movies Oram was Downeys personal fight coordinator in Avengers Age of Ultron and Captain America Civil War In December 2015 Downey received a full and unconditional pardon from Governor of California Jerry Brown for his prior drug convictions Oram wrote a letter in support of Downeys pardon to Governor Brown


==========
Downey has described his religious beliefs as Jewish Buddhist and he is reported to have consulted astrologers In the past Downey has been interested in Christianity and the Hare Krishna movement


==========
In a 2008 interview Downey stated that his time in prison changed his political point of view somewhat saying I have a really interesting political point of view and its not always something I say too loud at dinner tables here but you cant go from a 2000anight suite at La Mirage to a penitentiary and really understand it and come out a liberal You cant I wouldnt wish that experience on anyone else but it was very very very educational for me and has informed my proclivities and politics ever since However when asked about the quote in a 2015 interview to promote Avengers Age of Ultron he denied that his previous statement reflected any longstanding beliefs on his part and stated I wouldnt say that Im a Republican or a liberal or a DemocratDowney serves on the board of the AntiRecidivism CoalitionIn 2016 Downey appeared in an antiTrump commercial with other celebrities encouraging people to register to vote in the 2016 election


==========


==========


==========
The Futurist 2004


==========


==========


==========
Robert Downey Jr on Twitter
Robert Downey Jr on IMDb 
Robert Downey Jr at Box Office Mojo
Robert Downey Jr Interview The Games Afoot at Los Angeles Times Magazine
Robert Downey Jr interview at wwwreviewgraveyardcomIn computer science the analysis of algorithms is the determination of the computational complexity of algorithms that is the amount of time storage andor other resources necessary to execute them Usually this involves determining a function that relates the length of an algorithms input to the number of steps it takes its time complexity or the number of storage locations it uses its space complexity An algorithm is said to be efficient when this functions values are small or grow slowly compared to a growth in the size of the input Different inputs of the same length may cause the algorithm to have different behavior so best worst and average case descriptions might all be of practical interest  When not otherwise specified the function describing the performance of an algorithm is usually an upper bound determined from the worst case inputs to the algorithm
The term analysis of algorithms was coined by Donald Knuth Algorithm analysis is an important part of a broader computational complexity theory which provides theoretical estimates for the resources needed by any algorithm which solves a given computational problem These estimates provide an insight into reasonable directions of search for efficient algorithms
In theoretical analysis of algorithms it is common to estimate their complexity in the asymptotic sense ie to estimate the complexity function for arbitrarily large input Big O notation Bigomega notation and Bigtheta notation are used to this end For instance binary search is said to run in a number of steps proportional to the logarithm of the length of the sorted list being searched or in Ologn colloquially in logarithmic time Usually asymptotic estimates are used because different implementations of the same algorithm may differ in efficiency However the efficiencies of any two reasonable implementations of a given algorithm are related by a constant multiplicative factor  called a hidden constant
Exact not asymptotic measures of efficiency can sometimes be computed but they usually require certain assumptions concerning the particular implementation of the algorithm called model of computation A model of computation may be defined in terms of an abstract computer eg Turing machine andor by postulating that certain operations are executed in unit time
For example if the sorted list to which we apply binary search has n elements and we can guarantee that each lookup of an element in the list can be done in unit time then at most log2 n + 1 time units are needed to return an answer


==========
Time efficiency estimates depend on what we define to be a step For the analysis to correspond usefully to the actual execution time the time required to perform a step must be guaranteed to be bounded above by a constant One must be careful here for instance some analyses count an addition of two numbers as one step This assumption may not be warranted in certain contexts For example if the numbers involved in a computation may be arbitrarily large the time required by a single addition can no longer be assumed to be constant
Two cost models are generally used
the uniform cost model also called uniformcost measurement and similar variations assigns a constant cost to every machine operation regardless of the size of the numbers involved
the logarithmic cost model also called logarithmiccost measurement and similar variations assigns a cost to every machine operation proportional to the number of bits involvedThe latter is more cumbersome to use so its only employed when necessary for example in the analysis of arbitraryprecision arithmetic algorithms like those used in cryptography
A key point which is often overlooked is that published lower bounds for problems are often given for a model of computation that is more restricted than the set of operations that you could use in practice and therefore there are algorithms that are faster than what would naively be thought possible


==========
Runtime analysis is a theoretical classification that estimates and anticipates the increase in running time or runtime of an algorithm as its input size usually denoted as n increases  Runtime efficiency is a topic of great interest in computer science  A program can take seconds hours or even years to finish executing depending on which algorithm it implements While software profiling techniques can be used to measure an algorithms runtime in practice they cannot provide timing data for all infinitely many possible inputs the latter can only be achieved by the theoretical methods of runtime analysis


==========
Since algorithms are platformindependent ie a given algorithm can be implemented in an arbitrary programming language on an arbitrary computer running an arbitrary operating system there are additional significant drawbacks to using an empirical approach to gauge the comparative performance of a given set of algorithms
Take as an example a program that looks up a specific entry in a sorted list of size n  Suppose this program were implemented on Computer A a stateoftheart machine using a linear search algorithm and on Computer B a much slower machine using a binary search algorithm  Benchmark testing on the two computers running their respective programs might look something like the following

Based on these metrics it would be easy to jump to the conclusion that Computer A is running an algorithm that is far superior in efficiency to that of Computer B  However if the size of the inputlist is increased to a sufficient number that conclusion is dramatically demonstrated to be in error

Computer A running the linear search program exhibits a linear growth rate  The programs runtime is directly proportional to its input size  Doubling the input size doubles the run time quadrupling the input size quadruples the runtime and so forth  On the other hand Computer B running the binary search program exhibits a logarithmic growth rate  Quadrupling the input size only increases the run time by a constant amount in this example 50000 ns  Even though Computer A is ostensibly a faster machine Computer B will inevitably surpass Computer A in runtime because its running an algorithm with a much slower growth rate


==========

Informally an algorithm can be said to exhibit a growth rate on the order of a mathematical function if beyond a certain input size n the function 
  
    
      
        f
        
        n
        
      
    
    displaystyle fn
   times a positive constant provides an upper bound or limit for the runtime of that algorithm  In other words for a given input size n greater than some n0 and a constant c the running time of that algorithm will never be larger than 
  
    
      
        c
        ×
        f
        
        n
        
      
    
    displaystyle ctimes fn
    This concept is frequently expressed using Big O notation  For example since the runtime of insertion sort grows quadratically as its input size increases insertion sort can be said to be of order On2
Big O notation is a convenient way to express the worstcase scenario for a given algorithm although it can also be used to express the averagecase — for example the worstcase scenario for quicksort is On2 but the averagecase runtime is On log n


==========
Assuming the execution time follows power rule t ≈ k na the coefficient a can be found  by taking empirical measurements of run time 
  
    
      
        
        
          t
          
            1
          
        
        
        
          t
          
            2
          
        
        
      
    
    displaystyle t1t2
   at some problemsize points 
  
    
      
        
        
          n
          
            1
          
        
        
        
          n
          
            2
          
        
        
      
    
    displaystyle n1n2
   and calculating 
  
    
      
        
          t
          
            2
          
        
        
          
        
        
          t
          
            1
          
        
        =
        
        
          n
          
            2
          
        
        
          
        
        
          n
          
            1
          
        
        
          
          
            a
          
        
      
    
    displaystyle t2t1=n2n1a
   so that 
  
    
      
        a
        =
        log
        ⁡
        
        
          t
          
            2
          
        
        
          
        
        
          t
          
            1
          
        
        
        
          
        
        log
        ⁡
        
        
          n
          
            2
          
        
        
          
        
        
          n
          
            1
          
        
        
      
    
    displaystyle a=logt2t1logn2n1
   In other words this measures the slope of the empirical line on the log–log plot of execution time vs problem size at some size point If the order of growth indeed follows the power rule and so the line on log–log plot is indeed a straight line the empirical value of a will  stay constant at different ranges and if not it will change and the line is a curved line  but still could serve for comparison of any two given algorithms as to their empirical local orders of growth behaviour Applied to the above table

It is clearly seen that the first algorithm exhibits a linear order of growth indeed following the power rule The empirical values for the second one are diminishing rapidly suggesting it follows another rule of growth and in any case has much lower local orders of growth and improving further still empirically than the first one


==========
The runtime complexity for the worstcase scenario of a given algorithm can sometimes be evaluated by examining the structure of the algorithm and making some simplifying assumptions  Consider the following pseudocode

1    get a positive integer from input
2    if n  10
3        print This might take a while
4    for i = 1 to n
5        for j = 1 to i
6            print i  j
7    print Done

A given computer will take a discrete amount of time to execute each of the instructions involved with carrying out this algorithm  The specific amount of time to carry out a given instruction will vary depending on which instruction is being executed and which computer is executing it but on a conventional computer this amount will be deterministic  Say that the actions carried out in step 1 are considered to consume time T1 step 2 uses time T2 and so forth
In the algorithm above steps 1 2 and 7 will only be run once  For a worstcase evaluation it should be assumed that step 3 will be run as well  Thus the total amount of time to run steps 13 and step 7 is

  
    
      
        
          T
          
            1
          
        
        +
        
          T
          
            2
          
        
        +
        
          T
          
            3
          
        
        +
        
          T
          
            7
          
        
        
        
      
    
    displaystyle T1+T2+T3+T7
  The loops in steps 4 5 and 6 are trickier to evaluate  The outer loop test in step 4 will execute  n + 1 
times note that an extra step is required to terminate the for loop hence n + 1 and not n executions which will consume T4 n + 1  time  The inner loop on the other hand is governed by the value of j which iterates from 1 to i  On the first pass through the outer loop j iterates from 1 to 1  The inner loop makes one pass so running the inner loop body step 6 consumes T6 time and the inner loop test step 5 consumes 2T5 time  During the next pass through the outer loop j iterates from 1 to 2  the inner loop makes two passes so running the inner loop body step 6 consumes 2T6 time and the inner loop test step 5 consumes 3T5 time
Altogether the total time required to run the inner loop body can be expressed as an arithmetic progression

  
    
      
        
          T
          
            6
          
        
        +
        2
        
          T
          
            6
          
        
        +
        3
        
          T
          
            6
          
        
        +
        ⋯
        +
        
        n
        −
        1
        
        
          T
          
            6
          
        
        +
        n
        
          T
          
            6
          
        
      
    
    displaystyle T6+2T6+3T6+cdots +n1T6+nT6
  which can be factored as

  
    
      
        
          T
          
            6
          
        
        
          
          
            1
            +
            2
            +
            3
            +
            ⋯
            +
            
            n
            −
            1
            
            +
            n
          
          
        
        =
        
          T
          
            6
          
        
        
          
          
            
              
                1
                2
              
            
            
            
              n
              
                2
              
            
            +
            n
            
          
          
        
      
    
    displaystyle T6left1+2+3+cdots +n1+nright=T6leftfrac 12n2+nright
  The total time required to run the outer loop test can be evaluated similarly

  
    
      
        
          
            
              
              
                2
                
                  T
                  
                    5
                  
                
                +
                3
                
                  T
                  
                    5
                  
                
                +
                4
                
                  T
                  
                    5
                  
                
                +
                ⋯
                +
                
                n
                −
                1
                
                
                  T
                  
                    5
                  
                
                +
                n
                
                  T
                  
                    5
                  
                
                +
                
                n
                +
                1
                
                
                  T
                  
                    5
                  
                
              
            
            
              
                =
                 
              
              
                
                  T
                  
                    5
                  
                
                +
                2
                
                  T
                  
                    5
                  
                
                +
                3
                
                  T
                  
                    5
                  
                
                +
                4
                
                  T
                  
                    5
                  
                
                +
                ⋯
                +
                
                n
                −
                1
                
                
                  T
                  
                    5
                  
                
                +
                n
                
                  T
                  
                    5
                  
                
                +
                
                n
                +
                1
                
                
                  T
                  
                    5
                  
                
                −
                
                  T
                  
                    5
                  
                
              
            
          
        
      
    
    displaystyle beginaligned2T5+3T5+4T5+cdots +n1T5+nT5+n+1T5= T5+2T5+3T5+4T5+cdots +n1T5+nT5+n+1T5T5endaligned
  which can be factored as

  
    
      
        
          
            
              
              
                
                  T
                  
                    5
                  
                
                
                  
                  
                    1
                    +
                    2
                    +
                    3
                    +
                    ⋯
                    +
                    
                    n
                    −
                    1
                    
                    +
                    n
                    +
                    
                    n
                    +
                    1
                    
                  
                  
                
                −
                
                  T
                  
                    5
                  
                
              
            
            
              
                =
              
              
                
                  
                  
                    
                      
                        1
                        2
                      
                    
                    
                    
                      n
                      
                        2
                      
                    
                    +
                    n
                    
                  
                  
                
                
                  T
                  
                    5
                  
                
                +
                
                n
                +
                1
                
                
                  T
                  
                    5
                  
                
                −
                
                  T
                  
                    5
                  
                
              
            
            
              
                =
              
              
                
                  T
                  
                    5
                  
                
                
                  
                  
                    
                      
                        1
                        2
                      
                    
                    
                    
                      n
                      
                        2
                      
                    
                    +
                    n
                    
                  
                  
                
                +
                n
                
                  T
                  
                    5
                  
                
              
            
            
              
                =
              
              
                
                  
                  
                    
                      
                        1
                        2
                      
                    
                    
                    
                      n
                      
                        2
                      
                    
                    +
                    3
                    n
                    
                  
                  
                
                
                  T
                  
                    5
                  
                
              
            
          
        
      
    
    displaystyle beginalignedT5left1+2+3+cdots +n1+n+n+1rightT5=leftfrac 12n2+nrightT5+n+1T5T5=T5leftfrac 12n2+nright+nT5=leftfrac 12n2+3nrightT5endaligned
  Therefore the total running time for this algorithm is

  
    
      
        f
        
        n
        
        =
        
          T
          
            1
          
        
        +
        
          T
          
            2
          
        
        +
        
          T
          
            3
          
        
        +
        
          T
          
            7
          
        
        +
        
        n
        +
        1
        
        
          T
          
            4
          
        
        +
        
          
          
            
              
                1
                2
              
            
            
            
              n
              
                2
              
            
            +
            n
            
          
          
        
        
          T
          
            6
          
        
        +
        
          
          
            
              
                1
                2
              
            
            
            
              n
              
                2
              
            
            +
            3
            n
            
          
          
        
        
          T
          
            5
          
        
      
    
    displaystyle fn=T1+T2+T3+T7+n+1T4+leftfrac 12n2+nrightT6+leftfrac 12n2+3nrightT5
  which reduces to

  
    
      
        f
        
        n
        
        =
        
          
          
            
              
                1
                2
              
            
            
            
              n
              
                2
              
            
            +
            n
            
          
          
        
        
          T
          
            6
          
        
        +
        
          
          
            
              
                1
                2
              
            
            
            
              n
              
                2
              
            
            +
            3
            n
            
          
          
        
        
          T
          
            5
          
        
        +
        
        n
        +
        1
        
        
          T
          
            4
          
        
        +
        
          T
          
            1
          
        
        +
        
          T
          
            2
          
        
        +
        
          T
          
            3
          
        
        +
        
          T
          
            7
          
        
      
    
    displaystyle fn=leftfrac 12n2+nrightT6+leftfrac 12n2+3nrightT5+n+1T4+T1+T2+T3+T7
  As a ruleofthumb one can assume that the highestorder term in any given function dominates its rate of growth and thus defines its runtime order  In this example n2 is the highestorder term so one can conclude that fn = On2  Formally this can be proven as follows

Prove that 
  
    
      
        
          
          
            
              
                1
                2
              
            
            
            
              n
              
                2
              
            
            +
            n
            
          
          
        
        
          T
          
            6
          
        
        +
        
          
          
            
              
                1
                2
              
            
            
            
              n
              
                2
              
            
            +
            3
            n
            
          
          
        
        
          T
          
            5
          
        
        +
        
        n
        +
        1
        
        
          T
          
            4
          
        
        +
        
          T
          
            1
          
        
        +
        
          T
          
            2
          
        
        +
        
          T
          
            3
          
        
        +
        
          T
          
            7
          
        
        ≤
        c
        
          n
          
            2
          
        
        
         
        n
        ≥
        
          n
          
            0
          
        
      
    
    displaystyle leftfrac 12n2+nrightT6+leftfrac 12n2+3nrightT5+n+1T4+T1+T2+T3+T7leq cn2 ngeq n0
  

  
    
      
        
          
            
              
              
                
                  
                  
                    
                      
                        1
                        2
                      
                    
                    
                    
                      n
                      
                        2
                      
                    
                    +
                    n
                    
                  
                  
                
                
                  T
                  
                    6
                  
                
                +
                
                  
                  
                    
                      
                        1
                        2
                      
                    
                    
                    
                      n
                      
                        2
                      
                    
                    +
                    3
                    n
                    
                  
                  
                
                
                  T
                  
                    5
                  
                
                +
                
                n
                +
                1
                
                
                  T
                  
                    4
                  
                
                +
                
                  T
                  
                    1
                  
                
                +
                
                  T
                  
                    2
                  
                
                +
                
                  T
                  
                    3
                  
                
                +
                
                  T
                  
                    7
                  
                
              
            
            
              
                ≤
              
              
                
                
                
                  n
                  
                    2
                  
                
                +
                n
                
                
                  T
                  
                    6
                  
                
                +
                
                
                  n
                  
                    2
                  
                
                +
                3
                n
                
                
                  T
                  
                    5
                  
                
                +
                
                n
                +
                1
                
                
                  T
                  
                    4
                  
                
                +
                
                  T
                  
                    1
                  
                
                +
                
                  T
                  
                    2
                  
                
                +
                
                  T
                  
                    3
                  
                
                +
                
                  T
                  
                    7
                  
                
                 
                
                
                  for 
                
                n
                ≥
                0
                
              
            
          
        
      
    
    displaystyle beginalignedleftfrac 12n2+nrightT6+leftfrac 12n2+3nrightT5+n+1T4+T1+T2+T3+T7leq n2+nT6+n2+3nT5+n+1T4+T1+T2+T3+T7 textfor ngeq 0endaligned
  

Let k be a constant greater than or equal to T1T7

  
    
      
        
          
            
              
              
                
                  T
                  
                    6
                  
                
                
                
                  n
                  
                    2
                  
                
                +
                n
                
                +
                
                  T
                  
                    5
                  
                
                
                
                  n
                  
                    2
                  
                
                +
                3
                n
                
                +
                
                n
                +
                1
                
                
                  T
                  
                    4
                  
                
                +
                
                  T
                  
                    1
                  
                
                +
                
                  T
                  
                    2
                  
                
                +
                
                  T
                  
                    3
                  
                
                +
                
                  T
                  
                    7
                  
                
                ≤
                k
                
                
                  n
                  
                    2
                  
                
                +
                n
                
                +
                k
                
                
                  n
                  
                    2
                  
                
                +
                3
                n
                
                +
                k
                n
                +
                5
                k
              
            
            
              
                =
              
              
                2
                k
                
                  n
                  
                    2
                  
                
                +
                5
                k
                n
                +
                5
                k
                ≤
                2
                k
                
                  n
                  
                    2
                  
                
                +
                5
                k
                
                  n
                  
                    2
                  
                
                +
                5
                k
                
                  n
                  
                    2
                  
                
                 
                
                
                  for 
                
                n
                ≥
                1
                
                =
                12
                k
                
                  n
                  
                    2
                  
                
              
            
          
        
      
    
    displaystyle beginalignedT6n2+n+T5n2+3n+n+1T4+T1+T2+T3+T7leq kn2+n+kn2+3n+kn+5k=2kn2+5kn+5kleq 2kn2+5kn2+5kn2 textfor ngeq 1=12kn2endaligned
  

Therefore 
  
    
      
        
          
          
            
              
                1
                2
              
            
            
            
              n
              
                2
              
            
            +
            n
            
          
          
        
        
          T
          
            6
          
        
        +
        
          
          
            
              
                1
                2
              
            
            
            
              n
              
                2
              
            
            +
            3
            n
            
          
          
        
        
          T
          
            5
          
        
        +
        
        n
        +
        1
        
        
          T
          
            4
          
        
        +
        
          T
          
            1
          
        
        +
        
          T
          
            2
          
        
        +
        
          T
          
            3
          
        
        +
        
          T
          
            7
          
        
        ≤
        c
        
          n
          
            2
          
        
        
        n
        ≥
        
          n
          
            0
          
        
        
           for 
        
        c
        =
        12
        k
        
        
          n
          
            0
          
        
        =
        1
      
    
    displaystyle leftfrac 12n2+nrightT6+leftfrac 12n2+3nrightT5+n+1T4+T1+T2+T3+T7leq cn2ngeq n0text for c=12kn0=1
  

A more elegant approach to analyzing this algorithm would be to declare that T1T7 are all equal to one unit of time in a system of units chosen so that one unit is greater than or equal to the actual times for these steps  This would mean that the algorithms running time breaks down as follows

  
    
      
        4
        +
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        i
        ≤
        4
        +
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        n
        =
        4
        +
        
          n
          
            2
          
        
        ≤
        5
        
          n
          
            2
          
        
         
        
        
          for 
        
        n
        ≥
        1
        
        =
        O
        
        
          n
          
            2
          
        
        
        
      
    
    displaystyle 4+sum i=1nileq 4+sum i=1nn=4+n2leq 5n2 textfor ngeq 1=On2
  


==========
The methodology of runtime analysis can also be utilized for predicting other growth rates such as consumption of memory space  As an example consider the following pseudocode which manages and reallocates memory usage by a program based on the size of a file which that program manages

while file still open
    let n = size of file
    for every 100000 kilobytes of increase in file size
        double the amount of memory reserved

In this instance as the file size n increases memory will be consumed at an exponential growth rate which is order O2n This is an extremely rapid and most likely unmanageable growth rate for consumption of memory resources


==========
Algorithm analysis is important in practice because the accidental or unintentional use of an inefficient algorithm can significantly impact system performance In timesensitive applications an algorithm taking too long to run can render its results outdated or useless An inefficient algorithm can also end up requiring an uneconomical amount of computing power or storage in order to run again rendering it practically useless


==========
Analysis of algorithms typically focuses on the asymptotic performance particularly at the elementary level but in practical applications constant factors are important and realworld data is in practice always limited in size The limit is typically the size of addressable memory so on 32bit machines 232 = 4 GiB greater if segmented memory is used and on 64bit machines 264 = 16 EiB Thus given a limited size an order of growth time or space can be replaced by a constant factor and in this sense all practical algorithms are O1 for a large enough constant or for small enough data
This interpretation is primarily useful for functions that grow extremely slowly binary iterated logarithm log is less than 5 for all practical data 265536 bits binary loglog log log n is less than 6 for virtually all practical data 264 bits and binary log log n is less than 64 for virtually all practical data 264 bits An algorithm with nonconstant complexity may nonetheless be more efficient than an algorithm with constant complexity on practical data if the overhead of the constant time algorithm results in a larger constant factor eg one may have 
  
    
      
        K
        
        k
        log
        ⁡
        log
        ⁡
        n
      
    
    displaystyle Kklog log n
   so long as 
  
    
      
        K
        
          
        
        k
        
        6
      
    
    displaystyle Kk6
   and 
  
    
      
        n
        
        
          2
          
            
              2
              
                6
              
            
          
        
        =
        
          2
          
            64
          
        
      
    
    displaystyle n226=264
  
For large data linear or quadratic factors cannot be ignored but for small data an asymptotically inefficient algorithm may be more efficient This is particularly used in hybrid algorithms like Timsort which use an asymptotically efficient algorithm here merge sort with time complexity 
  
    
      
        n
        log
        ⁡
        n
      
    
    displaystyle nlog n
   but switch to an asymptotically inefficient algorithm here insertion sort with time complexity 
  
    
      
        
          n
          
            2
          
        
      
    
    displaystyle n2
   for small data as the simpler algorithm is faster on small data


==========
Amortized analysis
Analysis of parallel algorithms
Asymptotic computational complexity
Best worst and average case
Big O notation
Computational complexity theory
Master theorem analysis of algorithms
NPComplete
Numerical analysis
Polynomial time
Program optimization
Profiling computer programming
Scalability
Smoothed analysis
Termination analysis — the subproblem of checking whether a program will terminate at all
Time complexity — includes table of orders of growth for common algorithms
Informationbased complexity


==========


==========
Cormen Thomas H Leiserson Charles E Rivest Ronald L  Stein Clifford 2001 Introduction to Algorithms Chapter 1 Foundations Second ed Cambridge MA MIT Press and McGrawHill pp 3–122 ISBN 0262032937
Sedgewick Robert 1998 Algorithms in C Parts 14 Fundamentals Data Structures Sorting Searching 3rd ed Reading MA AddisonWesley Professional ISBN 9780201314526
Knuth Donald The Art of Computer Programming AddisonWesley
Greene Daniel A Knuth Donald E 1982 Mathematics for the Analysis of Algorithms Second ed Birkhäuser ISBN 376433102X
Goldreich Oded 2010 Computational Complexity A Conceptual Perspective Cambridge University Press ISBN 9780521884730The Analytical Engine was a proposed mechanical generalpurpose computer designed by English mathematician and computer pioneer Charles Babbage It was first described in 1837 as the successor to Babbages difference engine a design for a simpler mechanical computerThe Analytical Engine incorporated an arithmetic logic unit control flow in the form of conditional branching and loops and integrated memory making it the first design for a generalpurpose computer that could be described in modern terms as Turingcomplete In other words the logical structure of the Analytical Engine was essentially the same as that which has dominated computer design in the electronic era The Analytical Engine is one of the most successful achievements of Charles Babbage
Babbage was never able to complete construction of any of his machines due to conflicts with his chief engineer and inadequate funding It was not until 1941 that the first generalpurpose computer Z3 was actually built more than a century after Babbage had proposed the pioneering Analytical Engine in 1837


==========

Babbages first attempt at a mechanical computing device the Difference Engine was a specialpurpose machine designed to tabulate logarithms and trigonometric functions by evaluating finite differences to create approximating polynomials Construction of this machine was never completed Babbage had conflicts with his chief engineer Joseph Clement and ultimately the British government withdrew its funding for the projectDuring this project he realized that a much more general design the Analytical Engine was possible The work on the design of the Analytical Engine started in c 1833The input consisting of programs and data formulae and data was to be provided to the machine via punched cards a method being used at the time to direct mechanical looms such as the Jacquard loom For output the machine would have a printer a curve plotter and a bell The machine would also be able to punch numbers onto cards to be read in later It employed ordinary base10 fixedpoint arithmeticThere was to be a store that is a memory capable of holding 1000 numbers of 40 decimal digits each ca 162 kB An arithmetic unit the mill would be able to perform all four arithmetic operations plus comparisons and optionally square roots Initially 1838 it was conceived as a difference engine curved back upon itself in a generally circular layout with the long store exiting off to one side Later drawings 1858 depict a regularized grid layout Like the central processing unit CPU in a modern computer the mill would rely upon its own internal procedures to be stored in the form of pegs inserted into rotating drums called barrels to carry out some of the more complex instructions the users program might specifyThe programming language to be employed by users was akin to modern day assembly languages Loops and conditional branching were possible and so the language as conceived would have been Turingcomplete as later defined by Alan Turing Three different types of punch cards were used one for arithmetical operations one for numerical constants and one for load and store operations transferring numbers from the store to the arithmetical unit or back There were three separate readers for the three types of cards Babbage developed some two dozen programs for the Analytical Engine between 1837 and 1840 and one program later These programs treat polynomials iterative formulas Gaussian elimination and Bernoulli numbersIn 1842 the Italian mathematician Luigi Federico Menabrea published a description of the engine based on a lecture by Babbage in French In 1843 the description was translated into English and extensively annotated by Ada Lovelace who had become interested in the engine eight years earlier In recognition of her additions to Menabreas paper which included a way to calculate Bernoulli numbers using the machine widely considered to be the first complete computer program she has been described as the first computer programmer


==========

Late in his life Babbage sought ways to build a simplified version of the machine and assembled a small part of it before his death in 1871In 1878 a committee of the British Association for the Advancement of Science described the Analytical Engine as a marvel of mechanical ingenuity but recommended against constructing it The committee acknowledged the usefulness and value of the machine but could not estimate the cost of building it and were unsure whether the machine would function correctly after being builtIntermittently from 1880 to 1910 Babbages son Henry Prevost Babbage was constructing a part of the mill and the printing apparatus In 1910 it was able to calculate a faulty list of multiples of pi This constituted only a small part of the whole engine it was not programmable and had no storage Popular images of this section have sometimes been mislabelled implying that it was the entire mill or even the entire engine Henry Babbages Analytical Engine Mill is on display at the Science Museum in London Henry also proposed building a demonstration version of the full engine with a smaller storage capacity perhaps for a first machine ten columns would do with fifteen wheels in each Such a version could manipulate 20 numbers of 25 digits each and what it could be told to do with those numbers could still be impressive It is only a question of cards and time wrote Henry Babbage in 1888  and there is no reason why twenty thousand cards should not be used if necessary in an Analytical Engine for the purposes of the mathematicianIn 1991 the London Science Museum built a complete and working specimen of Babbages Difference Engine No 2 a design that incorporated refinements Babbage discovered during the development of the Analytical Engine This machine was built using materials and engineering tolerances that would have been available to Babbage quelling the suggestion that Babbages designs could not have been produced using the manufacturing technology of his timeIn October 2010 John GrahamCumming started a Plan 28 campaign to raise funds by public subscription to enable serious historical and academic study of Babbages plans with a view to then build and test a fully working virtual design which will then in turn enable construction of the physical Analytical Engine  As of May 2016 actual construction had not been attempted since no consistent understanding could yet be obtained from Babbages original design drawings In particular it was unclear whether it could handle the indexed variables which were required for Lovelaces Bernoulli program By 2017 the Plan 28 effort reported that a searchable database of all catalogued material was available and an initial review of Babbages voluminous Scribbling Books had been completed


==========

Babbage is not known to have written down an explicit set of instructions for the engine in the manner of a modern processor manual  Instead he showed his programs as lists of states during their execution showing what operator was run at each step with little indication of how the control flow would be guided
Allan G Bromley has assumed that the card deck could be read in forwards and backwards directions as a function of conditional branching after testing for conditions which would make the engine Turingcomplete

the cards could be ordered to move forward and reverse and hence to loop
The introduction for the first time in 1845 of user operations for a variety of service functions including most importantly an effective system for user control of looping in user programs
There is no indication how the direction of turning of the operation and variable cards is specified In the absence of other evidence I have had to adopt the minimal default assumption that both the operation and variable cards can only be turned backward as is necessary to implement the loops used in Babbage’s sample programs There would be no mechanical or microprogramming difficulty in placing the direction of motion under the control of the user
In their emulator of the engine Fourmilab say

The Engines Card Reader is not constrained to simply process the cards in a chain one after another from start to finish It can in addition directed by the very cards it reads and advised by the whether the Mills runup lever is activated either advance the card chain forward skipping the intervening cards or backward causing previouslyread cards to be processed once again
This emulator does provide a written symbolic instruction set though this has been constructed by its authors rather than based on Babbages original works  For example a factorial program would be written as

N0 6
N1 1
N2 1
×
L1
L0
S1
–
L0
L2
S0
L2
L0
CB11

where the CB is the conditional branch instruction or combination card used to make the control flow jump in this case backwards by 11 cards


==========


==========
Babbage understood that the existence of an automatic computer would kindle interest in the field now known as algorithmic efficiency writing in his Passages from the Life of a Philosopher As soon as an Analytical Engine exists it will necessarily guide the future course of the science Whenever any result is sought by its aid the question will then arise—By what course of calculation can these results be arrived at by the machine in the shortest time


==========
From 1872 Henry continued diligently with his fathers work and then intermittently in retirement in 1875Percy Ludgate wrote about the engine in 1914  and published his own design for an Analytical Engine in 1909   It was drawn up in detail but never built and the drawings have never been found Ludgates engine would be much smaller about 8 cubic feet 230 L than Babbages and hypothetically would be capable of multiplying two 20decimaldigit numbers in about six secondsTorres y Quevedo wrote about Babbages engines in Essays on Automatics 1913 Book contains design for an electromechanical machine capable of calculating completely automatically the value of a functionVannevar Bushs paper Instrumental Analysis 1936 included several references to Babbages work In the same year started Rapid Arithmetical Machine project to investigate the problems of constructing an electronic digital computerDespite this groundwork Babbages work fell into historical obscurity and the Analytical Engine was unknown to builders of electromechanical and electronic computing machines in the 1930s and 1940s when they began their work resulting in the need to reinvent many of the architectural innovations Babbage had proposed Howard Aiken who built the quicklyobsoleted electromechanical calculator the Harvard Mark I between 1937 and 1945 praised Babbages work likely as a way of enhancing his own stature but knew nothing of the Analytical Engines architecture during the construction of the Mark I and considered his visit to the constructed portion of the Analytical Engine the greatest disappointment of my life The Mark I showed no influence from the Analytical Engine and lacked the Analytical Engines most prescient architectural feature conditional branching J Presper Eckert and John W Mauchly similarly were not aware of the details of Babbages Analytical Engine work prior to the completion of their design for the first electronic generalpurpose computer the ENIAC


==========
If the Analytical Engine had been built it would have been digital programmable and Turingcomplete It would however have been very slow Luigi Federico Menabrea reported in Sketch of the Analytical Engine Mr Babbage believes he can by his engine form the product of two numbers each containing twenty figures in three minutes
By comparison the Harvard Mark I could perform the same task in just six seconds A modern PC can do the same thing in well under a billionth of a second


==========
The cyberpunk novelists William Gibson and Bruce Sterling coauthored a steampunk novel of alternative history titled The Difference Engine in which Babbages Difference and Analytical Engines became available to Victorian society The novel explores the consequences and implications of the early introduction of computational technology
There is also mention of the Analytical Engine or the Clockwork Ouroboros as it is also known there in The Book of the War a Faction Paradox anthology edited by Lawrence Miles This machine was used to calculate a way into the Eleven Day Empire Its use resulted in the destruction of the original Houses of Parliament
In the novel Perdido Street Station by British author China Miéville engines similar to Babbages serve as brains for the robotic constructs of the city of New Crobuzon One such engine even develops sentient thought due to a recursive algorithmic loop
The British Empire of The Peshawar Lancers by S M Stirling features a massive waterpowered analytical engine at Oxford used by two of the main characters It is noted that most of the engines run on steam and that an even larger one is under construction at the British Capital in Delhi
In the Michael Flynn novel In the Country of the Blind a secret society calling itself the Babbage Society secretly financed the building of Babbage Engines in the mid19th century In the novel the Society uses the Babbage engines along with a statistical science called Cliology to predict and manipulate future history In the process they predict the rise of the Nazis and accidentally start the US Civil War
In the Neal Stephenson novel The Diamond Age ubiquitous molecular nanotechnology is described to make use of rod logic similar to that imagined by Babbages design for the Analytical Engine
Moriarty by Modem a short story by Jack Nimersheim describes an alternate history where Babbages Analytical Engine was indeed completed and had been deemed highly classified by the British government The characters of Sherlock Holmes and Moriarty had in reality been a set of prototype programs written for the Analytical Engine This short story follows Holmes as his program is implemented on modern computers and he is forced to compete against his nemesis yet again in the modern counterparts of Babbages Analytical Engine
A similar setting is used by Sydney Padua in the webcomic The Thrilling Adventures of Lovelace and Babbage It features an alternate history where Ada Lovelace and Babbage have built the Analytical Engine and use it to fight crime at Queen Victorias request The comic is based on thorough research on the biographies of and correspondence between Babbage and Lovelace which is then twisted for humorous effect
Georgia on My Mind is a novelette by Charles Sheffield which involves two major themes being widowed and the quest for a legendary Babbage computer
Hugh Cooks fantasy novels The Wishstone and the Wonderworkers and The Wazir and the Witch feature an Analytical Engine created by the scientist Ivan Petrov It is used to calculate income tax
The Orions Arm online project features the Machina Babbagenseii fully sentient Babbage inspired mechanical computers Each is the size of a large asteroid only capable surviving in microgravity conditions and processes data at 05 the speed of a human brain
The flying ships in the anime Last Exile are seen to have analytical engines inside of them Although some have more advanced technology the common ships use analytical engines and even some of the advanced ships are seen to have clockwork mechanisms as well
A working version of the Analytical Engine created by fictional inventor Ernest Harding and based on the Babbage concept was featured on the Murdoch Mysteries also called The Artful Detective in Season 5 Episode 9 Invention Convention


==========


==========


==========

The Analytical Engine at Fourmilab includes historical documents and online simulations
Image of the General Plan of Babbages great calculating engine 1840 plus a modern description of operational  programming features Archived from the original on 21 August 2008
Image of a later Plan of Analytical Engine with grid layout 1858
First working Babbage barrel actually assembled circa 2005
Special issue IEEE Annals of the History of Computing Volume 22 Number 4 October–December 2000 subscription required
Babbage Science Museum London
The Marvellous Analytical Engine How It Works 2D Goggles 31 May 2015 Retrieved 23 August 2017Applicationrelease automation ARA refers to the process of packaging and deploying an application or update of an application from development across various environments and ultimately to production ARA solutions must combine the capabilities of deployment automation environment management and modeling and release coordination


==========
ARA tools help cultivate DevOps best practices by providing a combination of automation environment modeling and workflowmanagement capabilities These practices help teams deliver software rapidly reliably and responsibly ARA tools achieve a key DevOps goal of implementing continuous delivery with a large quantity of releases quickly 


==========
ARA is more than just softwaredeployment automation – it deploys applications using structured releaseautomation techniques that allow for an increase in visibility for the whole team It combines workload automation and releasemanagement tools as they relate to release packages as well as movement through different environments within the DevOps pipeline ARA tools help regulate deployments how environments are created and deployed and how and when releases are deployed


==========
Gartner and Forrester have published lists of ARA tools in their ARA Magic Quadrant and Wave reports respectively All ARA solutions must include capabilities in automation environment modeling and release coordination Additionally the solution must provide this functionality without reliance on other tools 


==========Application security encompasses measures taken to improve the security of an application often by finding fixing and preventing security vulnerabilities
Different techniques are used to surface such security vulnerabilities at different stages of an applications lifecycle such as design development deployment upgrade maintenance
An always evolving but largely consistent set of common security flaws are seen across different applications see common flaws


==========
Asset Resource of value such as the data in a database money in an account file on the filesystem or any system resource
Vulnerability A weakness or gap in security program that can be exploited by threats to gain unauthorized access to an asset
Attack or exploit An action taken to harm an asset
Threat Anything that can exploit a vulnerability and obtain damage or destroy an asset


==========
Different techniques will find different subsets of the security vulnerabilities lurking in an application and are most effective at different times in the software lifecycle They each represent different tradeoffs of time effort cost and vulnerabilities found

Whitebox security review or code review This is a security engineer deeply understanding the application through manually reviewing the source code and noticing security flaws Through comprehension of the application vulnerabilities unique to the application can be found
Blackbox security audit This is only through use of an application testing it for security vulnerabilities no source code required
Design review Before code is written working through a threat model of the application Sometimes alongside a spec or design document
Tooling There exist many automated tools that test for security flaws often with a higher false positive rate than having a human involved
Coordinated vulnerability platforms These are hackerpowered application security solutions offered by many websites and software developers by which individuals can receive recognition and compensation for reporting bugsUtilizing these techniques appropriately throughout the software development life cycle SDLC to maximize security is the role of an application security team


==========
According to the patterns  practices Improving Web Application Security book the following are classes of common application security threats  attacks

The OWASP community publishes a list of the top 10 vulnerabilities for web applications and outlines best security practices for organizations and while aiming to create open standards for the industry As of 2017 the organization lists the top application security threats as


==========

The proportion of mobile devices providing open platform functionality is expected to continue to increase in future The openness of these platforms offers significant opportunities to all parts of the mobile ecosystem by delivering the ability for flexible program and service delivery= options that may be installed removed or refreshed multiple times in line with the users needs and requirements However with openness comes responsibility and unrestricted access to mobile resources and APIs by applications of unknown or untrusted origin could result in damage to the user the device the network or all of these if not managed by suitable security architectures and network precautions Application security is provided in some form on most open OS mobile devices Symbian OS Microsoft BREW etc In 2017 Google expanded their Vulnerability Reward Program to cover vulnerabilities found in applications developed by third parties and made available through the Google Play Store Industry groups have also created recommendations including the GSM Association and Open Mobile Terminal Platform OMTPThere are several strategies to enhance mobile application security including

Application white listing
Ensuring transport layer security
Strong authentication and authorization
Encryption of data when written to memory
Sandboxing of applications
Granting application access on a perAPI level
Processes tied to a user ID
Predefined interactions between the mobile application and the OS
Requiring user input for privilegedelevated access
Proper session handling


==========
Security testing techniques scour for vulnerabilities or security holes in applications These vulnerabilities leave applications open to exploitation  Ideally security testing is implemented throughout the entire software development life cycle SDLC so that vulnerabilities may be addressed in a timely and thorough manner Unfortunately testing is often conducted as an afterthought at the end of the development cycle With the growth of Continuous delivery and DevOps as popular software development and deployment models continuous security models are becoming more popularVulnerability scanners and more specifically web application scanners otherwise known as penetration testing tools ie ethical hacking tools have been historically used by security organizations within corporations and security consultants to automate the security testing of http requestresponses however this is not a  substitute for the need for actual source code review Physical code reviews of an applications source code can be accomplished manually or in an automated fashion Given the common size of individual programs often 500000 lines of code or more the human brain cannot execute a comprehensive data flow analysis needed in order to completely check all circuitous paths of an application program to find vulnerability points The human brain is suited more for filtering interrupting and reporting the outputs of automated source code analysis tools available commercially versus trying to trace every possible path through a compiled code base to find the root cause level vulnerabilities
There are many kinds of automated tools for identifying vulnerabilities in applications Some require a great deal of security expertise to use and others are designed for fully automated use The results are dependent on the types of information source binary HTTP traffic configuration libraries connections provided to the tool the quality of the analysis and the scope of vulnerabilities covered Common technologies used for identifying application vulnerabilities include
Static Application Security Testing SAST is a technology that is frequently used as a Source Code Analysis tool The method analyzes source code for security vulnerabilities prior to the launch of an application and is used to strengthen code This method produces fewer false positives but for most implementations requires access to an applications source code and requires expert configuration and lots of processing powerDynamic Application Security Testing DAST is a technology which is able to find visible vulnerabilities by feeding a URL into an automated scanner This method is highly scalable easily integrated and quick DASTs drawbacks lie in the need for expert configuration and the high possibility of false positives and negativesInteractive Application Security Testing IAST is a solution that assesses applications from within using software instrumentation This technique allows IAST to combine the strengths of both SAST and DAST methods as well as providing access to code HTTP traffic library information backend connections and configuration information Some IAST products require the application to be attacked while others can be used during normal quality assurance testing


==========
The advances in professional Malware targeted at the Internet customers of online organizations have seen a change in Web application design requirements since 2007 It is generally assumed that a sizable percentage of Internet users will be compromised through malware and that any data coming from their infected host may be tainted Therefore application security has begun to manifest more advanced antifraud and heuristic detection systems in the backoffice rather than within the clientside or Web server code As of 2016 runtime application selfprotection RASP technologies have been developed RASP is a technology deployed within or alongside the application runtime environment that instruments an application and enables detection and prevention of attacks


==========
The CERT Coordination Center describes Coordinated Vulnerability Disclosure CVD as a “process for reducing adversary advantage while an information security vulnerability is being mitigated”  CVD is an iterative multiphase process that involves multiple stakeholders users vendors security researchers who may have different priorities and who must work together to resolve the vulnerability Because CVD processes involve multiple stakeholders managing communication about the vulnerability and its resolution is critical to success
From an operational perspective many tools and processes can aid in CVD These include email and web forms bug tracking systems and Coordinated vulnerability platforms


==========
CERT Secure Coding
CWE
DISASTIG
GrammLeachBliley Act
Health Insurance Portability and Accountability Act HIPAA
ISOIEC 2703412011 Information technology — Security techniques — Application security  Part 1 Overview and concepts
ISOIEC TR 247722013 Information technology — Programming languages — Guidance to avoiding vulnerabilities in programming languages through language selection and use
NIST Special Publication 80053
OWASP
PCI Data Security Standard PCI DSS
SarbanesOxley Act SOX


==========
Application portfolio attack surface
Countermeasure
Data security
Database security
HERASAF
Information security
Trustworthy Computing Security Development Lifecycle
Web application
Web application framework


==========In computer programming a parameter or a  formal argument is a special kind of variable used in a subroutine to refer to one of the pieces of data provided as input to the subroutine These pieces of data are the values of the arguments often called actual arguments or actual parameters with which the subroutine is going to be calledinvoked An ordered list of parameters is usually included in the definition of a subroutine so that each time the subroutine is called its arguments for that call are evaluated and the resulting values can be assigned to the corresponding parameters
Unlike argument in usual mathematical usage the argument in computer science is thus the actual input expression passedsupplied to a function procedure or routine in the invocationcall statement whereas the parameter is the variable inside the implementation of the subroutine For example if one defines the add subroutine as def addx y return x + y then x y are parameters while if this is called as add2 3 then 2 3 are the arguments Note that variables and expressions thereof from the calling context can be arguments if the subroutine is called as a = 2 b = 3 adda b then the variables a b are the arguments not the values 2 3 See the Parameters and arguments section for more information
In the most common case call by value a parameter acts within the subroutine as a new local variable initialized to the value of the argument a local isolated copy of the argument if the argument is a variable but in other cases eg call by reference the argument variable supplied by the caller can be affected by actions within the called subroutine as discussed in evaluation strategy 
The semantics for how parameters can be declared and how the value of arguments are passed to the parameters of subroutines are defined by the language but the details of how this is represented in any particular computer system depend on the calling conventions of that system


==========
The following program in the C programming language defines a function that is named SalesTax and has one parameter named price The type of price is double ie a doubleprecision floating point number The functions return type is also a double

After the function has been defined it can be invoked as follows

In this example the function has been invoked with the argument 1000 When this happens 1000 will be assigned to price and the function begins calculating its result The steps for producing the result are specified below enclosed in  005  price indicates that the first thing to do is multiply 005 by the value of price which gives 050 return means the function will produce the result of 005  price Therefore the final result ignoring possible roundoff errors one encounters with representing decimal fractions as binary fractions is 050


==========
The terms parameter and argument may have different meanings in different programming languages Sometimes they are used interchangeably and the context is used to distinguish the meaning  The term parameter sometimes called formal parameter is often used to refer to the variable as found in the function definition while argument sometimes called actual parameter refers to the actual input supplied at function call For example if one defines a function as def fx  then x is the parameter and if it is called by a =  fa then a is the argument A parameter is an unbound variable while the argument can be a value or variable or more complex expression involving values and variables In case of call by value what is passed to the function is the value of the argument – for example f2 and a = 2 fa are equivalent calls – while in call by reference with a variable as argument what is passed is a reference to that variable  even though the syntax for the function call could stay the same The specification for passbyreference or passbyvalue would be made in the function declaration andor definition
Parameters appear in procedure definitions arguments appear in procedure calls In the function definition fx = xx the variable x is a parameter in the function call f2 the value 2 is the argument of the function Loosely a parameter is a type and an argument is an instance
A parameter is an intrinsic property of the procedure included in its definition For example in many languages a procedure to add two supplied integers together and calculate the sum would need two parameters one for each integer In general a procedure may be defined with any number of parameters or no parameters at all If a procedure has parameters the part of its definition that specifies the parameters is called its parameter list
By contrast the arguments are the expressions supplied to the procedure when it is called usually one expression matching one of the parameters Unlike the parameters which form an unchanging part of the procedures definition the arguments may vary from call to call Each time a procedure is called the part of the procedure call that specifies the arguments is called the argument list
Although parameters are also commonly referred to as arguments arguments are sometimes thought of as the actual values or references assigned to the parameter variables when the subroutine is called at runtime When discussing code that is calling into a subroutine any values or references passed into the subroutine are the arguments and the place in the code where these values or references are given is the parameter list When discussing the code inside the subroutine definition the variables in the subroutines parameter list are the parameters while the values of the parameters at runtime are the arguments For example in C when dealing with threads its common to pass in an argument of type void and cast it to an expected type

To better understand the difference consider the following function written in C

The function Sum has two parameters named addend1 and addend2 It adds the values passed into the parameters and returns the result to the subroutines caller using a technique automatically supplied by the C compiler
The code which calls the Sum function might look like this

The variables value1 and value2 are initialized with values value1 and value2 are both arguments to the sum function in this context
At runtime the values assigned to these variables are passed to the function Sum as arguments In the Sum function the parameters addend1 and addend2 are evaluated yielding the arguments 40 and 2 respectively The values of the arguments are added and the result is returned to the caller where it is assigned to the variable sumvalue
Because of the difference between parameters and arguments it is possible to supply inappropriate arguments to a procedure The call may supply too many or too few arguments one or more of the arguments may be a wrong type or arguments may be supplied in the wrong order Any of these situations causes a mismatch between the parameter and argument lists and the procedure will often return an unintended answer or generate a runtime error


==========
Within the Eiffel software development method and language the terms argument and parameter have distinct uses established by convention The term argument is used exclusively in reference to a routines inputs and the term parameter is used exclusively in type parameterization for generic classesConsider the following routine definition

The routine sum takes two arguments addend1 and addend2 which are called the routines formal arguments  A call to sum specifies actual arguments as shown below with value1 and value2

Parameters are also thought of as either formal or actual Formal generic parameters are used in the definition of generic classes In the example below the class HASHTABLE  is declared as a generic class which has two formal generic parameters G representing data of interest and K representing the hash key for the data

When a class becomes a client to HASHTABLE the formal generic parameters are substituted with actual generic parameters in a generic derivation In the following attribute declaration mydictionary is to be used as a character string based dictionary As such both data and key formal generic parameters are substituted with actual generic parameters of type STRING


==========
In strongly typed programming languages each parameters type must be specified in the procedure declaration  Languages using type inference attempt to discover the types automatically from the functions body and usage Dynamically typed programming languages defer type resolution until runtime Weakly typed languages perform little to no type resolution relying instead on the programmer for correctness
Some languages use a special keyword eg void to indicate that the subroutine has no parameters in formal type theory such functions take an empty parameter list whose type is not void but rather unit


==========
The exact mechanism for assigning arguments to parameters called argument passing depends upon the evaluation strategy used for that parameter typically call by value which may be specified using keywords


==========
Some programming languages such as Ada C++ Clojure Common Lisp Fortran 90 Python Ruby Tcl and Windows PowerShell allow for a default argument to be explicitly or implicitly given in a subroutines declaration This allows the caller to omit that argument when calling the subroutine If the default argument is explicitly given then that value is used if it is not provided by the caller If the default argument is implicit sometimes by using a keyword such as Optional then the language provides a wellknown value such as null Empty zero an empty string etc if a value is not provided by the caller
PowerShell example

Default arguments can be seen as a special case of the variablelength argument list


==========
Some languages allow subroutines to be defined to accept a variable number of arguments For such languages the subroutines must iterate through the list of arguments
PowerShell example


==========
Some programming languages—such as Ada and Windows PowerShell—allow subroutines to have named parameters This allows the calling code to be more selfdocumenting It also provides more flexibility to the caller often allowing the order of the arguments to be changed or for arguments to be omitted as needed
PowerShell example


==========
In lambda calculus each function has exactly one parameter What is thought of as functions with multiple parameters is usually represented in lambda calculus as a function which takes the first argument and returns a function which takes the rest of the arguments this is a transformation known as currying Some programming languages like ML and Haskell follow this scheme In these languages every function has exactly one parameter and what may look like the definition of a function of multiple parameters is actually syntactic sugar for the definition of a function that returns a function etc Function application is leftassociative in these languages as well as in lambda calculus so what looks like an application of a function to multiple arguments is correctly evaluated as the function applied to the first argument then the resulting function applied to the second argument etc


==========
An output parameter also known as an out parameter or return parameter is a parameter used for output rather than the more usual use for input Using call by reference parameters or call by value parameters where the value is a reference as output parameters is an idiom in some languages notably C and C++ while other languages have builtin support for output parameters Languages with builtin support for output parameters include Ada see Ada subprograms Fortran since Fortran 90 see Fortran intent various procedural extensions to SQL such as PLSQL see PLSQL functions and TransactSQL C and the NET Framework Swift and the scripting language TScript see TScript function declarations
More precisely one may distinguish three types of parameters or parameter modes input parameters output parameters and inputoutput parameters these are often denoted in out and in out or inout An input argument the argument to an input parameter must be a value such as an initialized variable or literal and must not be redefined or assigned to an output argument must be an assignable variable but it need not be initialized any existing value is not accessible and must be assigned a value and an inputoutput argument must be an initialized assignable variable and can optionally be assigned a value The exact requirements and enforcement vary between languages – for example in Ada 83 output parameters can only be assigned to not read even after assignment this was removed in Ada 95 to remove the need for an auxiliary accumulator variable These are analogous to the notion of a value in an expression being an rvalue has a value an lvalue can be assigned or an rvaluelvalue has a value and can be assigned respectively though these terms have specialized meanings in C
In some cases only input and inputoutput are distinguished with output being considered a specific use of inputoutput and in other cases only input and output but not inputoutput are supported The default mode varies between languages in Fortran 90 inputoutput is default while in C and SQL extensions input is default and in TScript each parameter is explicitly specified as input or output
Syntactically parameter mode is generally indicated with a keyword in the function declaration such as void fout int x in C Conventionally output parameters are often put at the end of the parameter list to clearly distinguish them though this is not always followed TScript uses a different approach where in the function declaration input parameters are listed then output parameters separated by a colon  and there is no return type to the function itself as in this function which computes the size of a text fragment

Parameter modes are a form of denotational semantics stating the programmers intent and allowing compilers to catch errors and apply optimizations – they do not necessarily imply operational semantics how the parameter passing actually occurs Notably while input parameters can be implemented by call by value and output and inputoutput parameters by call by reference – and this is a straightforward way to implement these modes in languages without builtin support – this is not always how they are implemented This distinction is discussed in detail in the Ada 83 Rationale which emphasizes that the parameter mode is abstracted from which parameter passing mechanism by reference or by copy is actually implemented For instance while in C input parameters default no keyword are passed by value and output and inputoutput parameters out and ref are passed by reference in PLSQL input parameters IN are passed by reference and output and inputoutput parameters OUT and IN OUT are by default passed by value and the result copied back but can be passed by reference by using the NOCOPY compiler hintA syntactically similar construction to output parameters is to assign the return value to a variable with the same name as the function This is found in Pascal and Fortran 66 and Fortran 77 as in this Pascal example

This is semantically different in that when called the function is simply evaluated – it is not passed a variable from the calling scope to store the output in


==========
The primary use of output parameters is to return multiple values from a function while the use of inputoutput parameters is to modify state using parameter passing rather than by shared environment as in global variables An important use of returning multiple values is to solve the semipredicate problem of returning both a value and an error status – see Semipredicate problem Multivalued return
For example to return two variables from a function in C one may write

where x is an input parameter and width and height are output parameters
A common use case in C and related languages is for exception handling where a function places the return value in an output variable and returns a boolean corresponding to whether the function succeeded or not An archetypal example is the TryParse method in NET especially C which parses a string into an integer returning true on success and false on failure This has the following signature

and may be used as follows

Similar considerations apply to returning a value of one of several possible types where the return value can specify the type and then value is stored in one of several output variables


==========
Output parameters are often discouraged in modern programming essentially as being awkward confusing and too lowlevel – commonplace return values are considerably easier to understand and work with Notably output parameters involve functions with side effects modifying the output parameter and are semantically similar to references which are more confusing than pure functions and values and the distinction between output parameters and inputoutput parameters can be subtle Further since in common programming styles most parameters are simply input parameters output parameters and inputoutput parameters are unusual and hence susceptible to misunderstanding
Output and inputoutput parameters prevent function composition since the output is stored in variables rather than in the value of an expression Thus one must initially declare a variable and then each step of a chain of functions must be a separate statement For example in C++ the following function composition

when written with output and inputoutput parameters instead becomes for F it is an output parameter for G an inputoutput parameter

In the special case of a function with a single output or inputoutput parameter and no return value function composition is possible if the output or inputoutput parameter or in CC++ its address is also returned by the function in which case the above becomes


==========
There are various alternatives to the use cases of output parameters
For returning multiple values from a function an alternative is to return a tuple Syntactically this is clearer if automatic sequence unpacking and parallel assignment can be used as in Go or Python such as

For returning a value of one of several types a tagged union can be used instead the most common cases are nullable types option types where the return value can be null to indicate failure For exception handling one can return a nullable type or raise an exception For example in Python one might have either

or more idiomatically

The microoptimization of not requiring a local variable and copying the return when using output variables can also be applied to conventional functions and return values by sufficiently sophisticated compilers
The usual alternative to output parameters in C and related languages is to return a single data structure containing all return values For example given a structure encapsulating width and height one can write

In objectoriented languages instead of using inputoutput parameters one can often use call by sharing passing a reference to an object and then mutating the object though not changing which object the variable refers to


==========
Commandline argument
Evaluation strategy
Operator overloading
Free variables and bound variables


==========


==========An artifact is one of many kinds of tangible byproducts produced during the development of software Some artifacts eg use cases class diagrams and other Unified Modeling Language UML models requirements and design documents help describe the function architecture and design of software Other artifacts are concerned with the process of development itself—such as project plans business cases and risk assessments
The term artifact in connection with software development is largely associated with specific development methods or processes eg Unified Process This usage of the term may have originated with those methods
Build tools often refer to source code compiled for testing as an artifact because the executable is necessary to carrying out the testing plan  Without the executable to test the testing plan artifact is limited to nonexecution based testing  In nonexecution based testing the artifacts are the walkthroughs inspections and correctness proofs  On the other hand execution based testing requires at minimum two artifacts a test suite and the executable  An artifact occasionally may be used to refer to the released code in the case of a code library or released executable in the case of a program produced but the more common usage is in referring to the byproducts of software development rather than the product itself Open source code libraries often contain a testing harness to allow contributors to ensure their changes do not cause regression bugs in the code library
Much of what are considered artifacts is software documentation
In enduser development an artifact is either an application or a complex data object that is created by an enduser without the need to know a general programming language Artifacts describe automated behavior or control sequences such as database requests or grammar rules or usergenerated content
Artifacts vary in their maintainability  Maintainability is primarily affected by the role the artifact fulfills  The role can be either practical or symbolic  In the earliest stages of software development artifacts may be created by the design team to serve a symbolic role to show the project sponsor how serious the contractor is about meeting the projects needs  Symbolic artifacts often convey information poorly but are impressivelooking Symbolic enhance understanding  Generally speaking Illuminated Scrolls are also considered unmaintainable due to the diligence it requires to preserve the symbolic quality  For this reason once Illuminated Scrolls are shown to the project sponsor and approved they are replaced by artifacts which serve a practical role  Practical artifacts usually need to be maintained throughout the project lifecycle and as such are generally highly maintainable
Artifacts are significant from a project management perspective as deliverables The deliverables of a software project are likely to be the same as its artifacts with the addition of the software itself
The sense of artifacts as byproducts is similar to the use of the term artifact in science to refer to something that arises from the process in hand rather than the issue itself ie a result of interest that stems from the means rather than the end
To collect organize and manage artifacts a Software development folder may be utilized

 POST apiTodo
HttpPost
public async TaskActionResultTodoItem PostTodoItemTodoItem item

    contextTodoItemsAdditem
    await contextSaveChangesAsync
 
    return CreatedAtActionnameofGetTodoItem new  id = itemId  item



==========
Artifact UML
Software development folder


==========


==========
Per Kroll  Philippe Kruchten 2003 The Rational Unified Process Made Easy A Practitioners Guide to the RUP ISBN 0321166094